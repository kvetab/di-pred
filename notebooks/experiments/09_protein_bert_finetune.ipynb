{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff8a9d94-4216-4b46-92e0-67298b77bf45",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from os import path\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3394e398-aa33-45b9-981f-ee3ead567624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteinbert.finetuning import encode_train_and_valid_sets, encode_dataset\n",
    "from proteinbert import OutputType, OutputSpec, evaluate_by_len, load_pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a869012f-8189-4cd3-b7a7-f614ce87bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteinbert import OutputType, OutputSpec, FinetuningModelGenerator, load_pretrained_model, \\\n",
    "finetune, evaluate_by_len\n",
    "\n",
    "from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d2ed9b0-3023-4f0c-b93e-3330a86f35ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4798e75-9168-4aa2-b76c-bf37d692efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33c6481c-b3ce-47c3-86c4-5258a9553fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a5db7bda-d751-4876-916f-23e824fd7b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TYPE = OutputType(False, 'binary')\n",
    "UNIQUE_LABELS = [0, 1]\n",
    "OUTPUT_SPEC = OutputSpec(OUTPUT_TYPE, UNIQUE_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78d4d0ff-52b0-4152-9447-182709e45923",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_generator, input_encoder = load_pretrained_model(\"../../data/protein_bert/\", \"epoch_92400_sample_23500000.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "59ffc4b8-405d-4244-a4bc-f0ab0190c73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbdaf9de-0881-43dc-8d2e-317db6302948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkvetab\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/April_finetune/runs/1upyssz2\" target=\"_blank\">morning-totem-1</a></strong> to <a href=\"https://wandb.ai/kvetab/April_finetune\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/kvetab/April_finetune/runs/1upyssz2?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fcbc1f9fdd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=f\"April_finetune\", entity=\"kvetab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c5d33f-1465-4902-b24e-9a0e45c99d44",
   "metadata": {},
   "source": [
    "# Split 4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1743c445-302b-4f4e-9325-a6ef4fb17aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antibody_ID</th>\n",
       "      <th>heavy</th>\n",
       "      <th>light</th>\n",
       "      <th>Y</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>2g60</td>\n",
       "      <td>EVQLQQSGGELAKPGASVKMSCKSSGYTFTAYAIHWAKQAAGAGLE...</td>\n",
       "      <td>DVLMTQAPLTLPVSLGDQASISCRSSQAIVHANGNTYLEWYLQKPG...</td>\n",
       "      <td>0</td>\n",
       "      <td>911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>4ffy</td>\n",
       "      <td>QVQLLQPGAELVKPGASMKLSCKASGYTFTNWWMHWVRLRPGRGLE...</td>\n",
       "      <td>NIVLTQSPASLAVSLGQRATISCRASESVDHYGNSFIYWYQQKPGQ...</td>\n",
       "      <td>0</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>1w72</td>\n",
       "      <td>EVQLVESGGGLVQPGRSLRLSCAASGFTFDDYAMHWVRQAPGKGLE...</td>\n",
       "      <td>SYVLTQPPSVSVAPGQTARITCGGNNIGSRSVHWYQQKPGQAPVLV...</td>\n",
       "      <td>0</td>\n",
       "      <td>836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>2xwt</td>\n",
       "      <td>EVQLVQSGAEVKKPGQSLKISCKASGYSLTDNWIGWVRQKPGKGLE...</td>\n",
       "      <td>QSVLTQPPSVSAAPGQKVTISCSGSSSDIGSNYVSWYQQFPGTAPK...</td>\n",
       "      <td>0</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>2ipt</td>\n",
       "      <td>QVTLKESGPGILKPSQTLSLTCSFSGFSLSTSGMGVGWIRQPSGKG...</td>\n",
       "      <td>DVLMTQTPLSLPVSLGDQASISCRSSQSIVHSNGNTYLEWYLQKPG...</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Antibody_ID                                              heavy  \\\n",
       "535         2g60  EVQLQQSGGELAKPGASVKMSCKSSGYTFTAYAIHWAKQAAGAGLE...   \n",
       "1120        4ffy  QVQLLQPGAELVKPGASMKLSCKASGYTFTNWWMHWVRLRPGRGLE...   \n",
       "408         1w72  EVQLVESGGGLVQPGRSLRLSCAASGFTFDDYAMHWVRQAPGKGLE...   \n",
       "669         2xwt  EVQLVQSGAEVKKPGQSLKISCKASGYSLTDNWIGWVRQKPGKGLE...   \n",
       "568         2ipt  QVTLKESGPGILKPSQTLSLTCSFSGFSLSTSGMGVGWIRQPSGKG...   \n",
       "\n",
       "                                                  light  Y  cluster  \n",
       "535   DVLMTQAPLTLPVSLGDQASISCRSSQAIVHANGNTYLEWYLQKPG...  0      911  \n",
       "1120  NIVLTQSPASLAVSLGQRATISCRASESVDHYGNSFIYWYQQKPGQ...  0      478  \n",
       "408   SYVLTQPPSVSVAPGQTARITCGGNNIGSRSVHWYQQKPGQAPVLV...  0      836  \n",
       "669   QSVLTQPPSVSAAPGQKVTISCSGSSSDIGSNYVSWYQQFPGTAPK...  0      545  \n",
       "568   DVLMTQTPLSLPVSLGDQASISCRSSQSIVHSNGNTYLEWYLQKPG...  0       55  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/crossval/chen_4_a.csv\"), index_col=0)\n",
    "test_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/crossval/chen_4_b.csv\"), index_col=0)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c09b813-ffc8-48f7-99a5-586c76ac33f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"seq\"] = train_data[\"heavy\"] + train_data[\"light\"]\n",
    "test_data[\"seq\"] = test_data[\"heavy\"] + test_data[\"light\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fe63df3-96bf-40ed-9065-216a8e67b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience = 3, factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "    keras.callbacks.EarlyStopping(patience = 6, restore_best_weights = True),\n",
    "    WandbCallback()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faeffd00-5c33-40ed-ad02-1c71cb739527",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 50\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd45319f-0dd8-489a-b105-7163d1679298",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config = {\n",
    "      \"learning_rate\": learning_rate,\n",
    "      \"epochs\": epoch_num * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16a259a0-5115-4000-a24a-42193248ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Validating using test data!!! #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72bd9e53-8469-410c-9d42-6daa3811fc7e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_09-11:29:56] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-11:29:56] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-11:29:56] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-09 11:29:59.899855: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-09 11:30:03.060697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9656 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "2022-04-09 11:30:05.411634: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-09 11:30:17.126370: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 7605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 15s 770ms/step - loss: 1.0164 - val_loss: 0.9833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.6325 - val_loss: 0.6123\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.5371 - val_loss: 0.5515\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4412 - val_loss: 0.5147\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4427 - val_loss: 0.5044\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4353 - val_loss: 0.5586\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4335 - val_loss: 0.4875\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3942 - val_loss: 0.5111\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3829 - val_loss: 0.4740\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3746 - val_loss: 0.4777\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3685 - val_loss: 0.4681\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3593 - val_loss: 0.4659\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3508 - val_loss: 0.4904\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3569 - val_loss: 0.4569\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3406 - val_loss: 0.4834\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3486 - val_loss: 0.4778\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3548 - val_loss: 0.4528\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3354 - val_loss: 0.4603\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3389 - val_loss: 0.4708\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3302 - val_loss: 0.4625\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 1s 230ms/step - loss: 0.3204 - val_loss: 0.4544\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3265 - val_loss: 0.4512\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3227 - val_loss: 0.4517\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3186 - val_loss: 0.4541\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3248 - val_loss: 0.4483\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3163 - val_loss: 0.4546\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3191 - val_loss: 0.4529\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3202 - val_loss: 0.4501\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3176 - val_loss: 0.4483\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3188 - val_loss: 0.4502\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3119 - val_loss: 0.4509\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-11:30:52] Training the entire fine-tuned model...\n",
      "[2022_04_09-11:31:01] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 9s 851ms/step - loss: 0.3148 - val_loss: 0.4640\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3163 - val_loss: 0.4411\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3134 - val_loss: 0.4793\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3154 - val_loss: 0.4379\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3071 - val_loss: 0.4541\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3037 - val_loss: 0.4414\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 2s 389ms/step - loss: 0.3061 - val_loss: 0.4384\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.2893 - val_loss: 0.4425\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.2869 - val_loss: 0.4494\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.2867 - val_loss: 0.4391\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_09-11:31:28] Training on final epochs of sequence length 1024...\n",
      "[2022_04_09-11:31:28] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_09-11:31:48] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 1022.\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3287WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1085s vs `on_train_batch_end` time: 0.1387s). Check your callbacks.\n",
      "10/10 [==============================] - 11s 625ms/step - loss: 0.3206 - val_loss: 0.4471\n"
     ]
    }
   ],
   "source": [
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], test_data['seq'], test_data['Y'], \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f7e7d4f-4ca7-47fd-ad0f-cedd9de6c46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d14dac3-1751-4172-96e6-7636bb9ad8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set performance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># records</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model seq len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>654</td>\n",
       "      <td>0.798584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>654</td>\n",
       "      <td>0.798584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               # records       AUC\n",
       "Model seq len                     \n",
       "512                  654  0.798584\n",
       "All                  654  0.798584"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>471</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  471  29\n",
       "1   99  55"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Test-set performance:')\n",
    "display(results)\n",
    "\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "601cc41a-442a-4b5a-a651-a22f09e5ad2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46218487394957986"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c444b8d-daf7-4e24-a86c-c97f47b80d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "2022-04-09 11:32:18.361248: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_01/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_01/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "mod = model_generator.create_model(seq_len = 512)\n",
    "mod.save(path.join(DATA_DIR, \"protein_bert/2022_04_09_01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8230234c-f488-4a51-91d1-14a68aa24d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = [(3,3), (4,3), (6,3), (8,4)]\n",
    "learning_rate = [1e-5, 5e-5, 1e-4, 5e-4]\n",
    "prepro = [\"scaling\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4969f0aa-0567-4f3c-aa9f-c9681cf164d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_by_settings(patience, lr, project_name):\n",
    "    wandb.init(project=project_name, entity=\"kvetab\")\n",
    "    model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
    "    training_callbacks = [\n",
    "        keras.callbacks.ReduceLROnPlateau(patience = patience[1], factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "        keras.callbacks.EarlyStopping(patience = patience[0], restore_best_weights = True),\n",
    "        WandbCallback()\n",
    "    ]\n",
    "    epoch_num = 100\n",
    "    batch_size = 128\n",
    "    wandb.config = {\n",
    "          \"learning_rate\": lr,\n",
    "          \"epochs\": epoch_num * 2,\n",
    "          \"batch_size\": batch_size\n",
    "        }\n",
    "    print(type(lr))\n",
    "    print(lr)\n",
    "    finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], test_data['seq'], test_data['Y'], \\\n",
    "            seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = lr, begin_with_frozen_pretrained_layers = True, \\\n",
    "            lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 512, final_lr = lr / 10, callbacks = training_callbacks)\n",
    "    \n",
    "    results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "            start_seq_len = 512, start_batch_size = 32)\n",
    "    print('Confusion matrix:')\n",
    "    display(confusion_matrix)\n",
    "    fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "    f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "    print(f1)\n",
    "    \n",
    "    mod = model_generator.create_model(seq_len = 512)\n",
    "    mod.save(path.join(DATA_DIR, f\"protein_bert/4b/2022_04_09_{patience[0]}_{patience[1]}_{lr}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "591b1a01-6aee-4329-936e-30c6cdfa6091",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1brb55vo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 29462... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">easy-dew-5</strong>: <a href=\"https://wandb.ai/kvetab/April_finetune/runs/1brb55vo\" target=\"_blank\">https://wandb.ai/kvetab/April_finetune/runs/1brb55vo</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_163056-1brb55vo/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1brb55vo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/x0l7hmmm\" target=\"_blank\">bright-dragon-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-16:39:18] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:39:18] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:39:18] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 713ms/step - loss: 0.9282 - val_loss: 0.6173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5671 - val_loss: 0.8156\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.5555 - val_loss: 0.5091\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4692 - val_loss: 0.6209\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4401 - val_loss: 0.4939\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4065 - val_loss: 0.5786\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4133 - val_loss: 0.4855\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4013 - val_loss: 0.5161\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3902 - val_loss: 0.4835\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3693 - val_loss: 0.4710\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3567 - val_loss: 0.4843\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3608 - val_loss: 0.4609\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3549 - val_loss: 0.4749\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3489 - val_loss: 0.4603\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3473 - val_loss: 0.4526\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3449 - val_loss: 0.4539\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3366 - val_loss: 0.4500\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3397 - val_loss: 0.5423\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3514 - val_loss: 0.4781\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3693 - val_loss: 0.5008\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-16:39:50] Training the entire fine-tuned model...\n",
      "[2022_04_09-16:40:15] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 843ms/step - loss: 0.3286 - val_loss: 0.4566\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3398 - val_loss: 0.4581\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3277 - val_loss: 0.4564\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3336 - val_loss: 0.4522\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3291 - val_loss: 0.4489\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3339 - val_loss: 0.4505\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3322 - val_loss: 0.4561\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.3296 - val_loss: 0.4557\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_09-16:40:38] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-16:40:38] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:40:42] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 828ms/step - loss: 0.3211 - val_loss: 0.4489\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>497</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  497   3\n",
       "1  139  15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1744186046511628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:x0l7hmmm) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 29567... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▂▃▃▄▁</td></tr><tr><td>loss</td><td>█▄▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▂▄▂▃▂▂▂▁▂▁▁▁▁▁▁▃▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>4</td></tr><tr><td>best_val_loss</td><td>0.44886</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32113</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.44892</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">bright-dragon-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/x0l7hmmm\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/x0l7hmmm</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_163902-x0l7hmmm/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:x0l7hmmm). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/1slmu6w1\" target=\"_blank\">faithful-universe-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-16:41:36] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:41:36] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:41:36] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 741ms/step - loss: 0.7370 - val_loss: 0.5284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.5380 - val_loss: 0.8164\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5044 - val_loss: 0.5934\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4704 - val_loss: 0.6402\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-16:41:50] Training the entire fine-tuned model...\n",
      "[2022_04_09-16:42:00] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 841ms/step - loss: 0.4469 - val_loss: 0.5329\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.4365 - val_loss: 0.5425\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 390ms/step - loss: 0.4402 - val_loss: 0.5487\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.4370 - val_loss: 0.5443\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_09-16:42:16] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-16:42:16] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:42:16] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 831ms/step - loss: 0.4404 - val_loss: 0.5329\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  500  0\n",
       "1  154  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1slmu6w1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 29817... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█▁▃▆█▁</td></tr><tr><td>loss</td><td>█▃▃▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>████▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁█▃▄▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.52842</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.44035</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.53288</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">faithful-universe-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/1slmu6w1\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/1slmu6w1</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_164120-1slmu6w1/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1slmu6w1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/kab6mhd9\" target=\"_blank\">dandy-cherry-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-16:43:10] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:43:11] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:43:11] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 713ms/step - loss: 0.9501 - val_loss: 0.7940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.5922 - val_loss: 0.7073\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5549 - val_loss: 0.5184\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4522 - val_loss: 0.5754\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4386 - val_loss: 0.4977\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4069 - val_loss: 0.5564\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4023 - val_loss: 0.4902\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4111 - val_loss: 0.5953\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4009 - val_loss: 0.4835\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3714 - val_loss: 0.5172\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3654 - val_loss: 0.4677\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3667 - val_loss: 0.5145\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3639 - val_loss: 0.4591\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3490 - val_loss: 0.4749\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3404 - val_loss: 0.4624\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3435 - val_loss: 0.4520\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3382 - val_loss: 0.4563\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3273 - val_loss: 0.4529\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3143 - val_loss: 0.4578\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-16:43:41] Training the entire fine-tuned model...\n",
      "[2022_04_09-16:44:08] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 839ms/step - loss: 0.3392 - val_loss: 0.4694\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3390 - val_loss: 0.4553\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3266 - val_loss: 0.4523\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3386 - val_loss: 0.4530\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.3088 - val_loss: 0.4455\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3098 - val_loss: 0.4374\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3091 - val_loss: 0.4598\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2986 - val_loss: 0.4381\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.2982 - val_loss: 0.4868\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_09-16:44:34] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-16:44:34] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:44:34] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 838ms/step - loss: 0.3082 - val_loss: 0.4469\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>497</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>135</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  497   3\n",
       "1  135  19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2159090909090909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:kab6mhd9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 29957... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██▁▁▂▂▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▃▄▂▃▂▄▂▃▂▃▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.43743</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30818</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.44687</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dandy-cherry-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/kab6mhd9\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/kab6mhd9</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_164255-kab6mhd9/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:kab6mhd9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/1y68zqev\" target=\"_blank\">astral-surf-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-16:45:25] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:45:26] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:45:26] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 743ms/step - loss: 0.9617 - val_loss: 0.8701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.6679 - val_loss: 0.6900\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.6828 - val_loss: 0.7909\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5107 - val_loss: 0.5265\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4873 - val_loss: 0.6536\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4285 - val_loss: 0.5117\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4055 - val_loss: 0.5781\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4024 - val_loss: 0.4814\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3845 - val_loss: 0.5196\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3815 - val_loss: 0.4698\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3737 - val_loss: 0.4783\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3587 - val_loss: 0.4712\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3477 - val_loss: 0.4729\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-16:45:51] Training the entire fine-tuned model...\n",
      "[2022_04_09-16:45:59] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 833ms/step - loss: 0.3899 - val_loss: 0.5564\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3862 - val_loss: 0.5032\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3813 - val_loss: 0.4683\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3533 - val_loss: 0.5134\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3512 - val_loss: 0.4561\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3334 - val_loss: 0.5163\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3195 - val_loss: 0.4404\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3178 - val_loss: 0.4499\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.3016 - val_loss: 0.4666\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3074 - val_loss: 0.5220\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_09-16:46:26] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-16:46:26] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:46:46] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 11s 830ms/step - loss: 0.3093 - val_loss: 0.4409\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>489</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  489  11\n",
       "1  126  28"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29015544041450775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1y68zqev) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 30195... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█▁▂▂▃▃▄▅▅▆▆▁</td></tr><tr><td>loss</td><td>█▅▅▃▃▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▇▂▄▂▃▂▂▁▂▂▂▃▂▁▂▁▂▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>6</td></tr><tr><td>best_val_loss</td><td>0.44043</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30927</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.44093</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">astral-surf-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/1y68zqev\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/1y68zqev</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_164510-1y68zqev/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1y68zqev). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/1lxd3gvi\" target=\"_blank\">lilac-waterfall-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-16:47:39] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:47:39] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:47:39] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 716ms/step - loss: 0.9378 - val_loss: 0.8009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.5406 - val_loss: 0.6441\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5385 - val_loss: 0.5082\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4710 - val_loss: 0.6338\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4640 - val_loss: 0.4911\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4294 - val_loss: 0.5828\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4093 - val_loss: 0.4854\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4012 - val_loss: 0.5138\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3740 - val_loss: 0.4745\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3704 - val_loss: 0.4765\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3633 - val_loss: 0.4840\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3561 - val_loss: 0.4612\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3554 - val_loss: 0.4923\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3471 - val_loss: 0.4564\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3467 - val_loss: 0.4608\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3432 - val_loss: 0.4849\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3425 - val_loss: 0.4486\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3388 - val_loss: 0.4882\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3412 - val_loss: 0.4720\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3267 - val_loss: 0.4441\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3208 - val_loss: 0.5063\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3268 - val_loss: 0.4420\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3153 - val_loss: 0.4943\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3279 - val_loss: 0.4448\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3130 - val_loss: 0.4544\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3112 - val_loss: 0.4405\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3087 - val_loss: 0.4467\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3140 - val_loss: 0.4461\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3100 - val_loss: 0.4488\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3083 - val_loss: 0.4460\n",
      "[2022_04_09-16:48:23] Training the entire fine-tuned model...\n",
      "[2022_04_09-16:48:32] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 834ms/step - loss: 0.3141 - val_loss: 0.4505\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 389ms/step - loss: 0.3030 - val_loss: 0.4447\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3034 - val_loss: 0.4427\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3113 - val_loss: 0.4440\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.3137 - val_loss: 0.4466\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3084 - val_loss: 0.4484\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.3012 - val_loss: 0.4470\n",
      "[2022_04_09-16:48:53] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-16:48:53] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:48:53] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 828ms/step - loss: 0.3086 - val_loss: 0.4425\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>494</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>134</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  494   6\n",
       "1  134  20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2222222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1lxd3gvi) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 30411... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▂▅▂▄▂▂▂▂▂▁▂▁▁▂▁▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>25</td></tr><tr><td>best_val_loss</td><td>0.44047</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30858</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.44246</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">lilac-waterfall-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/1lxd3gvi\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/1lxd3gvi</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_164724-1lxd3gvi/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1lxd3gvi). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/4law5jef\" target=\"_blank\">zany-gorge-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-16:49:47] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:49:47] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:49:48] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 697ms/step - loss: 0.7894 - val_loss: 0.5397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.5550 - val_loss: 0.8808\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5763 - val_loss: 0.5170\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4928 - val_loss: 0.7205\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5268 - val_loss: 0.4889\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4457 - val_loss: 0.5645\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4161 - val_loss: 0.4812\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3926 - val_loss: 0.5306\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3914 - val_loss: 0.4720\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3844 - val_loss: 0.5093\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3729 - val_loss: 0.4776\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3482 - val_loss: 0.4695\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3616 - val_loss: 0.4925\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3689 - val_loss: 0.4592\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3457 - val_loss: 0.4849\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3436 - val_loss: 0.4502\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3420 - val_loss: 0.4921\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3464 - val_loss: 0.4719\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3635 - val_loss: 0.5063\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3474 - val_loss: 0.4457\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3401 - val_loss: 0.4490\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3310 - val_loss: 0.4880\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3334 - val_loss: 0.4499\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3367 - val_loss: 0.4479\n",
      "[2022_04_09-16:50:25] Training the entire fine-tuned model...\n",
      "[2022_04_09-16:50:33] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 852ms/step - loss: 0.3348 - val_loss: 0.4609\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3285 - val_loss: 0.4468\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3161 - val_loss: 0.4549\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3153 - val_loss: 0.4590\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3232 - val_loss: 0.4467\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3143 - val_loss: 0.4461\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3105 - val_loss: 0.4490\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3149 - val_loss: 0.4507\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.3058 - val_loss: 0.4458\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3142 - val_loss: 0.4448\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3059 - val_loss: 0.4480\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3021 - val_loss: 0.4514\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2929 - val_loss: 0.4495\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 391ms/step - loss: 0.3028 - val_loss: 0.4469\n",
      "[2022_04_09-16:51:07] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-16:51:07] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:51:20] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 843ms/step - loss: 0.3157 - val_loss: 0.4437\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>494</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  494   6\n",
       "1  132  22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24175824175824176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:4law5jef) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 30695... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▃▄▄▄▅▅▁</td></tr><tr><td>loss</td><td>█▅▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▂▅▂▃▂▂▁▂▂▁▂▁▂▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.44373</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31568</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.44373</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">zany-gorge-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/4law5jef\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/4law5jef</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_164931-4law5jef/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:4law5jef). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/vlm5iaxz\" target=\"_blank\">warm-night-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-16:52:13] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:52:13] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:52:13] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 748ms/step - loss: 0.9450 - val_loss: 0.7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.7070 - val_loss: 0.8025\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.6830 - val_loss: 0.7416\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5224 - val_loss: 0.5091\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4643 - val_loss: 0.6603\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4753 - val_loss: 0.4978\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4017 - val_loss: 0.5697\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4062 - val_loss: 0.4844\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3927 - val_loss: 0.5295\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3929 - val_loss: 0.4739\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3823 - val_loss: 0.4974\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3764 - val_loss: 0.4751\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3683 - val_loss: 0.4743\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3499 - val_loss: 0.4630\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3494 - val_loss: 0.4714\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3494 - val_loss: 0.4781\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3474 - val_loss: 0.4616\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3449 - val_loss: 0.4641\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3461 - val_loss: 0.4777\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3447 - val_loss: 0.4658\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3458 - val_loss: 0.4641\n",
      "[2022_04_09-16:52:47] Training the entire fine-tuned model...\n",
      "[2022_04_09-16:53:11] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 829ms/step - loss: 0.3546 - val_loss: 0.4730\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3431 - val_loss: 0.4660\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3439 - val_loss: 0.4549\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3299 - val_loss: 0.4591\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3250 - val_loss: 0.4464\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3227 - val_loss: 0.4905\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3190 - val_loss: 0.4427\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2979 - val_loss: 0.4665\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 391ms/step - loss: 0.3022 - val_loss: 0.4379\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3118 - val_loss: 0.4393\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.2939 - val_loss: 0.4774\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 390ms/step - loss: 0.2818 - val_loss: 0.4366\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2726 - val_loss: 0.4791\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2555 - val_loss: 0.4436\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.2411 - val_loss: 0.4970\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.2406 - val_loss: 0.4679\n",
      "[2022_04_09-16:53:49] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-16:53:49] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:53:49] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 829ms/step - loss: 0.2798 - val_loss: 0.4403\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>487</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>117</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  487  13\n",
       "1  117  37"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3627450980392157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:vlm5iaxz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 30991... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▁</td></tr><tr><td>loss</td><td>█▆▅▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇█▇▂▅▂▄▂▃▂▂▂▂▂▂▂▁▂▂▂▂▂▂▁▁▁▂▁▂▁▁▂▁▂▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_val_loss</td><td>0.4366</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.27979</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.44027</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">warm-night-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/vlm5iaxz\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/vlm5iaxz</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_165157-vlm5iaxz/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:vlm5iaxz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/157kf3xo\" target=\"_blank\">absurd-snowball-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-16:54:42] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:54:42] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:54:42] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 690ms/step - loss: 0.8095 - val_loss: 0.5416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5587 - val_loss: 0.7806\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4939 - val_loss: 0.5513\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4627 - val_loss: 0.5822\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4202 - val_loss: 0.4954\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4103 - val_loss: 0.4917\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3937 - val_loss: 0.5137\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3992 - val_loss: 0.5078\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3927 - val_loss: 0.4891\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3892 - val_loss: 0.4909\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3855 - val_loss: 0.5021\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3877 - val_loss: 0.4900\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3814 - val_loss: 0.4873\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3830 - val_loss: 0.4856\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3803 - val_loss: 0.4858\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3804 - val_loss: 0.4897\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3833 - val_loss: 0.4904\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3820 - val_loss: 0.4908\n",
      "[2022_04_09-16:55:13] Training the entire fine-tuned model...\n",
      "[2022_04_09-16:55:21] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 831ms/step - loss: 0.4695 - val_loss: 0.5476\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.4188 - val_loss: 0.4885\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.4048 - val_loss: 0.5607\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3941 - val_loss: 0.4844\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3951 - val_loss: 0.4902\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3763 - val_loss: 0.4900\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3699 - val_loss: 0.4693\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3459 - val_loss: 0.4817\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3405 - val_loss: 0.4576\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3263 - val_loss: 0.4840\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.3086 - val_loss: 0.4567\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.2856 - val_loss: 0.4555\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.2646 - val_loss: 0.4916\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2896 - val_loss: 0.4506\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2283 - val_loss: 0.6072\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.2161 - val_loss: 0.6392\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 389ms/step - loss: 0.1594 - val_loss: 0.6541\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.1217 - val_loss: 0.6083\n",
      "[2022_04_09-16:56:03] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-16:56:03] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:56:03] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 840ms/step - loss: 0.2274 - val_loss: 0.4698\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>475</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  475  25\n",
       "1  114  40"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.365296803652968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:157kf3xo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 31275... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██▁</td></tr><tr><td>loss</td><td>█▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▅▄▄▄▄▄▄▃▃▃▃▃▂▃▂▂▁▁▂</td></tr><tr><td>lr</td><td>████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▃▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▃▂▂▂▁▂▁▂▁▁▂▁▄▅▅▄▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>13</td></tr><tr><td>best_val_loss</td><td>0.45055</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.22744</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.46978</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">absurd-snowball-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/157kf3xo\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/157kf3xo</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_165426-157kf3xo/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:157kf3xo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/2tgdyvl0\" target=\"_blank\">sparkling-morning-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-16:56:58] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:56:58] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:56:58] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 740ms/step - loss: 0.8213 - val_loss: 0.5363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.6028 - val_loss: 0.9385\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5866 - val_loss: 0.5230\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4629 - val_loss: 0.6229\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4657 - val_loss: 0.4973\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4512 - val_loss: 0.6312\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4691 - val_loss: 0.4857\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3933 - val_loss: 0.5145\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3883 - val_loss: 0.4791\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3861 - val_loss: 0.5273\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3850 - val_loss: 0.4687\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3698 - val_loss: 0.4880\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3760 - val_loss: 0.5149\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3713 - val_loss: 0.4582\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3412 - val_loss: 0.5214\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3683 - val_loss: 0.4663\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3660 - val_loss: 0.4505\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3481 - val_loss: 0.5280\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3564 - val_loss: 0.4566\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3630 - val_loss: 0.4757\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3220 - val_loss: 0.4459\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3253 - val_loss: 0.4536\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3271 - val_loss: 0.4545\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3200 - val_loss: 0.4556\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3117 - val_loss: 0.4546\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3195 - val_loss: 0.4524\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3255 - val_loss: 0.4524\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-16:57:39] Training the entire fine-tuned model...\n",
      "[2022_04_09-16:57:47] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 995ms/step - loss: 0.3186 - val_loss: 0.4563\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3294 - val_loss: 0.4643\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3231 - val_loss: 0.4556\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3272 - val_loss: 0.4497\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3230 - val_loss: 0.4485\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3154 - val_loss: 0.4535\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.3224 - val_loss: 0.4562\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3166 - val_loss: 0.4524\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3191 - val_loss: 0.4529\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3230 - val_loss: 0.4525\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.3141 - val_loss: 0.4522\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_09-16:58:15] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-16:58:16] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:58:16] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 837ms/step - loss: 0.3282 - val_loss: 0.4484\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>496</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>134</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  496   4\n",
       "1  134  20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2247191011235955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2tgdyvl0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 31555... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▅▅▃▃▃▃▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂█▂▄▂▄▂▂▁▂▁▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>20</td></tr><tr><td>best_val_loss</td><td>0.44589</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32819</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.44836</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sparkling-morning-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/2tgdyvl0\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/2tgdyvl0</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_165640-2tgdyvl0/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2tgdyvl0). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/263nke5k\" target=\"_blank\">dark-feather-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-16:59:10] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:59:10] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-16:59:10] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 733ms/step - loss: 0.9510 - val_loss: 0.9399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.6492 - val_loss: 0.6600\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5696 - val_loss: 0.5933\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4911 - val_loss: 0.5297\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4852 - val_loss: 0.5851\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4097 - val_loss: 0.4893\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4026 - val_loss: 0.5261\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3962 - val_loss: 0.4837\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3977 - val_loss: 0.4735\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3845 - val_loss: 0.5316\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3747 - val_loss: 0.4662\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3575 - val_loss: 0.4766\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3555 - val_loss: 0.4727\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3483 - val_loss: 0.4605\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3475 - val_loss: 0.4661\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3445 - val_loss: 0.4523\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3377 - val_loss: 0.4957\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3396 - val_loss: 0.4487\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3485 - val_loss: 0.4862\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3323 - val_loss: 0.4703\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3273 - val_loss: 0.4610\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3265 - val_loss: 0.4537\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3246 - val_loss: 0.4465\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3211 - val_loss: 0.4567\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3042 - val_loss: 0.4578\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3149 - val_loss: 0.4503\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3129 - val_loss: 0.4488\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3241 - val_loss: 0.4493\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3184 - val_loss: 0.4485\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-16:59:52] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:00:01] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 854ms/step - loss: 0.3264 - val_loss: 0.4590\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.3206 - val_loss: 0.4437\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3145 - val_loss: 0.4537\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3136 - val_loss: 0.4605\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.3076 - val_loss: 0.4365\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3071 - val_loss: 0.4584\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.3095 - val_loss: 0.4380\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.2976 - val_loss: 0.4378\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.2842 - val_loss: 0.4448\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 393ms/step - loss: 0.2863 - val_loss: 0.4494\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.2907 - val_loss: 0.4400\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_09-17:00:29] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:00:29] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:00:47] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 818ms/step - loss: 0.3109 - val_loss: 0.4372\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>491</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>129</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  491   9\n",
       "1  129  25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26595744680851063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:263nke5k) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 31857... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██▁▁▁▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▂▃▂▂▂▂▂▁▂▂▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>4</td></tr><tr><td>best_val_loss</td><td>0.43649</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31093</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43716</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dark-feather-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/263nke5k\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/263nke5k</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_165853-263nke5k/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:263nke5k). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/1aj828us\" target=\"_blank\">proud-river-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-17:01:42] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:01:42] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:01:42] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 711ms/step - loss: 0.8780 - val_loss: 0.6350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5322 - val_loss: 0.7783\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5661 - val_loss: 0.5314\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4555 - val_loss: 0.6621\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4508 - val_loss: 0.5034\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4182 - val_loss: 0.5705\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3931 - val_loss: 0.4917\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3958 - val_loss: 0.5120\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3816 - val_loss: 0.4721\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3725 - val_loss: 0.4636\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3640 - val_loss: 0.4964\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3546 - val_loss: 0.4588\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3461 - val_loss: 0.4856\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3492 - val_loss: 0.4539\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3706 - val_loss: 0.4631\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3548 - val_loss: 0.5431\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3508 - val_loss: 0.4482\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3309 - val_loss: 0.4726\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3436 - val_loss: 0.5047\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3324 - val_loss: 0.4425\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3143 - val_loss: 0.5020\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3204 - val_loss: 0.4401\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3274 - val_loss: 0.5004\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3278 - val_loss: 0.4415\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3220 - val_loss: 0.4334\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3295 - val_loss: 0.6038\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3561 - val_loss: 0.4507\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3320 - val_loss: 0.4577\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3007 - val_loss: 0.4327\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.2954 - val_loss: 0.4511\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.2955 - val_loss: 0.4450\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3081 - val_loss: 0.4425\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.2947 - val_loss: 0.4402\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3008 - val_loss: 0.4440\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3040 - val_loss: 0.4433\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-17:02:31] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:02:39] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 853ms/step - loss: 0.3010 - val_loss: 0.4416\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.2900 - val_loss: 0.4767\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.2890 - val_loss: 0.4283\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.2936 - val_loss: 0.4553\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.2879 - val_loss: 0.4405\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 392ms/step - loss: 0.2818 - val_loss: 0.4289\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.2695 - val_loss: 0.4333\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.2699 - val_loss: 0.4316\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2613 - val_loss: 0.4518\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_09-17:03:04] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:03:04] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:03:06] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 824ms/step - loss: 0.3096 - val_loss: 0.4317\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>488</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>122</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  488  12\n",
       "1  122  32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32323232323232326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1aj828us) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 32178... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▂▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅█▃▆▃▄▂▃▂▂▂▂▂▂▃▁▃▁▂▁▂▁▁▅▂▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_val_loss</td><td>0.42825</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30964</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.4317</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">proud-river-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/1aj828us\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/1aj828us</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_170126-1aj828us/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1aj828us). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/1qn8mmg0\" target=\"_blank\">avid-dragon-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-17:03:59] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:03:59] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:03:59] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 726ms/step - loss: 0.9665 - val_loss: 0.7567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5942 - val_loss: 0.7018\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5986 - val_loss: 0.5712\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4859 - val_loss: 0.5357\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4921 - val_loss: 0.5442\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4418 - val_loss: 0.4953\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4334 - val_loss: 0.5268\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4038 - val_loss: 0.4809\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3924 - val_loss: 0.4863\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3832 - val_loss: 0.4826\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3693 - val_loss: 0.4668\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3767 - val_loss: 0.5056\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3579 - val_loss: 0.4587\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3648 - val_loss: 0.4740\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3482 - val_loss: 0.4784\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3500 - val_loss: 0.4530\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3583 - val_loss: 0.5345\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3584 - val_loss: 0.4771\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3547 - val_loss: 0.5423\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3572 - val_loss: 0.4575\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3401 - val_loss: 0.4470\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3276 - val_loss: 0.4726\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3334 - val_loss: 0.4711\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3326 - val_loss: 0.4461\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3351 - val_loss: 0.4526\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3255 - val_loss: 0.4675\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3218 - val_loss: 0.4471\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3279 - val_loss: 0.4472\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3291 - val_loss: 0.4518\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3218 - val_loss: 0.4558\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-17:04:43] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:04:58] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 831ms/step - loss: 0.4715 - val_loss: 0.4684\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.4240 - val_loss: 0.4808\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3774 - val_loss: 0.4662\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3479 - val_loss: 0.4756\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3420 - val_loss: 0.4534\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3164 - val_loss: 0.4571\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.2993 - val_loss: 0.4474\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.2915 - val_loss: 0.4545\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3005 - val_loss: 0.4648\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.2772 - val_loss: 0.4403\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.2589 - val_loss: 0.4444\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.2306 - val_loss: 0.4848\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2118 - val_loss: 0.4537\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.1993 - val_loss: 0.4825\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.1829 - val_loss: 0.4660\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.1761 - val_loss: 0.4814\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_09-17:05:35] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:05:35] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:05:35] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 837ms/step - loss: 0.2651 - val_loss: 0.4395\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>483</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>117</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  483  17\n",
       "1  117  37"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3557692307692308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1qn8mmg0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 32496... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇███▁▁▂▂▂▂▃▃▃▄▄▄▄▁</td></tr><tr><td>loss</td><td>█▅▅▄▄▃▃▃▃▃▃▃▂▂▃▃▃▂▂▂▂▂▂▂▂▂▄▃▂▂▂▂▂▂▂▁▁▁▁▂</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▄▃▃▂▂▂▂▂▂▁▂▁▃▂▃▁▂▂▁▁▂▁▁▁▂▂▂▁▁▁▁▂▁▂▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.43949</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.2651</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.43949</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">avid-dragon-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/1qn8mmg0\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/1qn8mmg0</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_170343-1qn8mmg0/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1qn8mmg0). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/7wwqx2ep\" target=\"_blank\">playful-butterfly-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-17:06:39] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:06:39] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:06:39] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 733ms/step - loss: 0.9492 - val_loss: 0.7932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5404 - val_loss: 0.6475\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5217 - val_loss: 0.5091\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4430 - val_loss: 0.5725\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4164 - val_loss: 0.4996\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4153 - val_loss: 0.5683\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4068 - val_loss: 0.4882\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4002 - val_loss: 0.5204\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3940 - val_loss: 0.4902\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3821 - val_loss: 0.4646\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3632 - val_loss: 0.4915\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.3530 - val_loss: 0.4596\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 220ms/step - loss: 0.3666 - val_loss: 0.4721\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 220ms/step - loss: 0.3540 - val_loss: 0.5065\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 223ms/step - loss: 0.3499 - val_loss: 0.4520\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 221ms/step - loss: 0.3362 - val_loss: 0.4515\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 225ms/step - loss: 0.3339 - val_loss: 0.4882\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3426 - val_loss: 0.4451\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3328 - val_loss: 0.4821\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3387 - val_loss: 0.4445\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3231 - val_loss: 0.4568\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3199 - val_loss: 0.4803\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 229ms/step - loss: 0.3299 - val_loss: 0.4448\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3039 - val_loss: 0.4435\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3240 - val_loss: 0.4374\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3172 - val_loss: 0.4753\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 230ms/step - loss: 0.3341 - val_loss: 0.4446\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 230ms/step - loss: 0.3201 - val_loss: 0.4325\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 229ms/step - loss: 0.3100 - val_loss: 0.4484\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3023 - val_loss: 0.4850\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3141 - val_loss: 0.4300\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3242 - val_loss: 0.4302\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3207 - val_loss: 0.4485\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.2994 - val_loss: 0.4774\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3048 - val_loss: 0.4304\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.2934 - val_loss: 0.4429\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.2820 - val_loss: 0.4547\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3032 - val_loss: 0.4288\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.2924 - val_loss: 0.4431\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.2784 - val_loss: 0.4412\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.2882 - val_loss: 0.4369\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.2883 - val_loss: 0.4419\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.2948 - val_loss: 0.4362\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.2809 - val_loss: 0.4352\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.2921 - val_loss: 0.4373\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.2886 - val_loss: 0.4367\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-17:07:39] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:07:47] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 841ms/step - loss: 0.2937 - val_loss: 0.4418\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 390ms/step - loss: 0.2980 - val_loss: 0.4417\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2848 - val_loss: 0.4378\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.2927 - val_loss: 0.4354\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.2807 - val_loss: 0.4379\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3026 - val_loss: 0.4398\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 394ms/step - loss: 0.2809 - val_loss: 0.4374\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2851 - val_loss: 0.4380\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2816 - val_loss: 0.4379\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.2779 - val_loss: 0.4380\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2871 - val_loss: 0.4375\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.2881 - val_loss: 0.4370\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_09-17:08:18] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:08:18] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:08:35] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 845ms/step - loss: 0.2938 - val_loss: 0.4352\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>493</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  493   7\n",
       "1  126  28"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2962962962962963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:7wwqx2ep) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 32826... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▄▄▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▂▄▃▂▂▂▂▁▂▁▁▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>37</td></tr><tr><td>best_val_loss</td><td>0.42882</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.29384</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.4352</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">playful-butterfly-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/7wwqx2ep\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/7wwqx2ep</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_170623-7wwqx2ep/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:7wwqx2ep). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/2yx5h53t\" target=\"_blank\">glorious-terrain-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-17:09:28] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:09:28] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:09:28] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 716ms/step - loss: 0.9845 - val_loss: 0.7131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5405 - val_loss: 0.6665\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.5168 - val_loss: 0.5095\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4523 - val_loss: 0.5728\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4295 - val_loss: 0.4935\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4107 - val_loss: 0.5082\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3943 - val_loss: 0.4953\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3723 - val_loss: 0.4929\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3783 - val_loss: 0.4938\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3727 - val_loss: 0.4709\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3712 - val_loss: 0.4869\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3552 - val_loss: 0.4657\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3496 - val_loss: 0.4705\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3443 - val_loss: 0.4578\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3450 - val_loss: 0.4730\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3380 - val_loss: 0.4632\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3287 - val_loss: 0.4473\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3355 - val_loss: 0.4497\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3372 - val_loss: 0.5130\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3398 - val_loss: 0.4431\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3407 - val_loss: 0.4427\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3367 - val_loss: 0.4690\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3231 - val_loss: 0.4727\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3204 - val_loss: 0.4397\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3153 - val_loss: 0.4754\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3080 - val_loss: 0.4382\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3189 - val_loss: 0.4868\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3146 - val_loss: 0.4462\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3163 - val_loss: 0.4361\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 229ms/step - loss: 0.3047 - val_loss: 0.4415\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3028 - val_loss: 0.4327\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3132 - val_loss: 0.4345\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3058 - val_loss: 0.4326\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3012 - val_loss: 0.5364\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3109 - val_loss: 0.4331\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.2975 - val_loss: 0.4309\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3086 - val_loss: 0.4449\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.2850 - val_loss: 0.4530\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3074 - val_loss: 0.4500\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3285 - val_loss: 0.4368\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.2813 - val_loss: 0.4294\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.2840 - val_loss: 0.4468\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.2944 - val_loss: 0.4394\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.2886 - val_loss: 0.4421\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.2817 - val_loss: 0.4342\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.2901 - val_loss: 0.4359\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3053 - val_loss: 0.4411\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.2884 - val_loss: 0.4374\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.2880 - val_loss: 0.4362\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-17:10:32] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:10:40] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 845ms/step - loss: 0.3052 - val_loss: 0.4284\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.2939 - val_loss: 0.4583\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2825 - val_loss: 0.4280\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.2985 - val_loss: 0.4519\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2790 - val_loss: 0.4500\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.2773 - val_loss: 0.4279\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2910 - val_loss: 0.4609\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.2684 - val_loss: 0.4265\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.2598 - val_loss: 0.4426\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.2602 - val_loss: 0.4347\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.2638 - val_loss: 0.4419\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2529 - val_loss: 0.4282\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 389ms/step - loss: 0.2502 - val_loss: 0.4348\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2436 - val_loss: 0.4460\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 391ms/step - loss: 0.2562 - val_loss: 0.4554\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.2487 - val_loss: 0.4354\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_09-17:11:18] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:11:18] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:11:27] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 1s/step - loss: 0.2801 - val_loss: 0.4266\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>478</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  478  22\n",
       "1  102  52"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45614035087719296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2yx5h53t) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 33224... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▅▃▃▃▂▂▂▂▂▃▁▂▁▁▂▁▁▁▄▁▁▂▁▁▁▁▁▁▂▁▂▂▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.42651</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.28006</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42656</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">glorious-terrain-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/2yx5h53t\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/2yx5h53t</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_170913-2yx5h53t/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2yx5h53t). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/si51s1jz\" target=\"_blank\">denim-dust-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-17:12:21] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:12:22] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:12:22] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 740ms/step - loss: 0.9163 - val_loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.5534 - val_loss: 0.7228\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5346 - val_loss: 0.5068\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4591 - val_loss: 0.6068\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4520 - val_loss: 0.4922\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4052 - val_loss: 0.5228\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3876 - val_loss: 0.4806\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3786 - val_loss: 0.4944\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3739 - val_loss: 0.4801\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3627 - val_loss: 0.4999\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3754 - val_loss: 0.4646\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3714 - val_loss: 0.4749\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3556 - val_loss: 0.4986\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3526 - val_loss: 0.4555\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3424 - val_loss: 0.4787\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3362 - val_loss: 0.4491\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3598 - val_loss: 0.4484\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3434 - val_loss: 0.5033\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3514 - val_loss: 0.4440\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3427 - val_loss: 0.4442\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3308 - val_loss: 0.4410\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3254 - val_loss: 0.4766\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3261 - val_loss: 0.4510\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3311 - val_loss: 0.4445\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3152 - val_loss: 0.4907\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3155 - val_loss: 0.4397\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3230 - val_loss: 0.4341\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3110 - val_loss: 0.4762\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3111 - val_loss: 0.4444\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3083 - val_loss: 0.4336\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3151 - val_loss: 0.4505\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3115 - val_loss: 0.4441\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3090 - val_loss: 0.4353\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3035 - val_loss: 0.4523\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3105 - val_loss: 0.4480\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3100 - val_loss: 0.4390\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3071 - val_loss: 0.4368\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3040 - val_loss: 0.4381\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-17:13:13] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:14:01] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 836ms/step - loss: 0.3202 - val_loss: 0.4494\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3243 - val_loss: 0.4315\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3048 - val_loss: 0.4635\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3008 - val_loss: 0.4309\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3019 - val_loss: 0.4555\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3065 - val_loss: 0.4365\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3040 - val_loss: 0.4393\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.2943 - val_loss: 0.4568\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2888 - val_loss: 0.4329\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2787 - val_loss: 0.4293\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.2881 - val_loss: 0.4316\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.2721 - val_loss: 0.4458\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2722 - val_loss: 0.4467\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.2693 - val_loss: 0.4345\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2722 - val_loss: 0.4335\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.2656 - val_loss: 0.4337\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2718 - val_loss: 0.4362\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.2695 - val_loss: 0.4385\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "[2022_04_09-17:14:42] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:14:42] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:14:42] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 828ms/step - loss: 0.2707 - val_loss: 0.4332\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>491</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  491   9\n",
       "1  124  30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31088082901554404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:si51s1jz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 33657... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇█▃▂▃▃▂▂▂▃▂▁▃▁▁▂▂▂▁▂▁▂▁▂▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>0.4293</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.2707</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.43316</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">denim-dust-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/si51s1jz\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/si51s1jz</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_171205-si51s1jz/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:si51s1jz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/22rugpgg\" target=\"_blank\">vibrant-sunset-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-17:15:35] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:15:35] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:15:35] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 715ms/step - loss: 0.8268 - val_loss: 0.6135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.5810 - val_loss: 0.7842\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5206 - val_loss: 0.5237\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4567 - val_loss: 0.5790\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4123 - val_loss: 0.4925\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4157 - val_loss: 0.5063\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3869 - val_loss: 0.4877\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3831 - val_loss: 0.4739\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3751 - val_loss: 0.5026\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3737 - val_loss: 0.4650\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3643 - val_loss: 0.5057\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3473 - val_loss: 0.4645\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3471 - val_loss: 0.4814\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3461 - val_loss: 0.4596\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3553 - val_loss: 0.4867\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3368 - val_loss: 0.4550\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3326 - val_loss: 0.4523\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3300 - val_loss: 0.4579\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3332 - val_loss: 0.4747\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3202 - val_loss: 0.4496\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3150 - val_loss: 0.4503\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3221 - val_loss: 0.4453\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3207 - val_loss: 0.4565\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3075 - val_loss: 0.4422\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3144 - val_loss: 0.4411\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3112 - val_loss: 0.4525\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.2967 - val_loss: 0.4444\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3110 - val_loss: 0.4410\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3069 - val_loss: 0.4466\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.2993 - val_loss: 0.4463\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.2942 - val_loss: 0.4364\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.2968 - val_loss: 0.4529\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3022 - val_loss: 0.4420\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3090 - val_loss: 0.4448\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3022 - val_loss: 0.4424\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.2867 - val_loss: 0.4409\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3048 - val_loss: 0.4410\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.2835 - val_loss: 0.4428\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.2953 - val_loss: 0.4425\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-17:16:29] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:17:15] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 825ms/step - loss: 0.4349 - val_loss: 0.4600\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3718 - val_loss: 0.4421\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3284 - val_loss: 0.5194\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3156 - val_loss: 0.4460\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.2971 - val_loss: 0.4916\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.2987 - val_loss: 0.4327\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.2625 - val_loss: 0.4495\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.2669 - val_loss: 0.4385\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.2578 - val_loss: 0.4277\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.2489 - val_loss: 0.4297\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.2446 - val_loss: 0.4763\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.2308 - val_loss: 0.5276\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2026 - val_loss: 0.5470\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.1853 - val_loss: 0.4438\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.1720 - val_loss: 0.4531\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.1546 - val_loss: 0.4758\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.1482 - val_loss: 0.4826\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_09-17:17:54] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:17:54] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:18:01] Validation set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 838ms/step - loss: 0.2427 - val_loss: 0.4424\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>491</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  491   9\n",
       "1  124  30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31088082901554404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "for pat in patience:\n",
    "    for lr in learning_rate:\n",
    "        finetune_by_settings(pat, lr, \"Split 4a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ffa85cd4-1674-44cb-9f4b-47fed90ec037",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/iq605716\" target=\"_blank\">proud-dragon-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-17:22:57] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:22:57] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:22:57] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 54s 581ms/step - loss: 0.8790 - val_loss: 0.4708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.6815 - val_loss: 0.5763\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5838 - val_loss: 0.4740\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5411 - val_loss: 0.5429\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-17:23:56] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:24:08] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5247WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1173s vs `on_train_batch_end` time: 0.1209s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1173s vs `on_train_batch_end` time: 0.1209s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 682ms/step - loss: 0.5247 - val_loss: 0.4662\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.5210 - val_loss: 0.4659\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.5201 - val_loss: 0.4630\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.5215 - val_loss: 0.4605\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.5191 - val_loss: 0.4575\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.5173 - val_loss: 0.4555\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.5183 - val_loss: 0.4542\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.5246 - val_loss: 0.4540\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.5200 - val_loss: 0.4555\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 343ms/step - loss: 0.5233 - val_loss: 0.4566\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.5164 - val_loss: 0.4576\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_09-17:24:37] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:24:37] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:24:37] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5157WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0956s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0956s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 673ms/step - loss: 0.5177 - val_loss: 0.4539\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>529</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  529  0\n",
       "1  108  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:iq605716) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 34491... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▁▂▂▃▄▅▅▆▇▇█▁</td></tr><tr><td>loss</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂█▂▆▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.45389</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.51768</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.45389</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">proud-dragon-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/iq605716\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/iq605716</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_172247-iq605716/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:iq605716). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/uualznsv\" target=\"_blank\">sandy-planet-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-17:25:28] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:25:28] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:25:28] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 571ms/step - loss: 0.9884 - val_loss: 0.8532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.7348 - val_loss: 0.4670\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.5681 - val_loss: 0.4281\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5252 - val_loss: 0.5680\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5567 - val_loss: 0.4570\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4968 - val_loss: 0.5835\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-17:25:45] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:25:55] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4882WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1216s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1216s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 699ms/step - loss: 0.4896 - val_loss: 0.4518\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4907 - val_loss: 0.4375\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4800 - val_loss: 0.4252\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4774 - val_loss: 0.4387\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4732 - val_loss: 0.4328\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4716 - val_loss: 0.4195\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4676 - val_loss: 0.4244\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.4605 - val_loss: 0.4311\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4517 - val_loss: 0.4116\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.4455 - val_loss: 0.4132\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4344 - val_loss: 0.4121\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4168 - val_loss: 0.4165\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_09-17:26:27] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:26:27] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:26:27] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4424WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0978s vs `on_train_batch_end` time: 0.1227s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0978s vs `on_train_batch_end` time: 0.1227s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 674ms/step - loss: 0.4387 - val_loss: 0.4123\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>521</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  521  8\n",
       "1  105  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05042016806722689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:uualznsv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 34667... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▄▁▂▂▃▄▄▅▅▆▇▇█▁</td></tr><tr><td>loss</td><td>█▅▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▁▃▂▄▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.41161</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.43875</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41233</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sandy-planet-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/uualznsv\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/uualznsv</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_172513-uualznsv/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:uualznsv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/1yigexn9\" target=\"_blank\">breezy-durian-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-17:27:21] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:27:21] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:27:21] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 564ms/step - loss: 0.9048 - val_loss: 0.7479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.7445 - val_loss: 0.4370\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.6348 - val_loss: 0.4283\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.5251 - val_loss: 0.4871\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4881 - val_loss: 0.4153\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4796 - val_loss: 0.5000\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4763 - val_loss: 0.4090\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4636 - val_loss: 0.4435\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4476 - val_loss: 0.4009\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4436 - val_loss: 0.4027\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4501 - val_loss: 0.4195\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4368 - val_loss: 0.4038\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-17:27:44] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:27:52] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4465WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0964s vs `on_train_batch_end` time: 0.1238s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0964s vs `on_train_batch_end` time: 0.1238s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 708ms/step - loss: 0.4487 - val_loss: 0.4005\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4485 - val_loss: 0.4159\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4274 - val_loss: 0.4109\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4474 - val_loss: 0.4227\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_09-17:28:09] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:28:09] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:28:09] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4393WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0952s vs `on_train_batch_end` time: 0.1246s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0952s vs `on_train_batch_end` time: 0.1246s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 703ms/step - loss: 0.4399 - val_loss: 0.4003\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>520</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  520  9\n",
       "1  104  4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06611570247933884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1yigexn9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 34856... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█▁▂▂▃▁</td></tr><tr><td>loss</td><td>█▆▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▁▃▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.40032</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.43986</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.40032</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">breezy-durian-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/1yigexn9\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/1yigexn9</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_172704-1yigexn9/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1yigexn9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/3ca7hirr\" target=\"_blank\">volcanic-wood-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-17:29:02] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:29:02] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:29:02] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 571ms/step - loss: 1.0263 - val_loss: 1.1799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.7311 - val_loss: 0.5796\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.6268 - val_loss: 0.4795\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.5593 - val_loss: 0.4428\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5221 - val_loss: 0.4226\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4706 - val_loss: 0.4370\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4593 - val_loss: 0.4257\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4614 - val_loss: 0.4040\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4459 - val_loss: 0.3997\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4459 - val_loss: 0.4188\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4327 - val_loss: 0.4026\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4237 - val_loss: 0.4009\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-17:29:26] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:29:40] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5546WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0954s vs `on_train_batch_end` time: 0.1235s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0954s vs `on_train_batch_end` time: 0.1235s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 706ms/step - loss: 0.5525 - val_loss: 0.4014\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.4700 - val_loss: 0.4042\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4854 - val_loss: 0.4126\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4412 - val_loss: 0.3991\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4482 - val_loss: 0.4337\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4410 - val_loss: 0.4020\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4531 - val_loss: 0.4382\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_09-17:30:01] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:30:01] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:30:01] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4324WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1004s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1004s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 700ms/step - loss: 0.4296 - val_loss: 0.4038\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>516</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  516  13\n",
       "1  102   6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09448818897637795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3ca7hirr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 35036... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█▁▂▂▃▄▄▅▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▁▁▁▁▁▁▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>0.39912</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.42957</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.40377</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">volcanic-wood-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/3ca7hirr\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/3ca7hirr</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_172847-3ca7hirr/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3ca7hirr). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/3j7id2qm\" target=\"_blank\">ancient-valley-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-17:30:55] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:30:55] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:30:55] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 590ms/step - loss: 0.9398 - val_loss: 0.9604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.6932 - val_loss: 0.5022\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.5938 - val_loss: 0.4416\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.5174 - val_loss: 0.5099\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.5201 - val_loss: 0.4230\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4692 - val_loss: 0.4910\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4686 - val_loss: 0.4065\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4658 - val_loss: 0.4110\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4389 - val_loss: 0.4138\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4437 - val_loss: 0.4018\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4348 - val_loss: 0.3964\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4743 - val_loss: 0.4227\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4383 - val_loss: 0.3959\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4220 - val_loss: 0.3964\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4226 - val_loss: 0.3885\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4229 - val_loss: 0.3913\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4037 - val_loss: 0.4050\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4144 - val_loss: 0.4456\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4331 - val_loss: 0.3800\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4022 - val_loss: 0.3808\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4080 - val_loss: 0.3983\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3934 - val_loss: 0.3794\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3896 - val_loss: 0.4036\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4079 - val_loss: 0.3862\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4035 - val_loss: 0.3833\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4048 - val_loss: 0.3857\n",
      "[2022_04_09-17:31:35] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:31:54] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4081WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0968s vs `on_train_batch_end` time: 0.1243s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0968s vs `on_train_batch_end` time: 0.1243s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 658ms/step - loss: 0.4082 - val_loss: 0.3896\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3987 - val_loss: 0.3897\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4128 - val_loss: 0.3941\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4005 - val_loss: 0.3888\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3961 - val_loss: 0.3837\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3811 - val_loss: 0.3792\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3948 - val_loss: 0.3797\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3915 - val_loss: 0.3840\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3982 - val_loss: 0.3820\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3873 - val_loss: 0.3825\n",
      "[2022_04_09-17:32:20] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:32:20] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:32:20] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3958WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1003s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1003s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 688ms/step - loss: 0.3958 - val_loss: 0.3797\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>510</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  510  19\n",
       "1   89  19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2602739726027397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3j7id2qm) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 35237... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▂▂▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.37917</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.39584</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37969</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">ancient-valley-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/3j7id2qm\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/3j7id2qm</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_173039-3j7id2qm/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3j7id2qm). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/xxj6t3tk\" target=\"_blank\">major-cosmos-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-17:33:15] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:33:15] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:33:15] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 572ms/step - loss: 0.9796 - val_loss: 0.7330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.6496 - val_loss: 0.4352\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5586 - val_loss: 0.4319\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5078 - val_loss: 0.4981\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.5081 - val_loss: 0.4373\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4765 - val_loss: 0.4085\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4544 - val_loss: 0.4543\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4662 - val_loss: 0.4008\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4473 - val_loss: 0.4415\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4573 - val_loss: 0.4020\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4528 - val_loss: 0.3908\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4710 - val_loss: 0.4208\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4241 - val_loss: 0.4625\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4563 - val_loss: 0.4297\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4758 - val_loss: 0.4018\n",
      "[2022_04_09-17:33:41] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:33:49] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4313WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1222s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1222s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 670ms/step - loss: 0.4301 - val_loss: 0.4042\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.4243 - val_loss: 0.3918\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4270 - val_loss: 0.3901\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4166 - val_loss: 0.4090\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4090 - val_loss: 0.3861\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4108 - val_loss: 0.3929\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4020 - val_loss: 0.4064\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4006 - val_loss: 0.3833\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3936 - val_loss: 0.3902\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3896 - val_loss: 0.3843\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3890 - val_loss: 0.3873\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3851 - val_loss: 0.3857\n",
      "[2022_04_09-17:34:21] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:34:21] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:34:21] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3908WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0956s vs `on_train_batch_end` time: 0.1242s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0956s vs `on_train_batch_end` time: 0.1242s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 672ms/step - loss: 0.3907 - val_loss: 0.3837\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>515</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  515  14\n",
       "1   95  13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1925925925925926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:xxj6t3tk) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 35518... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█▁▁▂▃▃▃▄▅▅▅▆▇▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▂▂▂▁▂▁▁▂▃▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.38329</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.39074</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.38366</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">major-cosmos-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/xxj6t3tk\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/xxj6t3tk</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_173300-xxj6t3tk/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:xxj6t3tk). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/3pflob8e\" target=\"_blank\">easy-night-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-17:35:31] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:35:31] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:35:31] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 568ms/step - loss: 0.8316 - val_loss: 0.7136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.6507 - val_loss: 0.4898\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.5513 - val_loss: 0.4772\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.5333 - val_loss: 0.4468\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.4890 - val_loss: 0.4713\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.4778 - val_loss: 0.4139\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4720 - val_loss: 0.4745\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 0.5251 - val_loss: 0.5055\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 0.4613 - val_loss: 0.4186\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 0.4424 - val_loss: 0.4154\n",
      "[2022_04_09-17:35:53] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:36:01] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4531WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0971s vs `on_train_batch_end` time: 0.1240s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0971s vs `on_train_batch_end` time: 0.1240s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 667ms/step - loss: 0.4557 - val_loss: 0.4999\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4774 - val_loss: 0.4085\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4567 - val_loss: 0.4199\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.4388 - val_loss: 0.4136\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4282 - val_loss: 0.4091\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.4232 - val_loss: 0.4053\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4296 - val_loss: 0.4149\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4278 - val_loss: 0.4170\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4273 - val_loss: 0.4134\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4183 - val_loss: 0.4123\n",
      "[2022_04_09-17:36:28] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:36:28] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:36:28] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4286WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0940s vs `on_train_batch_end` time: 0.1273s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0940s vs `on_train_batch_end` time: 0.1273s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 673ms/step - loss: 0.4269 - val_loss: 0.4068\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>513</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  513  16\n",
       "1   99   9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13533834586466165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3pflob8e) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 35754... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█▁▂▃▃▄▅▆▆▇█▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▂▃▂▁▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▁▃▃▁▁▃▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.40534</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.42693</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.4068</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">easy-night-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/3pflob8e\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/3pflob8e</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_173515-3pflob8e/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3pflob8e). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/am0668wt\" target=\"_blank\">exalted-lake-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-17:37:21] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:37:21] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:37:21] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 573ms/step - loss: 0.8241 - val_loss: 0.5921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.6161 - val_loss: 0.5308\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5331 - val_loss: 0.4581\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.5192 - val_loss: 0.4286\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.5814 - val_loss: 0.6019\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5168 - val_loss: 0.4159\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4813 - val_loss: 0.4081\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4614 - val_loss: 0.4573\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4681 - val_loss: 0.4743\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4738 - val_loss: 0.4788\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.5501 - val_loss: 0.4040\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4613 - val_loss: 0.4878\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4567 - val_loss: 0.3983\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4427 - val_loss: 0.4172\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4302 - val_loss: 0.4084\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4274 - val_loss: 0.4012\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4249 - val_loss: 0.3993\n",
      "[2022_04_09-17:37:50] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:38:00] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4812WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 670ms/step - loss: 0.4771 - val_loss: 0.3990\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.4190 - val_loss: 0.3873\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.4155 - val_loss: 0.4008\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 308ms/step - loss: 0.4072 - val_loss: 0.4027\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.3951 - val_loss: 0.4106\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4177 - val_loss: 0.4133\n",
      "[2022_04_09-17:38:19] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:38:19] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:38:26] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4220WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0986s vs `on_train_batch_end` time: 0.1212s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0986s vs `on_train_batch_end` time: 0.1212s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 687ms/step - loss: 0.4212 - val_loss: 0.4009\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>507</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  507  22\n",
       "1   89  19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2550335570469799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:am0668wt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 35954... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██▁▁▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▅▃▃▄▃▂▂▂▂▄▂▂▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▃▂█▂▂▃▄▄▂▄▁▂▂▁▁▁▁▁▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.38733</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.42122</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.40093</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">exalted-lake-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/am0668wt\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/am0668wt</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_173706-am0668wt/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:am0668wt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/1waglaf7\" target=\"_blank\">vibrant-glade-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-17:39:21] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:39:21] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:39:21] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 547ms/step - loss: 0.8813 - val_loss: 1.3722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.8270 - val_loss: 0.6514\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.6990 - val_loss: 0.4738\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 0.5945 - val_loss: 0.4451\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.6084 - val_loss: 0.5054\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.7313 - val_loss: 0.4767\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.6031 - val_loss: 0.4671\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.5647 - val_loss: 0.4207\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4659 - val_loss: 0.4939\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4907 - val_loss: 0.4297\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4549 - val_loss: 0.4138\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4590 - val_loss: 0.4253\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4567 - val_loss: 0.4437\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 180ms/step - loss: 0.4493 - val_loss: 0.4103\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 0.4511 - val_loss: 0.4091\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4416 - val_loss: 0.4166\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4461 - val_loss: 0.4071\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4406 - val_loss: 0.4127\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4436 - val_loss: 0.4111\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4474 - val_loss: 0.4060\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4320 - val_loss: 0.4198\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4525 - val_loss: 0.4177\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4349 - val_loss: 0.4000\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4296 - val_loss: 0.4420\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 177ms/step - loss: 0.4490 - val_loss: 0.4078\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 175ms/step - loss: 0.4428 - val_loss: 0.3960\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 177ms/step - loss: 0.4451 - val_loss: 0.4473\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4302 - val_loss: 0.3960\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4342 - val_loss: 0.3981\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4173 - val_loss: 0.4009\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.4186 - val_loss: 0.3993\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.4270 - val_loss: 0.4003\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 180ms/step - loss: 0.4207 - val_loss: 0.4012\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 181ms/step - loss: 0.4155 - val_loss: 0.4017\n",
      "[2022_04_09-17:40:07] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:40:20] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4370WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0953s vs `on_train_batch_end` time: 0.1259s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0953s vs `on_train_batch_end` time: 0.1259s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 694ms/step - loss: 0.4364 - val_loss: 0.3974\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4286 - val_loss: 0.4092\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 309ms/step - loss: 0.4229 - val_loss: 0.4043\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.4315 - val_loss: 0.3986\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 309ms/step - loss: 0.4223 - val_loss: 0.3988\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.4256 - val_loss: 0.4004\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4203 - val_loss: 0.4025\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_09-17:40:40] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:40:40] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:40:40] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4156WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1216s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1216s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 684ms/step - loss: 0.4188 - val_loss: 0.3979\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>512</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  512  17\n",
       "1   97  11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16176470588235295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1waglaf7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 36169... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▇▅▄▄▆▄▃▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>27</td></tr><tr><td>best_val_loss</td><td>0.39603</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.41884</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.39785</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">vibrant-glade-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/1waglaf7\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/1waglaf7</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_173904-1waglaf7/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1waglaf7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/2or1kl6t\" target=\"_blank\">kind-field-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-17:41:35] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:41:35] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:41:35] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 558ms/step - loss: 0.9627 - val_loss: 0.9321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.6830 - val_loss: 0.4828\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.5887 - val_loss: 0.4431\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.5013 - val_loss: 0.5086\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.5022 - val_loss: 0.4210\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4904 - val_loss: 0.4119\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4621 - val_loss: 0.4129\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4486 - val_loss: 0.4035\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4616 - val_loss: 0.4299\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4311 - val_loss: 0.4123\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4397 - val_loss: 0.3968\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 177ms/step - loss: 0.4395 - val_loss: 0.5503\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 0.4825 - val_loss: 0.3960\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 175ms/step - loss: 0.4686 - val_loss: 0.4359\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4728 - val_loss: 0.4150\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4481 - val_loss: 0.5008\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 175ms/step - loss: 0.4669 - val_loss: 0.3878\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 0.4155 - val_loss: 0.3822\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4139 - val_loss: 0.3899\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4179 - val_loss: 0.4030\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 181ms/step - loss: 0.4028 - val_loss: 0.3818\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4135 - val_loss: 0.4052\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4042 - val_loss: 0.3802\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4154 - val_loss: 0.3915\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3968 - val_loss: 0.3884\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4075 - val_loss: 0.3823\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3929 - val_loss: 0.3935\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3917 - val_loss: 0.3924\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3904 - val_loss: 0.3830\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-17:42:16] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:42:33] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4045WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0964s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0964s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 682ms/step - loss: 0.4030 - val_loss: 0.3806\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 308ms/step - loss: 0.4105 - val_loss: 0.3821\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.3894 - val_loss: 0.3884\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3938 - val_loss: 0.3789\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4137 - val_loss: 0.3990\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3957 - val_loss: 0.3781\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3891 - val_loss: 0.3770\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3812 - val_loss: 0.4235\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3794 - val_loss: 0.3738\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3936 - val_loss: 0.3832\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3830 - val_loss: 0.3974\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3778 - val_loss: 0.3726\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3656 - val_loss: 0.3913\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3797 - val_loss: 0.3874\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3649 - val_loss: 0.3724\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3569 - val_loss: 0.4053\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3546 - val_loss: 0.3738\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3794 - val_loss: 0.3765\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3409 - val_loss: 0.3894\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3320 - val_loss: 0.3893\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3446 - val_loss: 0.3805\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_09-17:43:20] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:43:20] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:43:20] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3592WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1219s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1219s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 662ms/step - loss: 0.3617 - val_loss: 0.3724\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>506</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  506  23\n",
       "1   87  21"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27631578947368424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2or1kl6t) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 36480... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▇▇▇▇█▁▁▂▂▂▃▃▃▃▄▄▅▅▅▅▆▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▁▂▁▂▁▃▁▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.3724</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36172</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.3724</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">kind-field-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/2or1kl6t\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/2or1kl6t</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_174118-2or1kl6t/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2or1kl6t). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/34c02zic\" target=\"_blank\">driven-sea-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-17:44:54] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:44:54] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:44:54] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 571ms/step - loss: 0.8470 - val_loss: 0.6149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.6706 - val_loss: 0.4347\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5361 - val_loss: 0.4264\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4792 - val_loss: 0.4379\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4780 - val_loss: 0.4633\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4828 - val_loss: 0.4182\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4631 - val_loss: 0.4058\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4560 - val_loss: 0.4204\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4425 - val_loss: 0.4475\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4525 - val_loss: 0.3960\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4579 - val_loss: 0.3936\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4329 - val_loss: 0.4101\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4620 - val_loss: 0.5389\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4499 - val_loss: 0.3891\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4290 - val_loss: 0.3879\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4475 - val_loss: 0.4246\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4289 - val_loss: 0.3934\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4178 - val_loss: 0.3801\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4044 - val_loss: 0.3972\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3904 - val_loss: 0.3935\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3899 - val_loss: 0.3865\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4282 - val_loss: 0.3901\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3987 - val_loss: 0.3957\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4032 - val_loss: 0.3731\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3962 - val_loss: 0.3869\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3840 - val_loss: 0.3778\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3964 - val_loss: 0.3884\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3966 - val_loss: 0.3846\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3818 - val_loss: 0.3746\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3880 - val_loss: 0.3725\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3791 - val_loss: 0.3785\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3869 - val_loss: 0.3827\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3806 - val_loss: 0.3744\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3774 - val_loss: 0.3734\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3815 - val_loss: 0.3739\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3889 - val_loss: 0.3741\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_09-17:45:44] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:45:53] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3966WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0960s vs `on_train_batch_end` time: 0.1237s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0960s vs `on_train_batch_end` time: 0.1237s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 680ms/step - loss: 0.4024 - val_loss: 0.4048\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4004 - val_loss: 0.3693\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3745 - val_loss: 0.3774\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3822 - val_loss: 0.3780\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3625 - val_loss: 0.3702\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3687 - val_loss: 0.3734\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3679 - val_loss: 0.3869\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3555 - val_loss: 0.3766\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_09-17:46:16] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:46:16] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:46:16] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3781WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0954s vs `on_train_batch_end` time: 0.1268s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0954s vs `on_train_batch_end` time: 0.1268s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 680ms/step - loss: 0.3780 - val_loss: 0.3702\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>508</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  508  21\n",
       "1   84  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3137254901960784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:34c02zic) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 36845... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▁▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▃▄▂▂▂▂▂▂▆▂▂▃▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.36927</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37796</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.37018</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">driven-sea-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/34c02zic\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/34c02zic</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_174439-34c02zic/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:34c02zic). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/12czo9bj\" target=\"_blank\">hardy-terrain-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-17:47:11] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:47:11] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:47:11] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 590ms/step - loss: 0.8894 - val_loss: 0.6605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.6528 - val_loss: 0.4356\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5448 - val_loss: 0.4266\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4965 - val_loss: 0.4851\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4866 - val_loss: 0.4124\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4691 - val_loss: 0.4236\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4580 - val_loss: 0.4095\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4475 - val_loss: 0.4024\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4463 - val_loss: 0.4038\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4346 - val_loss: 0.4879\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4461 - val_loss: 0.3993\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4273 - val_loss: 0.4109\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4664 - val_loss: 0.4303\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4501 - val_loss: 0.4777\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4330 - val_loss: 0.3904\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4439 - val_loss: 0.3919\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4058 - val_loss: 0.4388\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4429 - val_loss: 0.3898\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4066 - val_loss: 0.3875\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4122 - val_loss: 0.4055\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4063 - val_loss: 0.3878\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4099 - val_loss: 0.3920\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3897 - val_loss: 0.3923\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3987 - val_loss: 0.3913\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4096 - val_loss: 0.3959\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-17:47:50] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:47:58] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5813WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1002s vs `on_train_batch_end` time: 0.1246s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1002s vs `on_train_batch_end` time: 0.1246s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 676ms/step - loss: 0.5795 - val_loss: 0.3912\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4567 - val_loss: 0.3947\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4434 - val_loss: 0.4136\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4191 - val_loss: 0.3878\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4158 - val_loss: 0.4236\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4022 - val_loss: 0.3811\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3669 - val_loss: 0.3753\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3908 - val_loss: 0.5027\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3880 - val_loss: 0.3699\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3468 - val_loss: 0.3804\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3566 - val_loss: 0.3788\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3286 - val_loss: 0.3964\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3215 - val_loss: 0.3687\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.2910 - val_loss: 0.3649\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.2859 - val_loss: 0.3654\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.2685 - val_loss: 0.3739\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.2690 - val_loss: 0.3674\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.2664 - val_loss: 0.3660\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.2460 - val_loss: 0.3670\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.2488 - val_loss: 0.3700\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "[2022_04_09-17:48:44] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:48:44] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:48:44] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.2896WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0950s vs `on_train_batch_end` time: 0.1270s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0950s vs `on_train_batch_end` time: 0.1270s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 687ms/step - loss: 0.2891 - val_loss: 0.3666\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>494</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  494  35\n",
       "1   75  33"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:12czo9bj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 37167... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇</td></tr><tr><td>loss</td><td>█▅▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▅▃▃▃▃▂▃▃▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▄▂▂▂▂▄▂▂▃▄▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▄▁▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>13</td></tr><tr><td>best_val_loss</td><td>0.36492</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.28906</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.3666</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">hardy-terrain-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/12czo9bj\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/12czo9bj</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_174654-12czo9bj/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:12czo9bj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/29bpsf7n\" target=\"_blank\">efficient-cloud-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-17:49:38] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:49:38] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:49:38] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 579ms/step - loss: 0.8873 - val_loss: 0.4887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.6215 - val_loss: 0.5977\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5988 - val_loss: 0.5325\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.6058 - val_loss: 0.4518\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5484 - val_loss: 0.4476\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.5268 - val_loss: 0.4996\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5930 - val_loss: 0.6074\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4993 - val_loss: 0.4885\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5286 - val_loss: 0.4983\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4594 - val_loss: 0.4046\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4868 - val_loss: 0.4018\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4481 - val_loss: 0.4248\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4432 - val_loss: 0.4008\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4415 - val_loss: 0.4095\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4261 - val_loss: 0.4274\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4287 - val_loss: 0.3987\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4414 - val_loss: 0.3968\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4286 - val_loss: 0.4237\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4245 - val_loss: 0.3957\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4295 - val_loss: 0.4030\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4283 - val_loss: 0.4006\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4303 - val_loss: 0.3931\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4329 - val_loss: 0.3946\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4226 - val_loss: 0.4094\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4169 - val_loss: 0.3967\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4278 - val_loss: 0.3940\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4116 - val_loss: 0.3967\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4211 - val_loss: 0.3917\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4256 - val_loss: 0.3927\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4120 - val_loss: 0.4062\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4187 - val_loss: 0.4111\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4200 - val_loss: 0.3942\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4218 - val_loss: 0.3933\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4178 - val_loss: 0.3929\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4092 - val_loss: 0.3931\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4236 - val_loss: 0.3933\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_09-17:50:27] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:50:36] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4318WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0967s vs `on_train_batch_end` time: 0.1258s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0967s vs `on_train_batch_end` time: 0.1258s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 718ms/step - loss: 0.4296 - val_loss: 0.3933\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4123 - val_loss: 0.4034\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4230 - val_loss: 0.4009\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4200 - val_loss: 0.3928\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4106 - val_loss: 0.3910\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4157 - val_loss: 0.3922\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4039 - val_loss: 0.3974\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4104 - val_loss: 0.3975\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4060 - val_loss: 0.3941\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4086 - val_loss: 0.3953\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4047 - val_loss: 0.3975\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4117 - val_loss: 0.3968\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4030 - val_loss: 0.3964\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_09-17:51:10] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:51:10] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:51:10] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4073WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1238s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1238s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 685ms/step - loss: 0.4059 - val_loss: 0.3909\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>513</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  513  16\n",
       "1   94  14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2028985507246377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:29bpsf7n) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 37490... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▄▄▄▃▄▂▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▆▃▅█▄▄▁▂▁▂▁▁▂▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.39094</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.40587</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.39094</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">efficient-cloud-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/29bpsf7n\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/29bpsf7n</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_174922-29bpsf7n/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:29bpsf7n). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/3kgedl1m\" target=\"_blank\">dulcet-capybara-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-17:52:05] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:52:05] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:52:05] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 570ms/step - loss: 1.0845 - val_loss: 0.9874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.6758 - val_loss: 0.4772\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5902 - val_loss: 0.4290\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5721 - val_loss: 0.5580\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.5690 - val_loss: 0.4551\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4999 - val_loss: 0.6714\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.5800 - val_loss: 0.4239\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4827 - val_loss: 0.4802\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4591 - val_loss: 0.4023\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4495 - val_loss: 0.4187\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4388 - val_loss: 0.4001\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4389 - val_loss: 0.4426\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4244 - val_loss: 0.3926\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4174 - val_loss: 0.4119\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4067 - val_loss: 0.3881\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4243 - val_loss: 0.4249\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4208 - val_loss: 0.3905\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4021 - val_loss: 0.3828\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3949 - val_loss: 0.3830\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4183 - val_loss: 0.4824\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4333 - val_loss: 0.3802\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4142 - val_loss: 0.3829\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4110 - val_loss: 0.3860\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4057 - val_loss: 0.4674\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4026 - val_loss: 0.3757\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4123 - val_loss: 0.3722\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3933 - val_loss: 0.4640\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4063 - val_loss: 0.3807\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4160 - val_loss: 0.3826\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3905 - val_loss: 0.3747\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3867 - val_loss: 0.3802\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3744 - val_loss: 0.3926\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3810 - val_loss: 0.3800\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3753 - val_loss: 0.3755\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_09-17:52:53] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:53:01] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3631WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0989s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0989s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 693ms/step - loss: 0.3634 - val_loss: 0.3989\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3832 - val_loss: 0.3726\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3752 - val_loss: 0.3894\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3850 - val_loss: 0.3730\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3776 - val_loss: 0.3801\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3873 - val_loss: 0.3729\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3710 - val_loss: 0.3785\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3654 - val_loss: 0.3784\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3575 - val_loss: 0.3762\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3438 - val_loss: 0.3849\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_09-17:53:28] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:53:28] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:53:28] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3696WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0959s vs `on_train_batch_end` time: 0.1236s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0959s vs `on_train_batch_end` time: 0.1236s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 710ms/step - loss: 0.3772 - val_loss: 0.3759\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>497</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  497  32\n",
       "1   82  26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3132530120481928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3kgedl1m) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 37842... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▂▃▂▂▂▂▂▂▁▂▂▁▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▂▄▂▂▂▁▂▁▁▁▂▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>25</td></tr><tr><td>best_val_loss</td><td>0.37216</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37723</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.3759</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dulcet-capybara-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/3kgedl1m\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/3kgedl1m</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_175149-3kgedl1m/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3kgedl1m). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/1nn5thbq\" target=\"_blank\">fanciful-terrain-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-17:54:23] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:54:23] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:54:23] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 560ms/step - loss: 0.9404 - val_loss: 0.9079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.6727 - val_loss: 0.4309\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5459 - val_loss: 0.4397\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5427 - val_loss: 0.4532\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4814 - val_loss: 0.4186\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4775 - val_loss: 0.4336\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4623 - val_loss: 0.4297\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4535 - val_loss: 0.3993\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4494 - val_loss: 0.3944\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4461 - val_loss: 0.4145\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4468 - val_loss: 0.3917\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4192 - val_loss: 0.4354\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4370 - val_loss: 0.3879\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4172 - val_loss: 0.4090\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4276 - val_loss: 0.4224\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4179 - val_loss: 0.3884\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4016 - val_loss: 0.3839\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4071 - val_loss: 0.3870\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4112 - val_loss: 0.3779\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4034 - val_loss: 0.3754\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4259 - val_loss: 0.5353\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4371 - val_loss: 0.3760\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4251 - val_loss: 0.4063\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4284 - val_loss: 0.3744\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3985 - val_loss: 0.3943\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3903 - val_loss: 0.3841\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4267 - val_loss: 0.3712\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4211 - val_loss: 0.6304\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.5101 - val_loss: 0.3699\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3940 - val_loss: 0.3771\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.3672 - val_loss: 0.3893\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3936 - val_loss: 0.6460\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4898 - val_loss: 0.3703\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3918 - val_loss: 0.4107\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3781 - val_loss: 0.3714\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 181ms/step - loss: 0.3564 - val_loss: 0.3855\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.3782 - val_loss: 0.3748\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_09-17:55:14] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:55:22] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3664WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0945s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0945s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 669ms/step - loss: 0.3707 - val_loss: 0.3746\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 309ms/step - loss: 0.3793 - val_loss: 0.3924\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3685 - val_loss: 0.3700\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3753 - val_loss: 0.4017\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.3552 - val_loss: 0.3753\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.3819 - val_loss: 0.3951\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.3526 - val_loss: 0.3685\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3870 - val_loss: 0.3700\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3619 - val_loss: 0.3792\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.3532 - val_loss: 0.3645\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.3356 - val_loss: 0.3859\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 309ms/step - loss: 0.3166 - val_loss: 0.3621\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.3132 - val_loss: 0.3723\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.3202 - val_loss: 0.3632\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3151 - val_loss: 0.3892\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.2973 - val_loss: 0.3725\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.2882 - val_loss: 0.3745\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3061 - val_loss: 0.3733\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 343ms/step - loss: 0.2958 - val_loss: 0.3698\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3005 - val_loss: 0.3785\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_09-17:56:07] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:56:07] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:56:07] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3271WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0972s vs `on_train_batch_end` time: 0.1252s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0972s vs `on_train_batch_end` time: 0.1252s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 687ms/step - loss: 0.3286 - val_loss: 0.3623\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>488</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  488  41\n",
       "1   74  34"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37158469945355194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1nn5thbq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 38172... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▃▃▃▃▄▄▄▅▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▂▂▂▂▂▂▂▃▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▂▂▁▁▁▂▂▂▁▁▁▃▁▁▁▁▄▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_val_loss</td><td>0.36209</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32863</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.36226</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fanciful-terrain-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/1nn5thbq\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/1nn5thbq</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_175407-1nn5thbq/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1nn5thbq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/305ts2o6\" target=\"_blank\">blooming-terrain-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-17:57:03] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:57:03] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:57:03] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 583ms/step - loss: 0.8860 - val_loss: 0.8218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.7015 - val_loss: 0.4321\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.6143 - val_loss: 0.5102\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.6983 - val_loss: 0.5746\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5711 - val_loss: 0.4482\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5011 - val_loss: 0.5322\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4948 - val_loss: 0.4104\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4657 - val_loss: 0.4103\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4718 - val_loss: 0.4719\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4672 - val_loss: 0.4059\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4611 - val_loss: 0.4112\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4519 - val_loss: 0.4146\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4553 - val_loss: 0.4040\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4490 - val_loss: 0.4078\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4455 - val_loss: 0.4159\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4469 - val_loss: 0.4026\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4416 - val_loss: 0.4086\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4412 - val_loss: 0.4087\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4416 - val_loss: 0.4031\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4433 - val_loss: 0.4187\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4359 - val_loss: 0.4082\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4291 - val_loss: 0.3978\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4404 - val_loss: 0.4001\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4297 - val_loss: 0.4110\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4337 - val_loss: 0.4084\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4349 - val_loss: 0.4014\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4293 - val_loss: 0.4002\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4313 - val_loss: 0.4008\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4248 - val_loss: 0.4031\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4347 - val_loss: 0.4058\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_09-17:57:46] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:57:55] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4720WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0937s vs `on_train_batch_end` time: 0.1264s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0937s vs `on_train_batch_end` time: 0.1264s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 698ms/step - loss: 0.4755 - val_loss: 0.3983\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4571 - val_loss: 0.3995\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4739 - val_loss: 0.4057\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4306 - val_loss: 0.3996\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4125 - val_loss: 0.3885\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3955 - val_loss: 0.3898\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3918 - val_loss: 0.3889\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3710 - val_loss: 0.3997\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3303 - val_loss: 0.4253\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3267 - val_loss: 0.3859\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3193 - val_loss: 0.3874\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3018 - val_loss: 0.4028\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.2842 - val_loss: 0.3901\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.2678 - val_loss: 0.4405\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.2534 - val_loss: 0.4133\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.2273 - val_loss: 0.4233\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.2194 - val_loss: 0.4215\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.2115 - val_loss: 0.4301\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "[2022_04_09-17:58:36] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:58:36] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:58:37] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.2989WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0987s vs `on_train_batch_end` time: 0.1239s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0987s vs `on_train_batch_end` time: 0.1239s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 676ms/step - loss: 0.2966 - val_loss: 0.4033\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>458</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  458  71\n",
       "1   66  42"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38009049773755654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/crossval/chen_4_b.csv\"), index_col=0)\n",
    "test_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/crossval/chen_4_a.csv\"), index_col=0)\n",
    "train_data[\"seq\"] = train_data[\"heavy\"] + train_data[\"light\"]\n",
    "test_data[\"seq\"] = test_data[\"heavy\"] + test_data[\"light\"]\n",
    "\n",
    "for pat in patience:\n",
    "    for lr in learning_rate:\n",
    "        finetune_by_settings(pat, lr, \"Split 4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23db2b59-81eb-47da-a60b-31e2d4892411",
   "metadata": {},
   "source": [
    "# 5x2cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6816ec8-f1f4-4cda-a4be-256d0fdf968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_by_settings_and_data(patience, lr, project_name, train_data, test_data, dir_name):\n",
    "    wandb.init(project=project_name, entity=\"kvetab\")\n",
    "    model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
    "    training_callbacks = [\n",
    "        keras.callbacks.ReduceLROnPlateau(patience = patience[1], factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "        keras.callbacks.EarlyStopping(patience = patience[0], restore_best_weights = True),\n",
    "        WandbCallback()\n",
    "    ]\n",
    "    epoch_num = 100\n",
    "    batch_size = 128\n",
    "    wandb.config = {\n",
    "          \"learning_rate\": lr,\n",
    "          \"epochs\": epoch_num * 2,\n",
    "          \"batch_size\": batch_size\n",
    "        }\n",
    "    print(type(lr))\n",
    "    print(lr)\n",
    "    finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], test_data['seq'], test_data['Y'], \\\n",
    "            seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = lr, begin_with_frozen_pretrained_layers = True, \\\n",
    "            lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 512, final_lr = lr / 10, callbacks = training_callbacks)\n",
    "    \n",
    "    results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "            start_seq_len = 512, start_batch_size = 32)\n",
    "    print('Confusion matrix:')\n",
    "    display(confusion_matrix)\n",
    "    fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "    f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "    print(f1)\n",
    "    \n",
    "    mod = model_generator.create_model(seq_len = 512)\n",
    "    mod.save(path.join(DATA_DIR, f\"protein_bert/{dir_name}/2022_04_09_{patience[0]}_{patience[1]}_{lr}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1750fff9-d429-4b0a-ac05-d14f629b52bc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:305ts2o6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 38557... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇███▁▁▂▂▂▂▃▃▃▄▄▄▅▅▁</td></tr><tr><td>loss</td><td>█▆▅▆▅▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▄▄▃▃▃▃▂▂▂▂▂▁▁▁▂</td></tr><tr><td>lr</td><td>█████▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▄▂▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>0.3859</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.29658</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.40332</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">blooming-terrain-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/305ts2o6\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/305ts2o6</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_175647-305ts2o6/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:305ts2o6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/3uy1bffg\" target=\"_blank\">rosy-wildflower-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-17:59:32] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:59:32] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:59:32] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 755ms/step - loss: 0.9407 - val_loss: 0.5439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5889 - val_loss: 0.6432\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5342 - val_loss: 0.5259\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4891 - val_loss: 0.4766\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4620 - val_loss: 0.4653\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4581 - val_loss: 0.4823\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4444 - val_loss: 0.5213\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4601 - val_loss: 0.4586\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4161 - val_loss: 0.4488\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4068 - val_loss: 0.4512\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4072 - val_loss: 0.4513\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4002 - val_loss: 0.4492\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-17:59:57] Training the entire fine-tuned model...\n",
      "[2022_04_09-18:00:05] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 868ms/step - loss: 0.3910 - val_loss: 0.4454\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3948 - val_loss: 0.4443\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3902 - val_loss: 0.4441\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3980 - val_loss: 0.4438\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3971 - val_loss: 0.4436\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3971 - val_loss: 0.4438\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3943 - val_loss: 0.4436\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3940 - val_loss: 0.4430\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3897 - val_loss: 0.4426\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3872 - val_loss: 0.4423\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3969 - val_loss: 0.4420\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3876 - val_loss: 0.4417\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3816 - val_loss: 0.4416\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3895 - val_loss: 0.4415\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3895 - val_loss: 0.4412\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3930 - val_loss: 0.4407\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3898 - val_loss: 0.4406\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3810 - val_loss: 0.4406\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3839 - val_loss: 0.4404\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3812 - val_loss: 0.4400\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3794 - val_loss: 0.4397\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3853 - val_loss: 0.4395\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3798 - val_loss: 0.4395\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3828 - val_loss: 0.4391\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3816 - val_loss: 0.4388\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3763 - val_loss: 0.4387\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3803 - val_loss: 0.4387\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3800 - val_loss: 0.4383\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3817 - val_loss: 0.4382\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3778 - val_loss: 0.4381\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3795 - val_loss: 0.4381\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3775 - val_loss: 0.4378\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3655 - val_loss: 0.4377\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3744 - val_loss: 0.4375\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3734 - val_loss: 0.4376\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3653 - val_loss: 0.4376\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3705 - val_loss: 0.4374\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3767 - val_loss: 0.4373\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3669 - val_loss: 0.4372\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3705 - val_loss: 0.4372\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3759 - val_loss: 0.4372\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3678 - val_loss: 0.4372\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3702 - val_loss: 0.4371\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3626 - val_loss: 0.4371\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3710 - val_loss: 0.4371\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3695 - val_loss: 0.4371\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3710 - val_loss: 0.4371\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3724 - val_loss: 0.4371\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3628 - val_loss: 0.4371\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3661 - val_loss: 0.4371\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3676 - val_loss: 0.4371\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3641 - val_loss: 0.4371\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3739 - val_loss: 0.4371\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3640 - val_loss: 0.4371\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3625 - val_loss: 0.4371\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3644 - val_loss: 0.4371\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3722 - val_loss: 0.4371\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3701 - val_loss: 0.4371\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3676 - val_loss: 0.4371\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3695 - val_loss: 0.4371\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3729 - val_loss: 0.4371\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3653 - val_loss: 0.4371\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3705 - val_loss: 0.4371\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3623 - val_loss: 0.4371\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3644 - val_loss: 0.4371\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3748 - val_loss: 0.4371\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3660 - val_loss: 0.4371\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3722 - val_loss: 0.4371\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3688 - val_loss: 0.4371\n",
      "[2022_04_09-18:02:21] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-18:02:21] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:02:21] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 856ms/step - loss: 0.3693 - val_loss: 0.4371\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>513</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  513  23\n",
       "1  118  22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23783783783783785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3uy1bffg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 38900... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███▁</td></tr><tr><td>loss</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▃▇▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.43708</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36933</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43708</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">rosy-wildflower-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/3uy1bffg\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/3uy1bffg</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_175916-3uy1bffg/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3uy1bffg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/fpe90fu1\" target=\"_blank\">absurd-grass-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-18:03:15] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:03:15] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:03:15] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 591ms/step - loss: 0.8500 - val_loss: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5719 - val_loss: 0.4701\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.5404 - val_loss: 0.6288\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5494 - val_loss: 0.5440\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4937 - val_loss: 0.4638\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4545 - val_loss: 0.5037\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4831 - val_loss: 0.5333\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4607 - val_loss: 0.4568\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4414 - val_loss: 0.5161\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4488 - val_loss: 0.4559\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4052 - val_loss: 0.4350\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4079 - val_loss: 0.4773\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4193 - val_loss: 0.4341\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3843 - val_loss: 0.4309\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3879 - val_loss: 0.5100\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3935 - val_loss: 0.4482\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3884 - val_loss: 0.4211\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3834 - val_loss: 0.4816\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4184 - val_loss: 0.4195\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3791 - val_loss: 0.4412\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3909 - val_loss: 0.5209\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3760 - val_loss: 0.4303\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-18:03:50] Training the entire fine-tuned model...\n",
      "[2022_04_09-18:04:13] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3479WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1069s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1069s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 696ms/step - loss: 0.3479 - val_loss: 0.4213\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3603 - val_loss: 0.4230\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3602 - val_loss: 0.4207\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3485 - val_loss: 0.4185\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3524 - val_loss: 0.4195\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3447 - val_loss: 0.4240\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3555 - val_loss: 0.4211\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_09-18:04:35] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-18:04:35] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:04:35] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3540WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1287s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1287s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 681ms/step - loss: 0.3540 - val_loss: 0.4185\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>475</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  475  18\n",
       "1  100  22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2716049382716049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:fpe90fu1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 39439... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▆▄▂▃▄▂▃▂▁▂▁▁▃▂▁▃▁▂▄▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>0.41851</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35401</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41853</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">absurd-grass-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/fpe90fu1\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/fpe90fu1</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_180259-fpe90fu1/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:fpe90fu1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/38cy9uyx\" target=\"_blank\">vital-sound-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-18:05:30] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:05:30] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:05:30] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 712ms/step - loss: 0.9674 - val_loss: 0.4989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.5820 - val_loss: 0.6508\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5725 - val_loss: 0.5735\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5036 - val_loss: 0.5139\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-18:05:46] Training the entire fine-tuned model...\n",
      "[2022_04_09-18:05:58] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 850ms/step - loss: 0.4843 - val_loss: 0.4984\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.4837 - val_loss: 0.4975\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.4787 - val_loss: 0.4965\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.4770 - val_loss: 0.4953\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.4745 - val_loss: 0.4938\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.4752 - val_loss: 0.4925\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.4747 - val_loss: 0.4898\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.4729 - val_loss: 0.4875\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.4669 - val_loss: 0.4848\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.4636 - val_loss: 0.4812\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.4562 - val_loss: 0.4767\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.4483 - val_loss: 0.4709\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.4438 - val_loss: 0.4654\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.4401 - val_loss: 0.4602\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.4148 - val_loss: 0.4555\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.4107 - val_loss: 0.4566\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3997 - val_loss: 0.4540\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3766 - val_loss: 0.4635\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3793 - val_loss: 0.4561\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3719 - val_loss: 0.4808\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_09-18:06:43] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-18:06:43] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:06:43] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 856ms/step - loss: 0.3854 - val_loss: 0.4544\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>522</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  522  14\n",
       "1  123  17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19883040935672514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:38cy9uyx) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 39685... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██▁</td></tr><tr><td>loss</td><td>█▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▅▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>16</td></tr><tr><td>best_val_loss</td><td>0.45397</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38536</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.45437</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">vital-sound-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/38cy9uyx\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/38cy9uyx</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_180514-38cy9uyx/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:38cy9uyx). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/3a9t2hv4\" target=\"_blank\">amber-donkey-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-18:07:38] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:07:38] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:07:38] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 575ms/step - loss: 0.8347 - val_loss: 0.9391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.7329 - val_loss: 0.5444\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5357 - val_loss: 0.4912\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4665 - val_loss: 0.5315\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4772 - val_loss: 0.5027\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4544 - val_loss: 0.4457\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4178 - val_loss: 0.4422\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4115 - val_loss: 0.4403\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4038 - val_loss: 0.4396\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4033 - val_loss: 0.4327\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3888 - val_loss: 0.4374\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4004 - val_loss: 0.4299\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3842 - val_loss: 0.4400\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3750 - val_loss: 0.4222\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3694 - val_loss: 0.4405\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3842 - val_loss: 0.4188\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4257 - val_loss: 0.4929\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3889 - val_loss: 0.4426\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3655 - val_loss: 0.4475\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-18:08:10] Training the entire fine-tuned model...\n",
      "[2022_04_09-18:08:36] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3746WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0987s vs `on_train_batch_end` time: 0.1290s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0987s vs `on_train_batch_end` time: 0.1290s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 815ms/step - loss: 0.3746 - val_loss: 0.4213\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3686 - val_loss: 0.4264\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3576 - val_loss: 0.4175\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3640 - val_loss: 0.4188\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3380 - val_loss: 0.4170\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3454 - val_loss: 0.4163\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3396 - val_loss: 0.4235\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3508 - val_loss: 0.4168\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3429 - val_loss: 0.4164\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_09-18:09:02] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-18:09:02] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:09:02] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3406WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0978s vs `on_train_batch_end` time: 0.1304s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0978s vs `on_train_batch_end` time: 0.1304s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 683ms/step - loss: 0.3406 - val_loss: 0.4163\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>474</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  474  19\n",
       "1   98  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2909090909090909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3a9t2hv4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 39906... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██▁▁▂▂▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▇▄▃▃▃▂▂▂▂▂▂▂▂▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▃▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.41629</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34061</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41629</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">amber-donkey-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/3a9t2hv4\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/3a9t2hv4</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_180722-3a9t2hv4/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3a9t2hv4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/2n88bin4\" target=\"_blank\">icy-jazz-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-18:09:57] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:09:57] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:09:57] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 741ms/step - loss: 0.7771 - val_loss: 0.7373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.6015 - val_loss: 0.5730\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4997 - val_loss: 0.4772\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4597 - val_loss: 0.4763\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4497 - val_loss: 0.5151\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4442 - val_loss: 0.5026\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4207 - val_loss: 0.4971\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-18:10:15] Training the entire fine-tuned model...\n",
      "[2022_04_09-18:10:25] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 830ms/step - loss: 0.4458 - val_loss: 0.4645\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.4358 - val_loss: 0.4616\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.4231 - val_loss: 0.4597\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.4209 - val_loss: 0.4536\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.4091 - val_loss: 0.4502\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.4033 - val_loss: 0.4468\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.4002 - val_loss: 0.4454\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3815 - val_loss: 0.4483\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3721 - val_loss: 0.4458\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3573 - val_loss: 0.4545\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_09-18:10:52] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-18:10:52] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:11:12] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 834ms/step - loss: 0.3852 - val_loss: 0.4452\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>516</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  516  20\n",
       "1  121  19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2122905027932961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2n88bin4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 40148... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▁▂▃▃▄▅▆▆▇█▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>lr</td><td>███████▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▃▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.44525</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38516</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.44525</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">icy-jazz-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/2n88bin4\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/2n88bin4</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_180940-2n88bin4/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2n88bin4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/147ecf81\" target=\"_blank\">soft-violet-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-18:12:06] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:12:07] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:12:07] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 597ms/step - loss: 0.8661 - val_loss: 1.1843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.8090 - val_loss: 0.7294\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5965 - val_loss: 0.4673\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5579 - val_loss: 0.4600\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4857 - val_loss: 0.5224\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4853 - val_loss: 0.4819\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4413 - val_loss: 0.4523\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4344 - val_loss: 0.4719\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4172 - val_loss: 0.4512\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4075 - val_loss: 0.4425\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3969 - val_loss: 0.4338\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3876 - val_loss: 0.4318\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3822 - val_loss: 0.4331\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3877 - val_loss: 0.4276\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3742 - val_loss: 0.4251\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3803 - val_loss: 0.4222\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3715 - val_loss: 0.4206\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3627 - val_loss: 0.4193\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3704 - val_loss: 0.4181\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3510 - val_loss: 0.4320\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3643 - val_loss: 0.4178\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3453 - val_loss: 0.4152\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3543 - val_loss: 0.4316\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3643 - val_loss: 0.4134\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3464 - val_loss: 0.4375\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3471 - val_loss: 0.4741\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4055 - val_loss: 0.4404\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-18:12:48] Training the entire fine-tuned model...\n",
      "[2022_04_09-18:13:05] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3563WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1297s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1297s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 678ms/step - loss: 0.3563 - val_loss: 0.4198\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3416 - val_loss: 0.4416\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3578 - val_loss: 0.4209\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3384 - val_loss: 0.4091\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3367 - val_loss: 0.4080\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3218 - val_loss: 0.4136\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3269 - val_loss: 0.4087\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.2985 - val_loss: 0.4100\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_09-18:13:29] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-18:13:29] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:13:29] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3172WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0988s vs `on_train_batch_end` time: 0.1287s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0988s vs `on_train_batch_end` time: 0.1287s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 694ms/step - loss: 0.3172 - val_loss: 0.4080\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>474</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  474  19\n",
       "1   96  26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31137724550898205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:147ecf81) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 40344... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▇▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████████████▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.408</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31721</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.408</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">soft-violet-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/147ecf81\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/147ecf81</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_181152-147ecf81/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:147ecf81). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/llchnnj9\" target=\"_blank\">likely-snow-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-18:14:23] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:14:23] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:14:23] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 696ms/step - loss: 0.9180 - val_loss: 0.5101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.6086 - val_loss: 0.6289\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5188 - val_loss: 0.5980\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4846 - val_loss: 0.5315\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-18:14:37] Training the entire fine-tuned model...\n",
      "[2022_04_09-18:14:50] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 855ms/step - loss: 0.4872 - val_loss: 0.5029\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.4862 - val_loss: 0.4933\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.4680 - val_loss: 0.4842\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.4526 - val_loss: 0.4676\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.4215 - val_loss: 0.4536\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3914 - val_loss: 0.4629\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.4022 - val_loss: 0.4757\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3937 - val_loss: 0.4742\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_09-18:15:13] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-18:15:13] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:15:13] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 824ms/step - loss: 0.3941 - val_loss: 0.4504\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>535</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  535  1\n",
       "1  140  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:llchnnj9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 40619... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▄▁▂▃▄▅▆▇█▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>████▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▇▄▃▃▂▂▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.45041</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3941</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.45041</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">likely-snow-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/llchnnj9\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/llchnnj9</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_181407-llchnnj9/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:llchnnj9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/3c5lseqk\" target=\"_blank\">morning-wildflower-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-18:16:06] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:16:06] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:16:06] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 614ms/step - loss: 0.8720 - val_loss: 0.9071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.6363 - val_loss: 0.5643\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5486 - val_loss: 0.4761\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4695 - val_loss: 0.4723\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4484 - val_loss: 0.4510\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4344 - val_loss: 0.4545\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4209 - val_loss: 0.4470\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4103 - val_loss: 0.4659\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4200 - val_loss: 0.4352\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3946 - val_loss: 0.4394\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4123 - val_loss: 0.4572\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3792 - val_loss: 0.4290\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3915 - val_loss: 0.4341\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3864 - val_loss: 0.4254\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3815 - val_loss: 0.4400\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3764 - val_loss: 0.4252\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3631 - val_loss: 0.4204\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3548 - val_loss: 0.4218\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3531 - val_loss: 0.4223\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3541 - val_loss: 0.4180\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3601 - val_loss: 0.4158\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3446 - val_loss: 0.4131\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3483 - val_loss: 0.4212\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3599 - val_loss: 0.4133\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3523 - val_loss: 0.4144\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-18:16:45] Training the entire fine-tuned model...\n",
      "[2022_04_09-18:17:04] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.7537WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0990s vs `on_train_batch_end` time: 0.1297s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0990s vs `on_train_batch_end` time: 0.1297s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 694ms/step - loss: 0.7537 - val_loss: 0.7001\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4359 - val_loss: 0.4711\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4366 - val_loss: 0.4403\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3740 - val_loss: 0.4224\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3545 - val_loss: 0.4480\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3750 - val_loss: 0.4159\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3372 - val_loss: 0.4114\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3329 - val_loss: 0.4176\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3012 - val_loss: 0.4139\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3060 - val_loss: 0.4192\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_09-18:17:32] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-18:17:32] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:17:32] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3355WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1295s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1295s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 827ms/step - loss: 0.3355 - val_loss: 0.4115\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>472</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  472  21\n",
       "1   98  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2874251497005988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3c5lseqk) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 40780... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▇▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████████▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▂▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>6</td></tr><tr><td>best_val_loss</td><td>0.41136</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33552</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.41146</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">morning-wildflower-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/3c5lseqk\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/3c5lseqk</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_181550-3c5lseqk/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3c5lseqk). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/o54tvdui\" target=\"_blank\">laced-pine-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-18:18:27] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:18:27] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:18:27] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 727ms/step - loss: 0.7654 - val_loss: 0.7796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.6207 - val_loss: 0.6364\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.5286 - val_loss: 0.4760\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5086 - val_loss: 0.4672\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4580 - val_loss: 0.4716\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4266 - val_loss: 0.4634\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4246 - val_loss: 0.4648\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4150 - val_loss: 0.4480\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4043 - val_loss: 0.4527\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3937 - val_loss: 0.4813\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3873 - val_loss: 0.4835\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3971 - val_loss: 0.4506\n",
      "[2022_04_09-18:18:50] Training the entire fine-tuned model...\n",
      "[2022_04_09-18:19:00] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 837ms/step - loss: 0.3979 - val_loss: 0.4484\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3991 - val_loss: 0.4478\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3905 - val_loss: 0.4466\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3941 - val_loss: 0.4463\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3982 - val_loss: 0.4460\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3994 - val_loss: 0.4457\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3976 - val_loss: 0.4457\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3982 - val_loss: 0.4455\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3933 - val_loss: 0.4450\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3863 - val_loss: 0.4447\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3944 - val_loss: 0.4444\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3970 - val_loss: 0.4442\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3872 - val_loss: 0.4442\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3839 - val_loss: 0.4438\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3880 - val_loss: 0.4433\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3905 - val_loss: 0.4430\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3982 - val_loss: 0.4433\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3890 - val_loss: 0.4427\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3837 - val_loss: 0.4424\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3832 - val_loss: 0.4424\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3805 - val_loss: 0.4422\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3873 - val_loss: 0.4419\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3803 - val_loss: 0.4417\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3791 - val_loss: 0.4414\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3842 - val_loss: 0.4412\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3744 - val_loss: 0.4409\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3730 - val_loss: 0.4407\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3804 - val_loss: 0.4406\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3673 - val_loss: 0.4405\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3771 - val_loss: 0.4404\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3783 - val_loss: 0.4401\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3756 - val_loss: 0.4399\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3763 - val_loss: 0.4398\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3641 - val_loss: 0.4397\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3698 - val_loss: 0.4398\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.3730 - val_loss: 0.4396\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3627 - val_loss: 0.4395\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3642 - val_loss: 0.4394\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3633 - val_loss: 0.4395\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3610 - val_loss: 0.4398\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3635 - val_loss: 0.4395\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3563 - val_loss: 0.4393\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3629 - val_loss: 0.4394\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3620 - val_loss: 0.4394\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3563 - val_loss: 0.4393\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3587 - val_loss: 0.4394\n",
      "[2022_04_09-18:20:33] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-18:20:33] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:20:33] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 812ms/step - loss: 0.3612 - val_loss: 0.4393\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>508</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  508  28\n",
       "1  116  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:o54tvdui) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 41055... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁</td></tr><tr><td>loss</td><td>█▆▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.43934</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36123</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43934</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">laced-pine-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/o54tvdui\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/o54tvdui</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_181810-o54tvdui/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:o54tvdui). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/fufzdyk3\" target=\"_blank\">rose-river-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-18:21:29] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:21:29] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:21:29] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 578ms/step - loss: 0.9156 - val_loss: 0.6895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.6020 - val_loss: 0.4753\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4858 - val_loss: 0.5096\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4742 - val_loss: 0.4620\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4492 - val_loss: 0.4588\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4541 - val_loss: 0.4562\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4292 - val_loss: 0.4881\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4248 - val_loss: 0.4527\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4188 - val_loss: 0.4480\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4187 - val_loss: 0.4509\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4495 - val_loss: 0.4835\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4154 - val_loss: 0.4546\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3993 - val_loss: 0.4364\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3760 - val_loss: 0.4353\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3827 - val_loss: 0.4342\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3700 - val_loss: 0.4285\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3772 - val_loss: 0.4278\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3760 - val_loss: 0.4272\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3723 - val_loss: 0.4329\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3708 - val_loss: 0.4258\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3736 - val_loss: 0.4265\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3720 - val_loss: 0.4259\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3557 - val_loss: 0.4243\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3662 - val_loss: 0.4258\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3562 - val_loss: 0.4247\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3699 - val_loss: 0.4247\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3608 - val_loss: 0.4246\n",
      "[2022_04_09-18:22:10] Training the entire fine-tuned model...\n",
      "[2022_04_09-18:22:18] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3660WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1027s vs `on_train_batch_end` time: 0.1270s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1027s vs `on_train_batch_end` time: 0.1270s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 688ms/step - loss: 0.3660 - val_loss: 0.4253\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3599 - val_loss: 0.4242\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3703 - val_loss: 0.4236\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3537 - val_loss: 0.4234\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3642 - val_loss: 0.4244\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3733 - val_loss: 0.4268\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3609 - val_loss: 0.4270\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3541 - val_loss: 0.4254\n",
      "[2022_04_09-18:22:41] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-18:22:41] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:22:41] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3559WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0978s vs `on_train_batch_end` time: 0.1297s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0978s vs `on_train_batch_end` time: 0.1297s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 682ms/step - loss: 0.3559 - val_loss: 0.4234\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>481</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  481  12\n",
       "1  107  15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20134228187919462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:fufzdyk3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 41466... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▂▂▂▃▂▂▂▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42339</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35586</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42339</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">rose-river-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/fufzdyk3\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/fufzdyk3</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_182112-fufzdyk3/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:fufzdyk3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/s9ldnd9u\" target=\"_blank\">true-star-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-18:23:39] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:23:39] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:23:40] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 1s/step - loss: 0.9679 - val_loss: 0.5012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.6272 - val_loss: 0.7644\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.7029 - val_loss: 0.4897\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.5461 - val_loss: 0.5629\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5114 - val_loss: 0.4704\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4503 - val_loss: 0.4805\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4593 - val_loss: 0.4625\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4462 - val_loss: 0.4931\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4385 - val_loss: 0.4682\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4091 - val_loss: 0.4637\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4015 - val_loss: 0.4445\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4000 - val_loss: 0.4456\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3965 - val_loss: 0.4460\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3876 - val_loss: 0.4435\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3910 - val_loss: 0.4432\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3990 - val_loss: 0.4430\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3848 - val_loss: 0.4445\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3860 - val_loss: 0.4424\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3851 - val_loss: 0.4420\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3857 - val_loss: 0.4446\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3841 - val_loss: 0.4417\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3924 - val_loss: 0.4411\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3746 - val_loss: 0.4416\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3794 - val_loss: 0.4403\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3797 - val_loss: 0.4399\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3876 - val_loss: 0.4390\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3783 - val_loss: 0.4386\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3762 - val_loss: 0.4381\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3775 - val_loss: 0.4405\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3783 - val_loss: 0.4377\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3752 - val_loss: 0.4372\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3664 - val_loss: 0.4387\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3679 - val_loss: 0.4371\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3704 - val_loss: 0.4406\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3640 - val_loss: 0.4369\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3716 - val_loss: 0.4366\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3637 - val_loss: 0.4366\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3649 - val_loss: 0.4367\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3610 - val_loss: 0.4367\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3651 - val_loss: 0.4367\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3658 - val_loss: 0.4366\n",
      "[2022_04_09-18:24:36] Training the entire fine-tuned model...\n",
      "[2022_04_09-18:25:18] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 853ms/step - loss: 0.3660 - val_loss: 0.4393\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3704 - val_loss: 0.4403\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3630 - val_loss: 0.4386\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3593 - val_loss: 0.4372\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3570 - val_loss: 0.4365\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3506 - val_loss: 0.4360\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3534 - val_loss: 0.4348\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3461 - val_loss: 0.4348\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3465 - val_loss: 0.4345\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3349 - val_loss: 0.4354\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3310 - val_loss: 0.4352\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3371 - val_loss: 0.4358\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3384 - val_loss: 0.4364\n",
      "[2022_04_09-18:25:50] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-18:25:50] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:25:50] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 853ms/step - loss: 0.3426 - val_loss: 0.4345\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>501</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  501  35\n",
       "1  111  29"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28431372549019607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:s9ldnd9u) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 41743... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▄▅▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂█▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.43454</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34262</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43454</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">true-star-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/s9ldnd9u\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/s9ldnd9u</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_182323-s9ldnd9u/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:s9ldnd9u). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/24wtkl8p\" target=\"_blank\">vital-sponge-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-18:26:59] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:26:59] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:26:59] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 597ms/step - loss: 0.7856 - val_loss: 0.8608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.6634 - val_loss: 0.5231\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5621 - val_loss: 0.5865\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5489 - val_loss: 0.6507\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.5354 - val_loss: 0.5866\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5085 - val_loss: 0.4526\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4436 - val_loss: 0.4673\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4281 - val_loss: 0.4664\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4283 - val_loss: 0.4500\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4184 - val_loss: 0.4509\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4167 - val_loss: 0.4538\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4129 - val_loss: 0.4462\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4135 - val_loss: 0.4444\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4092 - val_loss: 0.4477\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4063 - val_loss: 0.4425\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4178 - val_loss: 0.4428\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4027 - val_loss: 0.4437\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4032 - val_loss: 0.4387\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4013 - val_loss: 0.4375\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4053 - val_loss: 0.4372\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3939 - val_loss: 0.4377\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3996 - val_loss: 0.4354\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3936 - val_loss: 0.4366\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3886 - val_loss: 0.4414\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4048 - val_loss: 0.4394\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3954 - val_loss: 0.4363\n",
      "[2022_04_09-18:27:38] Training the entire fine-tuned model...\n",
      "[2022_04_09-18:27:57] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3969WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1301s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1301s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 676ms/step - loss: 0.3969 - val_loss: 0.4350\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3941 - val_loss: 0.4349\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3931 - val_loss: 0.4307\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3837 - val_loss: 0.4296\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3865 - val_loss: 0.4386\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3685 - val_loss: 0.4275\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3676 - val_loss: 0.4291\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3654 - val_loss: 0.4264\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3652 - val_loss: 0.4266\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3463 - val_loss: 0.4304\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3405 - val_loss: 0.4249\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3421 - val_loss: 0.4232\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3379 - val_loss: 0.4233\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3363 - val_loss: 0.4273\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3232 - val_loss: 0.4252\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3232 - val_loss: 0.4267\n",
      "[2022_04_09-18:28:37] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-18:28:37] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:28:37] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3366WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1006s vs `on_train_batch_end` time: 0.1293s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1006s vs `on_train_batch_end` time: 0.1293s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 688ms/step - loss: 0.3366 - val_loss: 0.4244\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>479</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  479  14\n",
       "1  100  22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27848101265822783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:24wtkl8p) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 42126... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇██▁▂▂▂▂▃▃▃▄▄▄▄▅▅▁</td></tr><tr><td>loss</td><td>█▆▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▄▅▄▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_val_loss</td><td>0.42321</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33664</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.4244</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">vital-sponge-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/24wtkl8p\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/24wtkl8p</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_182644-24wtkl8p/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:24wtkl8p). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/a22i6mjo\" target=\"_blank\">vocal-bush-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-18:29:40] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:29:40] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:29:40] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 704ms/step - loss: 0.9484 - val_loss: 0.4998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.6207 - val_loss: 0.6862\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5933 - val_loss: 0.5629\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4952 - val_loss: 0.5335\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4799 - val_loss: 0.4698\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4491 - val_loss: 0.4836\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4443 - val_loss: 0.4651\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4395 - val_loss: 0.4672\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4320 - val_loss: 0.4598\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4276 - val_loss: 0.4584\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4285 - val_loss: 0.4582\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4199 - val_loss: 0.4559\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4203 - val_loss: 0.4562\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4121 - val_loss: 0.4534\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4103 - val_loss: 0.4523\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4074 - val_loss: 0.4523\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4089 - val_loss: 0.4502\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4159 - val_loss: 0.4498\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4054 - val_loss: 0.4488\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4052 - val_loss: 0.4472\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4063 - val_loss: 0.4464\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4016 - val_loss: 0.4460\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4020 - val_loss: 0.4448\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3895 - val_loss: 0.4459\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3872 - val_loss: 0.4431\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3924 - val_loss: 0.4429\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3846 - val_loss: 0.4418\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3891 - val_loss: 0.4411\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3878 - val_loss: 0.4431\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3848 - val_loss: 0.4403\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3908 - val_loss: 0.4402\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3816 - val_loss: 0.4388\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3759 - val_loss: 0.4383\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3873 - val_loss: 0.4381\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3816 - val_loss: 0.4382\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3831 - val_loss: 0.4381\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3744 - val_loss: 0.4397\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3768 - val_loss: 0.4404\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3705 - val_loss: 0.4373\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3776 - val_loss: 0.4372\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3641 - val_loss: 0.4366\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3735 - val_loss: 0.4373\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3683 - val_loss: 0.4379\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3674 - val_loss: 0.4367\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3704 - val_loss: 0.4366\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3720 - val_loss: 0.4366\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3695 - val_loss: 0.4366\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3651 - val_loss: 0.4365\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3769 - val_loss: 0.4365\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3638 - val_loss: 0.4366\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3664 - val_loss: 0.4366\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3706 - val_loss: 0.4366\n",
      "[2022_04_09-18:30:47] Training the entire fine-tuned model...\n",
      "[2022_04_09-18:30:56] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 837ms/step - loss: 0.3767 - val_loss: 0.4460\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3792 - val_loss: 0.4361\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3701 - val_loss: 0.4351\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3600 - val_loss: 0.4352\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3637 - val_loss: 0.4350\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3538 - val_loss: 0.4363\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3490 - val_loss: 0.4343\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3452 - val_loss: 0.4349\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3493 - val_loss: 0.4341\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3502 - val_loss: 0.4340\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3441 - val_loss: 0.4338\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3438 - val_loss: 0.4338\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3373 - val_loss: 0.4340\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3351 - val_loss: 0.4340\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3343 - val_loss: 0.4340\n",
      "[2022_04_09-18:31:32] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-18:31:32] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:31:32] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 839ms/step - loss: 0.3440 - val_loss: 0.4338\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>502</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>113</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  502  34\n",
       "1  113  27"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26865671641791045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:a22i6mjo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 42439... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▄▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_val_loss</td><td>0.43375</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34403</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.43378</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">vocal-bush-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/a22i6mjo\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/a22i6mjo</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_182923-a22i6mjo/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:a22i6mjo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/2q85k5f9\" target=\"_blank\">pleasant-salad-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-18:32:25] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:32:25] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:32:25] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 588ms/step - loss: 0.8425 - val_loss: 0.6802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.6254 - val_loss: 0.4744\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5151 - val_loss: 0.5713\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5054 - val_loss: 0.4691\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4798 - val_loss: 0.4533\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4432 - val_loss: 0.4866\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4363 - val_loss: 0.4448\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4175 - val_loss: 0.4418\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3977 - val_loss: 0.4377\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3967 - val_loss: 0.4346\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3908 - val_loss: 0.4869\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4313 - val_loss: 0.4417\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4015 - val_loss: 0.4255\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3761 - val_loss: 0.4451\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3885 - val_loss: 0.4272\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3787 - val_loss: 0.4306\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3643 - val_loss: 0.4300\n",
      "[2022_04_09-18:32:56] Training the entire fine-tuned model...\n",
      "[2022_04_09-18:33:04] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3768WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0988s vs `on_train_batch_end` time: 0.1297s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0988s vs `on_train_batch_end` time: 0.1297s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 821ms/step - loss: 0.3768 - val_loss: 0.4526\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3821 - val_loss: 0.4317\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3723 - val_loss: 0.4387\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3750 - val_loss: 0.4212\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3616 - val_loss: 0.4215\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3555 - val_loss: 0.4193\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3481 - val_loss: 0.4323\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3449 - val_loss: 0.4195\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3415 - val_loss: 0.4266\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3275 - val_loss: 0.4194\n",
      "[2022_04_09-18:33:31] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-18:33:31] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:33:31] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3462WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1012s vs `on_train_batch_end` time: 0.1270s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1012s vs `on_train_batch_end` time: 0.1270s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 690ms/step - loss: 0.3462 - val_loss: 0.4193\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>475</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  475  18\n",
       "1  101  21"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2608695652173913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2q85k5f9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 42883... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██▁▁▂▂▃▃▄▄▅▅▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▅▂▂▃▂▂▁▁▃▂▁▂▁▁▁▂▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.41927</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34624</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.41927</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">pleasant-salad-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/2q85k5f9\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/2q85k5f9</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_183209-2q85k5f9/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2q85k5f9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/250h757e\" target=\"_blank\">radiant-bee-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-18:34:28] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:34:28] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:34:28] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 692ms/step - loss: 1.1098 - val_loss: 0.6225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.6621 - val_loss: 0.5864\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.6136 - val_loss: 0.4792\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5202 - val_loss: 0.5100\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.5070 - val_loss: 0.4641\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4533 - val_loss: 0.4683\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4454 - val_loss: 0.4549\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4162 - val_loss: 0.4504\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4069 - val_loss: 0.4460\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4009 - val_loss: 0.4480\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4003 - val_loss: 0.4434\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3901 - val_loss: 0.4409\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3822 - val_loss: 0.4488\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3965 - val_loss: 0.4604\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3909 - val_loss: 0.4482\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3732 - val_loss: 0.4380\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3759 - val_loss: 0.4382\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3800 - val_loss: 0.4450\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3701 - val_loss: 0.4370\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3708 - val_loss: 0.4366\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3740 - val_loss: 0.4374\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3679 - val_loss: 0.4369\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3636 - val_loss: 0.4366\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3694 - val_loss: 0.4359\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3661 - val_loss: 0.4364\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3680 - val_loss: 0.4369\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3632 - val_loss: 0.4360\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3650 - val_loss: 0.4359\n",
      "[2022_04_09-18:35:08] Training the entire fine-tuned model...\n",
      "[2022_04_09-18:35:25] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 846ms/step - loss: 0.4304 - val_loss: 0.4840\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3986 - val_loss: 0.4694\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3901 - val_loss: 0.4775\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.4063 - val_loss: 0.5156\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3794 - val_loss: 0.4543\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3594 - val_loss: 0.4713\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3396 - val_loss: 0.4497\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3377 - val_loss: 0.4432\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3154 - val_loss: 0.4385\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.2932 - val_loss: 0.4543\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.2769 - val_loss: 0.4516\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.2693 - val_loss: 0.4761\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.2390 - val_loss: 0.4801\n",
      "[2022_04_09-18:35:58] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-18:35:58] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:35:58] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 832ms/step - loss: 0.2848 - val_loss: 0.4396\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>496</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  496  40\n",
       "1  109  31"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2938388625592417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:250h757e) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 43119... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▃▄▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▃▂▃▄▂▂▂▁▁▂▂▃▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>23</td></tr><tr><td>best_val_loss</td><td>0.4359</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.28481</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.43955</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">radiant-bee-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/250h757e\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/250h757e</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_183410-250h757e/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:250h757e). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/3lwincx7\" target=\"_blank\">ancient-firefly-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-18:36:53] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:36:53] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:36:53] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 580ms/step - loss: 0.9755 - val_loss: 0.9667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.6637 - val_loss: 0.6087\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5404 - val_loss: 0.4658\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4862 - val_loss: 0.4877\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4625 - val_loss: 0.5022\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4587 - val_loss: 0.4500\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4279 - val_loss: 0.4469\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4193 - val_loss: 0.4940\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4356 - val_loss: 0.4401\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4104 - val_loss: 0.4838\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4307 - val_loss: 0.4667\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4074 - val_loss: 0.4479\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3889 - val_loss: 0.4316\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3827 - val_loss: 0.4303\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3846 - val_loss: 0.4305\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3827 - val_loss: 0.4314\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3760 - val_loss: 0.4281\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3774 - val_loss: 0.4308\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3715 - val_loss: 0.4266\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3767 - val_loss: 0.4261\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.3710 - val_loss: 0.4263\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3664 - val_loss: 0.4254\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3719 - val_loss: 0.4242\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3604 - val_loss: 0.4257\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3705 - val_loss: 0.4233\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3610 - val_loss: 0.4248\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 2s 330ms/step - loss: 0.3682 - val_loss: 0.4223\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3646 - val_loss: 0.4229\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3568 - val_loss: 0.4220\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3741 - val_loss: 0.4207\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3639 - val_loss: 0.4204\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3499 - val_loss: 0.4251\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3548 - val_loss: 0.4258\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3630 - val_loss: 0.4278\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3636 - val_loss: 0.4269\n",
      "[2022_04_09-18:37:42] Training the entire fine-tuned model...\n",
      "[2022_04_09-18:37:50] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4628WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0988s vs `on_train_batch_end` time: 0.1297s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0988s vs `on_train_batch_end` time: 0.1297s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 694ms/step - loss: 0.4628 - val_loss: 0.4606\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3938 - val_loss: 0.4399\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3707 - val_loss: 0.4253\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3442 - val_loss: 0.4096\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3502 - val_loss: 0.4414\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3758 - val_loss: 0.4083\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3168 - val_loss: 0.4515\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3136 - val_loss: 0.4268\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3127 - val_loss: 0.4167\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.2648 - val_loss: 0.4237\n",
      "[2022_04_09-18:38:18] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-18:38:18] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:38:18] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3104WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1016s vs `on_train_batch_end` time: 0.1255s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1016s vs `on_train_batch_end` time: 0.1255s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 697ms/step - loss: 0.3104 - val_loss: 0.4094\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>478</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  478  15\n",
       "1  101  21"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26582278481012656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3lwincx7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 43428... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▂▃▃</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▁▁▁</td></tr><tr><td>lr</td><td>███████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.40828</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31039</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.40945</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">ancient-firefly-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/3lwincx7\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/3lwincx7</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_183637-3lwincx7/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3lwincx7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/2b6iw2xl\" target=\"_blank\">valiant-breeze-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-18:39:35] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:39:35] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:39:35] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 729ms/step - loss: 0.9000 - val_loss: 0.7252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.6835 - val_loss: 0.6920\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.5445 - val_loss: 0.5705\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5138 - val_loss: 0.5122\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4754 - val_loss: 0.4617\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4249 - val_loss: 0.4729\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4371 - val_loss: 0.4528\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4108 - val_loss: 0.4562\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4127 - val_loss: 0.4457\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3982 - val_loss: 0.4434\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4042 - val_loss: 0.4671\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4012 - val_loss: 0.4816\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4052 - val_loss: 0.5113\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4252 - val_loss: 0.4420\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3864 - val_loss: 0.4575\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3850 - val_loss: 0.4459\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3867 - val_loss: 0.4409\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3732 - val_loss: 0.4386\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3732 - val_loss: 0.4383\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3768 - val_loss: 0.4396\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3644 - val_loss: 0.4364\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3791 - val_loss: 0.4362\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3725 - val_loss: 0.4361\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3706 - val_loss: 0.4361\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3728 - val_loss: 0.4359\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3685 - val_loss: 0.4361\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3573 - val_loss: 0.4377\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3596 - val_loss: 0.4352\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3668 - val_loss: 0.4363\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3688 - val_loss: 0.4351\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3526 - val_loss: 0.4355\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3604 - val_loss: 0.4345\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3593 - val_loss: 0.4339\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3607 - val_loss: 0.4359\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3584 - val_loss: 0.4335\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3502 - val_loss: 0.4333\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3559 - val_loss: 0.4353\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3670 - val_loss: 0.4337\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3583 - val_loss: 0.4401\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3634 - val_loss: 0.4339\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3584 - val_loss: 0.4326\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3508 - val_loss: 0.4327\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3547 - val_loss: 0.4329\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3464 - val_loss: 0.4333\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3465 - val_loss: 0.4334\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3494 - val_loss: 0.4332\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3542 - val_loss: 0.4331\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_09-18:40:37] Training the entire fine-tuned model...\n",
      "[2022_04_09-18:41:14] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 825ms/step - loss: 0.3549 - val_loss: 0.4334\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3466 - val_loss: 0.4344\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3431 - val_loss: 0.4327\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3601 - val_loss: 0.4328\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3495 - val_loss: 0.4327\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3487 - val_loss: 0.4332\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3503 - val_loss: 0.4331\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3395 - val_loss: 0.4326\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3411 - val_loss: 0.4324\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3487 - val_loss: 0.4323\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3464 - val_loss: 0.4323\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3528 - val_loss: 0.4323\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3487 - val_loss: 0.4322\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3477 - val_loss: 0.4322\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3635 - val_loss: 0.4322\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3407 - val_loss: 0.4322\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3475 - val_loss: 0.4322\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3531 - val_loss: 0.4322\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3434 - val_loss: 0.4322\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3464 - val_loss: 0.4322\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3542 - val_loss: 0.4322\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3439 - val_loss: 0.4322\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3523 - val_loss: 0.4322\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3438 - val_loss: 0.4322\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3411 - val_loss: 0.4322\n",
      "[2022_04_09-18:42:08] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-18:42:08] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:42:08] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 837ms/step - loss: 0.3498 - val_loss: 0.4322\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>506</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  506  30\n",
       "1  111  29"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2914572864321608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2b6iw2xl) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 43766... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▃▄▄▄▅▁</td></tr><tr><td>loss</td><td>█▅▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▃▂▂▁▂▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.43216</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34977</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43216</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">valiant-breeze-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/2b6iw2xl\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/2b6iw2xl</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_183915-2b6iw2xl/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2b6iw2xl). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/1mkle5nv\" target=\"_blank\">deft-terrain-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-18:43:36] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:43:36] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:43:36] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 571ms/step - loss: 0.8594 - val_loss: 0.4949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5311 - val_loss: 0.4942\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5085 - val_loss: 0.4816\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4488 - val_loss: 0.4637\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4305 - val_loss: 0.4885\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4511 - val_loss: 0.4572\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.4328 - val_loss: 0.4405\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4139 - val_loss: 0.4399\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3951 - val_loss: 0.4459\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4070 - val_loss: 0.4320\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4022 - val_loss: 0.4522\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4103 - val_loss: 0.4325\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3828 - val_loss: 0.4267\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3773 - val_loss: 0.4707\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3922 - val_loss: 0.4213\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3725 - val_loss: 0.4775\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4007 - val_loss: 0.4619\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3861 - val_loss: 0.5029\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4156 - val_loss: 0.4216\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3600 - val_loss: 0.4207\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3648 - val_loss: 0.4318\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3508 - val_loss: 0.4181\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3512 - val_loss: 0.4175\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3580 - val_loss: 0.4188\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3486 - val_loss: 0.4167\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3436 - val_loss: 0.4197\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3481 - val_loss: 0.4175\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3440 - val_loss: 0.4155\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3496 - val_loss: 0.4152\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3362 - val_loss: 0.4144\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3603 - val_loss: 0.4142\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3384 - val_loss: 0.4137\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3626 - val_loss: 0.4185\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3617 - val_loss: 0.4136\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3426 - val_loss: 0.4190\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3418 - val_loss: 0.4131\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3490 - val_loss: 0.4132\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3480 - val_loss: 0.4133\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3483 - val_loss: 0.4182\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3401 - val_loss: 0.4172\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3428 - val_loss: 0.4150\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3457 - val_loss: 0.4134\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_09-18:44:32] Training the entire fine-tuned model...\n",
      "[2022_04_09-18:44:41] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3497WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0987s vs `on_train_batch_end` time: 0.1277s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0987s vs `on_train_batch_end` time: 0.1277s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 704ms/step - loss: 0.3497 - val_loss: 0.4140\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3462 - val_loss: 0.4173\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3460 - val_loss: 0.4139\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3435 - val_loss: 0.4128\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3367 - val_loss: 0.4127\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3336 - val_loss: 0.4130\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3480 - val_loss: 0.4130\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3245 - val_loss: 0.4129\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3312 - val_loss: 0.4135\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3334 - val_loss: 0.4140\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3305 - val_loss: 0.4138\n",
      "[2022_04_09-18:45:10] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-18:45:10] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:45:11] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3393WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0998s vs `on_train_batch_end` time: 0.1271s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0998s vs `on_train_batch_end` time: 0.1271s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 681ms/step - loss: 0.3393 - val_loss: 0.4127\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>472</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  472  21\n",
       "1   96  26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3076923076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1mkle5nv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 44250... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▄▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇▇▆▇▄▃▄▂▄▂▅▂▅█▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.41273</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33925</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41273</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">deft-terrain-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/1mkle5nv\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/1mkle5nv</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_184320-1mkle5nv/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1mkle5nv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/399e2tzx\" target=\"_blank\">quiet-meadow-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-18:46:22] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:46:22] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:46:22] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 717ms/step - loss: 0.8579 - val_loss: 0.5612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5884 - val_loss: 0.6985\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5650 - val_loss: 0.6334\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5249 - val_loss: 0.5941\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5062 - val_loss: 0.4707\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4674 - val_loss: 0.5100\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4447 - val_loss: 0.4713\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4468 - val_loss: 0.4796\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4401 - val_loss: 0.4671\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4286 - val_loss: 0.4619\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4284 - val_loss: 0.4642\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4301 - val_loss: 0.4618\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4231 - val_loss: 0.4606\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4201 - val_loss: 0.4615\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4245 - val_loss: 0.4614\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4199 - val_loss: 0.4593\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4214 - val_loss: 0.4589\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4215 - val_loss: 0.4585\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4208 - val_loss: 0.4581\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4194 - val_loss: 0.4579\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4189 - val_loss: 0.4581\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4209 - val_loss: 0.4571\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4180 - val_loss: 0.4565\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4177 - val_loss: 0.4562\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4188 - val_loss: 0.4559\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4107 - val_loss: 0.4556\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4166 - val_loss: 0.4552\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4152 - val_loss: 0.4552\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4110 - val_loss: 0.4542\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4108 - val_loss: 0.4540\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4133 - val_loss: 0.4536\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4126 - val_loss: 0.4537\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4119 - val_loss: 0.4534\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4107 - val_loss: 0.4525\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4093 - val_loss: 0.4521\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3988 - val_loss: 0.4519\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4062 - val_loss: 0.4516\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4070 - val_loss: 0.4512\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4034 - val_loss: 0.4508\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4045 - val_loss: 0.4509\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4053 - val_loss: 0.4504\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4004 - val_loss: 0.4499\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4081 - val_loss: 0.4494\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4062 - val_loss: 0.4493\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4062 - val_loss: 0.4489\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4101 - val_loss: 0.4485\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4015 - val_loss: 0.4482\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3973 - val_loss: 0.4481\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4049 - val_loss: 0.4482\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4022 - val_loss: 0.4484\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3998 - val_loss: 0.4474\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3946 - val_loss: 0.4470\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3975 - val_loss: 0.4467\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3953 - val_loss: 0.4469\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3965 - val_loss: 0.4464\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3931 - val_loss: 0.4462\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3980 - val_loss: 0.4460\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3936 - val_loss: 0.4454\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3972 - val_loss: 0.4456\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3960 - val_loss: 0.4461\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3925 - val_loss: 0.4446\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3882 - val_loss: 0.4443\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3896 - val_loss: 0.4441\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3903 - val_loss: 0.4440\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3879 - val_loss: 0.4442\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3946 - val_loss: 0.4441\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3892 - val_loss: 0.4437\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3918 - val_loss: 0.4434\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3888 - val_loss: 0.4433\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3921 - val_loss: 0.4432\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3899 - val_loss: 0.4432\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3867 - val_loss: 0.4431\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3830 - val_loss: 0.4433\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3863 - val_loss: 0.4435\n",
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3827 - val_loss: 0.4434\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3897 - val_loss: 0.4434\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 1s 229ms/step - loss: 0.3860 - val_loss: 0.4433\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3926 - val_loss: 0.4433\n",
      "[2022_04_09-18:48:00] Training the entire fine-tuned model...\n",
      "[2022_04_09-18:49:02] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 1s/step - loss: 0.3869 - val_loss: 0.4425\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3937 - val_loss: 0.4429\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3848 - val_loss: 0.4403\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3800 - val_loss: 0.4392\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3737 - val_loss: 0.4403\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3759 - val_loss: 0.4379\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3619 - val_loss: 0.4375\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3776 - val_loss: 0.4389\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3683 - val_loss: 0.4377\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3728 - val_loss: 0.4373\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3646 - val_loss: 0.4413\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3485 - val_loss: 0.4401\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3565 - val_loss: 0.4371\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3512 - val_loss: 0.4375\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3459 - val_loss: 0.4391\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3435 - val_loss: 0.4387\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3346 - val_loss: 0.4392\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3411 - val_loss: 0.4394\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3404 - val_loss: 0.4396\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_09-18:49:45] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-18:49:45] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:49:45] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 846ms/step - loss: 0.3542 - val_loss: 0.4371\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>504</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  504  32\n",
       "1  109  31"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3054187192118227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:399e2tzx) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 44623... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁</td></tr><tr><td>lr</td><td>██▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅█▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>12</td></tr><tr><td>best_val_loss</td><td>0.43707</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35421</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.4371</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">quiet-meadow-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/399e2tzx\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/399e2tzx</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_184606-399e2tzx/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:399e2tzx). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/sgzudnry\" target=\"_blank\">expert-breeze-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-18:50:41] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:50:41] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:50:41] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 585ms/step - loss: 0.8986 - val_loss: 0.8876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.6803 - val_loss: 0.5366\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5217 - val_loss: 0.4905\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4736 - val_loss: 0.4778\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4522 - val_loss: 0.4549\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4315 - val_loss: 0.4508\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4200 - val_loss: 0.4436\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4099 - val_loss: 0.4412\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4080 - val_loss: 0.4491\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3968 - val_loss: 0.4362\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3984 - val_loss: 0.4429\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4179 - val_loss: 0.4318\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3931 - val_loss: 0.4578\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4078 - val_loss: 0.5263\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4330 - val_loss: 0.4314\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3926 - val_loss: 0.4491\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.3939 - val_loss: 0.4240\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.3866 - val_loss: 0.5109\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4216 - val_loss: 0.4200\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.3983 - val_loss: 0.5182\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4239 - val_loss: 0.4764\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3871 - val_loss: 0.4715\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3865 - val_loss: 0.4180\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3595 - val_loss: 0.4161\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3401 - val_loss: 0.4298\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3469 - val_loss: 0.4149\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3451 - val_loss: 0.4149\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3266 - val_loss: 0.4221\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3422 - val_loss: 0.4145\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3435 - val_loss: 0.4146\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3421 - val_loss: 0.4181\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3468 - val_loss: 0.4158\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3310 - val_loss: 0.4136\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3525 - val_loss: 0.4225\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3525 - val_loss: 0.4158\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3324 - val_loss: 0.4140\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3471 - val_loss: 0.4139\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3297 - val_loss: 0.4139\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3362 - val_loss: 0.4137\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_09-18:51:34] Training the entire fine-tuned model...\n",
      "[2022_04_09-18:52:19] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3354WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1294s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1294s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 694ms/step - loss: 0.3354 - val_loss: 0.4146\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3339 - val_loss: 0.4185\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3260 - val_loss: 0.4146\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3237 - val_loss: 0.4170\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3167 - val_loss: 0.4172\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3306 - val_loss: 0.4149\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3133 - val_loss: 0.4137\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3315 - val_loss: 0.4136\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3115 - val_loss: 0.4152\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3195 - val_loss: 0.4174\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3164 - val_loss: 0.4160\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3178 - val_loss: 0.4144\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3183 - val_loss: 0.4146\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3101 - val_loss: 0.4146\n",
      "[2022_04_09-18:52:56] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-18:52:56] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:52:56] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3314WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0964s vs `on_train_batch_end` time: 0.1313s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0964s vs `on_train_batch_end` time: 0.1313s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 660ms/step - loss: 0.3314 - val_loss: 0.4135\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>470</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  470  23\n",
       "1   96  26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30409356725146197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:sgzudnry) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 45233... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▁▂▁▁▂▃▁▁▂▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.41351</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33143</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41351</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">expert-breeze-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/sgzudnry\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/sgzudnry</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_185026-sgzudnry/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:sgzudnry). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/2hggqxpu\" target=\"_blank\">firm-monkey-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-18:53:49] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:53:49] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:53:49] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 703ms/step - loss: 0.8410 - val_loss: 0.7482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5827 - val_loss: 0.5547\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4718 - val_loss: 0.4992\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4746 - val_loss: 0.4688\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4623 - val_loss: 0.5052\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4777 - val_loss: 0.5284\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4440 - val_loss: 0.4766\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4174 - val_loss: 0.4516\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4097 - val_loss: 0.4537\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3936 - val_loss: 0.4533\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4072 - val_loss: 0.4511\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4090 - val_loss: 0.4499\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4048 - val_loss: 0.4499\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3997 - val_loss: 0.4472\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4044 - val_loss: 0.4476\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4025 - val_loss: 0.4547\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3992 - val_loss: 0.4463\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3862 - val_loss: 0.4450\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3901 - val_loss: 0.4442\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3906 - val_loss: 0.4431\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3875 - val_loss: 0.4423\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3854 - val_loss: 0.4422\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3858 - val_loss: 0.4437\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3832 - val_loss: 0.4407\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3737 - val_loss: 0.4408\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3767 - val_loss: 0.4396\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3764 - val_loss: 0.4390\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3757 - val_loss: 0.4389\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3743 - val_loss: 0.4386\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3647 - val_loss: 0.4372\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3712 - val_loss: 0.4383\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3659 - val_loss: 0.4366\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3655 - val_loss: 0.4377\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3708 - val_loss: 0.4359\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3648 - val_loss: 0.4371\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3669 - val_loss: 0.4354\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3619 - val_loss: 0.4351\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3676 - val_loss: 0.4359\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3653 - val_loss: 0.4352\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3541 - val_loss: 0.4348\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3607 - val_loss: 0.4348\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3725 - val_loss: 0.4372\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3610 - val_loss: 0.4338\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3497 - val_loss: 0.4392\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3565 - val_loss: 0.4333\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3562 - val_loss: 0.4332\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3603 - val_loss: 0.4341\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3589 - val_loss: 0.4330\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3598 - val_loss: 0.4422\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3530 - val_loss: 0.4410\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3605 - val_loss: 0.4383\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3599 - val_loss: 0.4433\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3519 - val_loss: 0.4332\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3551 - val_loss: 0.4331\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-18:54:57] Training the entire fine-tuned model...\n",
      "[2022_04_09-18:55:28] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 833ms/step - loss: 0.3514 - val_loss: 0.4323\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.3531 - val_loss: 0.4337\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3555 - val_loss: 0.4340\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3537 - val_loss: 0.4323\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3440 - val_loss: 0.4317\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3311 - val_loss: 0.4331\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3522 - val_loss: 0.4314\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3339 - val_loss: 0.4321\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3414 - val_loss: 0.4315\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3366 - val_loss: 0.4316\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3387 - val_loss: 0.4315\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3320 - val_loss: 0.4312\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3270 - val_loss: 0.4312\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3232 - val_loss: 0.4313\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3318 - val_loss: 0.4314\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3425 - val_loss: 0.4314\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 407ms/step - loss: 0.3349 - val_loss: 0.4314\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3260 - val_loss: 0.4314\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3304 - val_loss: 0.4314\n",
      "[2022_04_09-18:56:11] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-18:56:11] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:56:11] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 838ms/step - loss: 0.3284 - val_loss: 0.4313\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>498</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  498  38\n",
       "1  104  36"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3364485981308411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2hggqxpu) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 45680... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██▁▁▂▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>12</td></tr><tr><td>best_val_loss</td><td>0.43116</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32845</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.43126</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">firm-monkey-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/2hggqxpu\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/2hggqxpu</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_185333-2hggqxpu/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2hggqxpu). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/1e5sx0n1\" target=\"_blank\">morning-microwave-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-18:57:05] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:57:05] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:57:05] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 591ms/step - loss: 0.9344 - val_loss: 0.8754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.6861 - val_loss: 0.5029\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5251 - val_loss: 0.4807\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4586 - val_loss: 0.4632\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4448 - val_loss: 0.4615\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4357 - val_loss: 0.4515\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4244 - val_loss: 0.4470\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4157 - val_loss: 0.4430\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4054 - val_loss: 0.4568\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4024 - val_loss: 0.4368\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4054 - val_loss: 0.4415\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4054 - val_loss: 0.4800\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4110 - val_loss: 0.4449\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3903 - val_loss: 0.4261\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3897 - val_loss: 0.4236\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3709 - val_loss: 0.4404\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3852 - val_loss: 0.4225\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3695 - val_loss: 0.4255\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3809 - val_loss: 0.4236\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3689 - val_loss: 0.4215\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3659 - val_loss: 0.4219\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3651 - val_loss: 0.4208\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3632 - val_loss: 0.4239\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3672 - val_loss: 0.4210\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3599 - val_loss: 0.4215\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3537 - val_loss: 0.4201\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3660 - val_loss: 0.4194\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3580 - val_loss: 0.4193\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3640 - val_loss: 0.4196\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3567 - val_loss: 0.4209\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3617 - val_loss: 0.4203\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3687 - val_loss: 0.4194\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3598 - val_loss: 0.4189\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3593 - val_loss: 0.4189\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3661 - val_loss: 0.4190\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3664 - val_loss: 0.4193\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3522 - val_loss: 0.4193\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3568 - val_loss: 0.4193\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3577 - val_loss: 0.4193\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3575 - val_loss: 0.4193\n",
      "[2022_04_09-18:58:00] Training the entire fine-tuned model...\n",
      "[2022_04_09-18:58:44] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3749WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0996s vs `on_train_batch_end` time: 0.1276s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0996s vs `on_train_batch_end` time: 0.1276s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 693ms/step - loss: 0.3749 - val_loss: 0.4190\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3651 - val_loss: 0.4262\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3564 - val_loss: 0.4211\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3360 - val_loss: 0.4147\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3356 - val_loss: 0.4261\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3316 - val_loss: 0.4187\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3360 - val_loss: 0.4241\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3292 - val_loss: 0.4145\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3031 - val_loss: 0.4191\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3065 - val_loss: 0.4132\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.2978 - val_loss: 0.4180\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3038 - val_loss: 0.4147\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.2905 - val_loss: 0.4147\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.2908 - val_loss: 0.4151\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.2944 - val_loss: 0.4183\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.2804 - val_loss: 0.4200\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "[2022_04_09-18:59:22] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-18:59:22] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-18:59:22] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3075WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1299s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1299s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 668ms/step - loss: 0.3075 - val_loss: 0.4140\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>475</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  475  18\n",
       "1   94  28"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1e5sx0n1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 46146... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇██▁▁▂▂▂▂▂▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>0.41315</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30748</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.41402</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">morning-microwave-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/1e5sx0n1\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/1e5sx0n1</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_185648-1e5sx0n1/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1e5sx0n1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/3foevi30\" target=\"_blank\">earnest-armadillo-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-19:00:18] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:00:18] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:00:18] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 727ms/step - loss: 0.9435 - val_loss: 0.4988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.6383 - val_loss: 0.7721\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.7745 - val_loss: 0.4877\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.5466 - val_loss: 0.4939\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5166 - val_loss: 0.4725\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4746 - val_loss: 0.4636\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.4460 - val_loss: 0.4556\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4180 - val_loss: 0.4528\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4185 - val_loss: 0.4521\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4056 - val_loss: 0.4478\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3986 - val_loss: 0.4442\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3875 - val_loss: 0.4461\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3914 - val_loss: 0.4500\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3939 - val_loss: 0.4424\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3816 - val_loss: 0.4383\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3698 - val_loss: 0.4366\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3718 - val_loss: 0.4362\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3715 - val_loss: 0.4491\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3749 - val_loss: 0.4499\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3697 - val_loss: 0.4337\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3619 - val_loss: 0.4370\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3665 - val_loss: 0.4376\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3520 - val_loss: 0.4317\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3462 - val_loss: 0.4397\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3415 - val_loss: 0.4312\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3566 - val_loss: 0.4341\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3458 - val_loss: 0.4302\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3373 - val_loss: 0.4329\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3398 - val_loss: 0.4307\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3354 - val_loss: 0.4327\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3411 - val_loss: 0.4320\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3334 - val_loss: 0.4323\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3341 - val_loss: 0.4340\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_09-19:01:05] Training the entire fine-tuned model...\n",
      "[2022_04_09-19:01:14] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 838ms/step - loss: 0.3838 - val_loss: 0.5054\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3790 - val_loss: 0.4373\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3657 - val_loss: 0.4991\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3795 - val_loss: 0.4765\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3451 - val_loss: 0.4414\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3253 - val_loss: 0.4345\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3169 - val_loss: 0.4313\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3067 - val_loss: 0.4320\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3131 - val_loss: 0.4328\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2932 - val_loss: 0.4329\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2952 - val_loss: 0.4321\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.2987 - val_loss: 0.4326\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2881 - val_loss: 0.4327\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "[2022_04_09-19:01:45] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-19:01:45] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:01:45] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 845ms/step - loss: 0.3081 - val_loss: 0.4311\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>498</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  498  38\n",
       "1  106  34"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32075471698113206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3foevi30) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 46547... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▇▇▇███▁▁▁▂▂▂▃▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▆▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████████▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>26</td></tr><tr><td>best_val_loss</td><td>0.43018</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30807</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.43107</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">earnest-armadillo-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/3foevi30\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/3foevi30</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_190002-3foevi30/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3foevi30). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/1vxzofvn\" target=\"_blank\">elated-pyramid-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-19:02:54] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:02:54] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:02:54] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 805ms/step - loss: 0.8398 - val_loss: 1.0294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.6759 - val_loss: 0.6447\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5603 - val_loss: 0.4629\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4850 - val_loss: 0.4725\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4435 - val_loss: 0.4865\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4342 - val_loss: 0.4587\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4405 - val_loss: 0.4465\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4355 - val_loss: 0.5343\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4544 - val_loss: 0.4370\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4031 - val_loss: 0.4349\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3956 - val_loss: 0.4694\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4080 - val_loss: 0.4312\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3982 - val_loss: 0.4515\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4071 - val_loss: 0.4448\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3834 - val_loss: 0.4450\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3860 - val_loss: 0.4259\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3822 - val_loss: 0.4254\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3716 - val_loss: 0.4272\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3697 - val_loss: 0.4243\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3580 - val_loss: 0.4292\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3653 - val_loss: 0.4223\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3536 - val_loss: 0.4221\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3630 - val_loss: 0.4224\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3712 - val_loss: 0.4237\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3563 - val_loss: 0.4220\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3590 - val_loss: 0.4210\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3520 - val_loss: 0.4206\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3540 - val_loss: 0.4239\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3608 - val_loss: 0.4210\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3543 - val_loss: 0.4192\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3587 - val_loss: 0.4218\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3663 - val_loss: 0.4206\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3584 - val_loss: 0.4173\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3570 - val_loss: 0.4274\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3453 - val_loss: 0.4182\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3510 - val_loss: 0.4225\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3528 - val_loss: 0.4172\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3495 - val_loss: 0.4168\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3412 - val_loss: 0.4165\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3447 - val_loss: 0.4171\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3501 - val_loss: 0.4171\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3505 - val_loss: 0.4166\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3484 - val_loss: 0.4163\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3481 - val_loss: 0.4163\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3438 - val_loss: 0.4164\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3436 - val_loss: 0.4162\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3466 - val_loss: 0.4160\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3476 - val_loss: 0.4161\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3500 - val_loss: 0.4165\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3540 - val_loss: 0.4164\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3456 - val_loss: 0.4164\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3524 - val_loss: 0.4165\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3419 - val_loss: 0.4165\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "[2022_04_09-19:04:03] Training the entire fine-tuned model...\n",
      "[2022_04_09-19:04:32] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4891WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1315s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1315s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 681ms/step - loss: 0.4891 - val_loss: 0.4241\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3533 - val_loss: 0.4154\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3592 - val_loss: 0.4122\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3558 - val_loss: 0.4495\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3517 - val_loss: 0.4124\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3400 - val_loss: 0.4277\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3198 - val_loss: 0.4112\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.2986 - val_loss: 0.4241\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.2910 - val_loss: 0.4076\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.2874 - val_loss: 0.4083\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.2812 - val_loss: 0.4095\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.2729 - val_loss: 0.4098\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.2662 - val_loss: 0.4126\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.2529 - val_loss: 0.4099\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.2552 - val_loss: 0.4110\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "[2022_04_09-19:05:08] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-19:05:08] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:05:08] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.2893WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0993s vs `on_train_batch_end` time: 0.1292s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0993s vs `on_train_batch_end` time: 0.1292s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 688ms/step - loss: 0.2893 - val_loss: 0.4080\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>463</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  463  30\n",
       "1   92  30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32967032967032966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1vxzofvn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 46891... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇██▁▁▂▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▆▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.4076</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.28928</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.408</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">elated-pyramid-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/1vxzofvn\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/1vxzofvn</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_190238-1vxzofvn/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1vxzofvn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/2gach130\" target=\"_blank\">rosy-glade-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-19:06:01] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:06:01] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:06:01] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 712ms/step - loss: 0.8518 - val_loss: 0.5972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.6072 - val_loss: 0.7127\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5666 - val_loss: 0.5429\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4892 - val_loss: 0.5003\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4766 - val_loss: 0.4679\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 223ms/step - loss: 0.4474 - val_loss: 0.4548\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 230ms/step - loss: 0.4235 - val_loss: 0.4551\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4012 - val_loss: 0.4665\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4288 - val_loss: 0.4480\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3981 - val_loss: 0.4420\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3963 - val_loss: 0.4438\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3908 - val_loss: 0.4411\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3854 - val_loss: 0.4493\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3724 - val_loss: 0.4365\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3733 - val_loss: 0.4346\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3722 - val_loss: 0.4504\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3677 - val_loss: 0.4372\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3719 - val_loss: 0.4392\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3544 - val_loss: 0.4623\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3594 - val_loss: 0.4344\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3502 - val_loss: 0.4347\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3517 - val_loss: 0.4406\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3410 - val_loss: 0.4334\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3579 - val_loss: 0.4335\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3608 - val_loss: 0.4391\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3581 - val_loss: 0.4336\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3566 - val_loss: 0.4343\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3463 - val_loss: 0.4347\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3510 - val_loss: 0.4331\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3486 - val_loss: 0.4321\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3447 - val_loss: 0.4317\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3455 - val_loss: 0.4316\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3379 - val_loss: 0.4327\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3474 - val_loss: 0.4321\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3473 - val_loss: 0.4318\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3372 - val_loss: 0.4318\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3517 - val_loss: 0.4317\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3461 - val_loss: 0.4317\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3421 - val_loss: 0.4318\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3451 - val_loss: 0.4317\n",
      "[2022_04_09-19:06:55] Training the entire fine-tuned model...\n",
      "[2022_04_09-19:07:40] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 837ms/step - loss: 0.3461 - val_loss: 0.4321\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3402 - val_loss: 0.4323\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3429 - val_loss: 0.4316\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3535 - val_loss: 0.4315\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3497 - val_loss: 0.4310\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3486 - val_loss: 0.4311\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3473 - val_loss: 0.4312\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3386 - val_loss: 0.4308\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3360 - val_loss: 0.4309\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3434 - val_loss: 0.4308\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3396 - val_loss: 0.4307\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3468 - val_loss: 0.4306\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3407 - val_loss: 0.4319\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3377 - val_loss: 0.4312\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3390 - val_loss: 0.4307\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3338 - val_loss: 0.4307\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3459 - val_loss: 0.4307\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3287 - val_loss: 0.4308\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3441 - val_loss: 0.4310\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3465 - val_loss: 0.4310\n",
      "[2022_04_09-19:08:25] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-19:08:25] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:08:27] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 874ms/step - loss: 0.3390 - val_loss: 0.4306\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>501</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  501  35\n",
       "1  107  33"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3173076923076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2gach130) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 47346... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▄▄▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅█▃▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.43062</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33898</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43062</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">rosy-glade-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/2gach130\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/2gach130</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_190545-2gach130/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2gach130). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/i3h5yvry\" target=\"_blank\">restful-dragon-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-19:09:24] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:09:24] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:09:24] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 571ms/step - loss: 0.9551 - val_loss: 0.9818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.7369 - val_loss: 0.5884\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5824 - val_loss: 0.5058\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5248 - val_loss: 0.4810\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4638 - val_loss: 0.4561\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4417 - val_loss: 0.4727\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4401 - val_loss: 0.4624\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4182 - val_loss: 0.4843\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4151 - val_loss: 0.4791\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4198 - val_loss: 0.4483\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4240 - val_loss: 0.4603\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4076 - val_loss: 0.4448\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4039 - val_loss: 0.4417\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3925 - val_loss: 0.4350\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3977 - val_loss: 0.4332\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3924 - val_loss: 0.4351\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3906 - val_loss: 0.4344\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3913 - val_loss: 0.4315\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3875 - val_loss: 0.4338\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3927 - val_loss: 0.4323\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3757 - val_loss: 0.4301\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3778 - val_loss: 0.4310\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3822 - val_loss: 0.4289\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3788 - val_loss: 0.4287\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3776 - val_loss: 0.4287\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3815 - val_loss: 0.4275\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3605 - val_loss: 0.4280\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3708 - val_loss: 0.4265\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3733 - val_loss: 0.4248\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3679 - val_loss: 0.4290\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3653 - val_loss: 0.4237\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3751 - val_loss: 0.4235\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3756 - val_loss: 0.4267\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3629 - val_loss: 0.4236\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3687 - val_loss: 0.4259\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3596 - val_loss: 0.4211\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3630 - val_loss: 0.4205\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3605 - val_loss: 0.4238\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3546 - val_loss: 0.4205\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3640 - val_loss: 0.4197\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3665 - val_loss: 0.4178\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3520 - val_loss: 0.4201\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3554 - val_loss: 0.4170\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3538 - val_loss: 0.4168\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3631 - val_loss: 0.4218\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3567 - val_loss: 0.4158\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3578 - val_loss: 0.4172\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3433 - val_loss: 0.4173\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3565 - val_loss: 0.4217\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3564 - val_loss: 0.4158\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3454 - val_loss: 0.4138\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3492 - val_loss: 0.4137\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3571 - val_loss: 0.4135\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3483 - val_loss: 0.4168\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3609 - val_loss: 0.4133\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3518 - val_loss: 0.4131\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3540 - val_loss: 0.4138\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3492 - val_loss: 0.4141\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3371 - val_loss: 0.4134\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3476 - val_loss: 0.4134\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3481 - val_loss: 0.4134\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3437 - val_loss: 0.4140\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3490 - val_loss: 0.4146\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3460 - val_loss: 0.4149\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_09-19:10:44] Training the entire fine-tuned model...\n",
      "[2022_04_09-19:10:53] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3428WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1002s vs `on_train_batch_end` time: 0.1286s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1002s vs `on_train_batch_end` time: 0.1286s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 682ms/step - loss: 0.3428 - val_loss: 0.4151\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3386 - val_loss: 0.4143\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3433 - val_loss: 0.4134\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3336 - val_loss: 0.4151\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3362 - val_loss: 0.4141\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3391 - val_loss: 0.4137\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3468 - val_loss: 0.4151\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3373 - val_loss: 0.4148\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3281 - val_loss: 0.4147\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3327 - val_loss: 0.4141\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3386 - val_loss: 0.4139\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_09-19:11:22] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-19:11:22] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:11:22] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3435WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1010s vs `on_train_batch_end` time: 0.1287s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1010s vs `on_train_batch_end` time: 0.1287s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 699ms/step - loss: 0.3435 - val_loss: 0.4134\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>478</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  478  15\n",
       "1  103  19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24358974358974358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:i3h5yvry) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 47746... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▆▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>55</td></tr><tr><td>best_val_loss</td><td>0.41311</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34346</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41342</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">restful-dragon-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/i3h5yvry\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/i3h5yvry</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_190906-i3h5yvry/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:i3h5yvry). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/3ilez243\" target=\"_blank\">dark-moon-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-19:12:18] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:12:18] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:12:18] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 737ms/step - loss: 0.8396 - val_loss: 0.6326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5806 - val_loss: 0.5880\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5214 - val_loss: 0.4797\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4673 - val_loss: 0.4750\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4464 - val_loss: 0.4870\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4360 - val_loss: 0.4837\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4146 - val_loss: 0.4812\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4225 - val_loss: 0.4481\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4034 - val_loss: 0.4459\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3970 - val_loss: 0.4460\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3851 - val_loss: 0.4402\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3873 - val_loss: 0.4384\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3840 - val_loss: 0.4645\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3851 - val_loss: 0.4364\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3768 - val_loss: 0.4379\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3833 - val_loss: 0.4379\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3737 - val_loss: 0.4419\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3702 - val_loss: 0.4337\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3501 - val_loss: 0.4421\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3539 - val_loss: 0.4394\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3542 - val_loss: 0.4451\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3433 - val_loss: 0.4294\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3464 - val_loss: 0.4297\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3444 - val_loss: 0.4336\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3349 - val_loss: 0.4311\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3243 - val_loss: 0.4281\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3408 - val_loss: 0.4382\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3356 - val_loss: 0.4274\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3577 - val_loss: 0.4640\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3531 - val_loss: 0.4300\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3281 - val_loss: 0.4990\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3879 - val_loss: 0.4309\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3268 - val_loss: 0.4512\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3412 - val_loss: 0.4291\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3254 - val_loss: 0.4289\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3350 - val_loss: 0.4274\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_09-19:13:07] Training the entire fine-tuned model...\n",
      "[2022_04_09-19:13:17] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 824ms/step - loss: 0.3396 - val_loss: 0.4261\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3357 - val_loss: 0.4265\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3299 - val_loss: 0.4302\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3222 - val_loss: 0.4265\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3222 - val_loss: 0.4273\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3233 - val_loss: 0.4270\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3120 - val_loss: 0.4272\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3199 - val_loss: 0.4271\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3075 - val_loss: 0.4271\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_09-19:13:42] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-19:13:42] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:13:42] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 822ms/step - loss: 0.3322 - val_loss: 0.4265\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>501</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  501  35\n",
       "1  105  35"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3ilez243) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 48243... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▂▃</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████████████▃▃▃▃▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▃▃▃▃▃▂▂▁▁▂▁▁▂▁▂▁▂▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42609</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33223</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42649</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dark-moon-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/3ilez243\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/3ilez243</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_191201-3ilez243/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3ilez243). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/2m95f55j\" target=\"_blank\">sunny-meadow-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-19:14:36] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:14:36] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:14:36] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 586ms/step - loss: 0.9093 - val_loss: 0.7564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.6373 - val_loss: 0.4869\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4881 - val_loss: 0.4920\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4698 - val_loss: 0.5554\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5159 - val_loss: 0.4761\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4490 - val_loss: 0.4580\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4228 - val_loss: 0.4441\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4083 - val_loss: 0.4577\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4287 - val_loss: 0.4495\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4120 - val_loss: 0.4632\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4112 - val_loss: 0.4414\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3976 - val_loss: 0.4306\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3755 - val_loss: 0.4356\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3790 - val_loss: 0.4300\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3771 - val_loss: 0.4868\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3931 - val_loss: 0.4868\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3946 - val_loss: 0.4368\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3659 - val_loss: 0.4677\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3678 - val_loss: 0.4494\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3766 - val_loss: 0.4205\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3674 - val_loss: 0.4210\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3554 - val_loss: 0.4342\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3525 - val_loss: 0.4214\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3512 - val_loss: 0.4239\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3483 - val_loss: 0.4244\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3510 - val_loss: 0.4222\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3439 - val_loss: 0.4200\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3452 - val_loss: 0.4186\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3457 - val_loss: 0.4203\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3553 - val_loss: 0.4198\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3470 - val_loss: 0.4183\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3435 - val_loss: 0.4184\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3554 - val_loss: 0.4188\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3477 - val_loss: 0.4219\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3522 - val_loss: 0.4192\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3539 - val_loss: 0.4192\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3438 - val_loss: 0.4188\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3401 - val_loss: 0.4188\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3330 - val_loss: 0.4189\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_09-19:15:30] Training the entire fine-tuned model...\n",
      "[2022_04_09-19:15:38] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3446WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1002s vs `on_train_batch_end` time: 0.1282s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1002s vs `on_train_batch_end` time: 0.1282s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 720ms/step - loss: 0.3446 - val_loss: 0.4306\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3640 - val_loss: 0.4192\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3484 - val_loss: 0.4241\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3566 - val_loss: 0.4220\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3321 - val_loss: 0.4187\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3334 - val_loss: 0.4199\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3347 - val_loss: 0.4154\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3296 - val_loss: 0.4160\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3267 - val_loss: 0.4242\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3199 - val_loss: 0.4156\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3194 - val_loss: 0.4263\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3198 - val_loss: 0.4191\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3120 - val_loss: 0.4164\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3155 - val_loss: 0.4176\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3066 - val_loss: 0.4207\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_09-19:16:15] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-19:16:15] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:16:15] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3240WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0984s vs `on_train_batch_end` time: 0.1282s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0984s vs `on_train_batch_end` time: 0.1282s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 709ms/step - loss: 0.3240 - val_loss: 0.4156\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>475</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  475  18\n",
       "1   98  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2926829268292683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2m95f55j) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 48569... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▂▂▂▂▂▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>6</td></tr><tr><td>best_val_loss</td><td>0.41535</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.324</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41564</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sunny-meadow-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/2m95f55j\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/2m95f55j</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_191419-2m95f55j/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2m95f55j). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/esfd5mlz\" target=\"_blank\">trim-sun-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-19:17:21] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:17:21] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:17:21] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 722ms/step - loss: 0.9623 - val_loss: 0.5069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.6261 - val_loss: 0.6866\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5879 - val_loss: 0.5680\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4859 - val_loss: 0.5614\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4754 - val_loss: 0.5235\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4730 - val_loss: 0.4606\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.4424 - val_loss: 0.5003\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 228ms/step - loss: 0.4439 - val_loss: 0.4576\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.4323 - val_loss: 0.4693\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.4320 - val_loss: 0.4611\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.4208 - val_loss: 0.4568\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4209 - val_loss: 0.4535\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.4201 - val_loss: 0.4517\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4128 - val_loss: 0.4517\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4085 - val_loss: 0.4500\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4050 - val_loss: 0.4488\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4054 - val_loss: 0.4479\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4034 - val_loss: 0.4477\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3971 - val_loss: 0.4460\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4080 - val_loss: 0.4457\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3982 - val_loss: 0.4447\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3888 - val_loss: 0.4443\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3955 - val_loss: 0.4440\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3891 - val_loss: 0.4438\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3905 - val_loss: 0.4450\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3907 - val_loss: 0.4418\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3903 - val_loss: 0.4414\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3866 - val_loss: 0.4428\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3843 - val_loss: 0.4399\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3820 - val_loss: 0.4395\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3846 - val_loss: 0.4389\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3841 - val_loss: 0.4404\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3841 - val_loss: 0.4398\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3793 - val_loss: 0.4389\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3760 - val_loss: 0.4377\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3761 - val_loss: 0.4372\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3719 - val_loss: 0.4366\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3766 - val_loss: 0.4361\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3715 - val_loss: 0.4386\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3728 - val_loss: 0.4365\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3645 - val_loss: 0.4364\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3732 - val_loss: 0.4350\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3627 - val_loss: 0.4353\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3658 - val_loss: 0.4356\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3647 - val_loss: 0.4341\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3665 - val_loss: 0.4344\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3586 - val_loss: 0.4335\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3567 - val_loss: 0.4350\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3620 - val_loss: 0.4336\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3556 - val_loss: 0.4389\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3614 - val_loss: 0.4329\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3578 - val_loss: 0.4323\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3500 - val_loss: 0.4380\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3520 - val_loss: 0.4318\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3578 - val_loss: 0.4314\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3493 - val_loss: 0.4326\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3528 - val_loss: 0.4306\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3549 - val_loss: 0.4307\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3538 - val_loss: 0.4303\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3393 - val_loss: 0.4312\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3477 - val_loss: 0.4301\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3524 - val_loss: 0.4327\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3494 - val_loss: 0.4310\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3503 - val_loss: 0.4310\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3486 - val_loss: 0.4311\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3455 - val_loss: 0.4314\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3506 - val_loss: 0.4295\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3438 - val_loss: 0.4292\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3481 - val_loss: 0.4293\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3510 - val_loss: 0.4291\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3500 - val_loss: 0.4295\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3492 - val_loss: 0.4294\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3466 - val_loss: 0.4293\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3424 - val_loss: 0.4293\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3388 - val_loss: 0.4292\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3475 - val_loss: 0.4292\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3437 - val_loss: 0.4292\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3480 - val_loss: 0.4292\n",
      "[2022_04_09-19:18:57] Training the entire fine-tuned model...\n",
      "[2022_04_09-19:20:01] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 878ms/step - loss: 0.3529 - val_loss: 0.4324\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3553 - val_loss: 0.4293\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3296 - val_loss: 0.4292\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3309 - val_loss: 0.4284\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3293 - val_loss: 0.4312\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3340 - val_loss: 0.4300\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3428 - val_loss: 0.4297\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3211 - val_loss: 0.4290\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3198 - val_loss: 0.4314\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3181 - val_loss: 0.4291\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3072 - val_loss: 0.4301\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3091 - val_loss: 0.4298\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_09-19:20:31] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-19:20:31] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:20:31] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 866ms/step - loss: 0.3329 - val_loss: 0.4285\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>497</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  497  39\n",
       "1  101  39"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3577981651376147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:esfd5mlz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 48942... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▁▂▂▂</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅█▆▅▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>0.42842</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33294</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.42854</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">trim-sun-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/esfd5mlz\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/esfd5mlz</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_191704-esfd5mlz/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:esfd5mlz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/khhkc9y2\" target=\"_blank\">glad-dragon-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-19:21:43] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:21:43] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:21:43] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 571ms/step - loss: 0.7804 - val_loss: 0.7960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.7617 - val_loss: 0.4745\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.5667 - val_loss: 0.5698\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5172 - val_loss: 0.4718\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4537 - val_loss: 0.4498\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4428 - val_loss: 0.5063\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4698 - val_loss: 0.5320\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4223 - val_loss: 0.5405\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4600 - val_loss: 0.4374\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4312 - val_loss: 0.4692\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4217 - val_loss: 0.4904\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4085 - val_loss: 0.4294\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4025 - val_loss: 0.4720\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4346 - val_loss: 0.5387\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4148 - val_loss: 0.4269\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3868 - val_loss: 0.4316\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3741 - val_loss: 0.4288\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3750 - val_loss: 0.4257\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3642 - val_loss: 0.4531\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3820 - val_loss: 0.4292\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3655 - val_loss: 0.4221\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3518 - val_loss: 0.4188\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3546 - val_loss: 0.4162\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3519 - val_loss: 0.4240\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3670 - val_loss: 0.4293\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3467 - val_loss: 0.4219\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3433 - val_loss: 0.4922\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.3924 - val_loss: 0.4298\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3671 - val_loss: 0.4253\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.3551 - val_loss: 0.4246\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3382 - val_loss: 0.4343\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_09-19:22:26] Training the entire fine-tuned model...\n",
      "[2022_04_09-19:22:35] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3512WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0982s vs `on_train_batch_end` time: 0.1289s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0982s vs `on_train_batch_end` time: 0.1289s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 703ms/step - loss: 0.3512 - val_loss: 0.4322\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3703 - val_loss: 0.4150\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3450 - val_loss: 0.4128\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3437 - val_loss: 0.4196\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3349 - val_loss: 0.4130\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3199 - val_loss: 0.4108\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3162 - val_loss: 0.4062\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3154 - val_loss: 0.4097\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.2996 - val_loss: 0.4103\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.2934 - val_loss: 0.4223\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.2834 - val_loss: 0.4151\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.2791 - val_loss: 0.4218\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.2787 - val_loss: 0.4185\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.2656 - val_loss: 0.4183\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.2774 - val_loss: 0.4179\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_09-19:23:12] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-19:23:12] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:23:12] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3106WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1002s vs `on_train_batch_end` time: 0.1265s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1002s vs `on_train_batch_end` time: 0.1265s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 679ms/step - loss: 0.3106 - val_loss: 0.4092\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>481</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  481  12\n",
       "1   97  25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31446540880503143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:khhkc9y2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 667... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇███▁▁▂▂▂▂▃▃▃▄▄▄▁</td></tr><tr><td>loss</td><td>██▅▄▄▃▃▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂</td></tr><tr><td>lr</td><td>███████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▄▂▂▃▃▂▂▃▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>6</td></tr><tr><td>best_val_loss</td><td>0.40619</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31058</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.40922</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">glad-dragon-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/khhkc9y2\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/khhkc9y2</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_192127-khhkc9y2/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:khhkc9y2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/2te9dqnd\" target=\"_blank\">confused-jazz-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-19:24:16] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:24:16] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:24:16] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 697ms/step - loss: 0.8989 - val_loss: 0.6887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.6292 - val_loss: 0.7970\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.6436 - val_loss: 0.6441\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.5325 - val_loss: 0.5540\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4761 - val_loss: 0.5247\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4708 - val_loss: 0.4791\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4231 - val_loss: 0.4550\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4128 - val_loss: 0.4547\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4148 - val_loss: 0.4532\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4034 - val_loss: 0.4507\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4061 - val_loss: 0.4454\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3833 - val_loss: 0.4405\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3820 - val_loss: 0.4402\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3935 - val_loss: 0.4460\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3824 - val_loss: 0.4374\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3635 - val_loss: 0.4406\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3795 - val_loss: 0.4411\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3674 - val_loss: 0.4345\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3691 - val_loss: 0.4331\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3606 - val_loss: 0.4345\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3575 - val_loss: 0.4452\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3622 - val_loss: 0.4340\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3507 - val_loss: 0.4435\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3617 - val_loss: 0.4360\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3514 - val_loss: 0.4361\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 2s 407ms/step - loss: 0.3468 - val_loss: 0.4350\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3524 - val_loss: 0.4332\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_09-19:24:56] Training the entire fine-tuned model...\n",
      "[2022_04_09-19:25:13] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 843ms/step - loss: 0.4256 - val_loss: 0.4509\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3975 - val_loss: 0.4314\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3594 - val_loss: 0.4404\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3413 - val_loss: 0.4297\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3725 - val_loss: 0.4385\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3442 - val_loss: 0.4483\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3301 - val_loss: 0.4393\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3157 - val_loss: 0.4520\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3055 - val_loss: 0.4425\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.2992 - val_loss: 0.4441\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.2870 - val_loss: 0.4427\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.2906 - val_loss: 0.4426\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_09-19:25:43] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-19:25:43] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:26:00] Validation set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 848ms/step - loss: 0.3455 - val_loss: 0.4299\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>520</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  520  16\n",
       "1  119  21"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23728813559322035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_09_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2te9dqnd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1069... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▅▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▁▁▁▁▁▁▂</td></tr><tr><td>lr</td><td>███████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆█▅▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>0.42967</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34554</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.42994</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">confused-jazz-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/2te9dqnd\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/2te9dqnd</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_192400-2te9dqnd/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2te9dqnd). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/3kx9j6m8\" target=\"_blank\">fluent-aardvark-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-19:26:54] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:26:54] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:26:54] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 596ms/step - loss: 0.9052 - val_loss: 0.8349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.6210 - val_loss: 0.5450\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5201 - val_loss: 0.5144\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.5102 - val_loss: 0.4995\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4764 - val_loss: 0.4698\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4431 - val_loss: 0.4614\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4319 - val_loss: 0.4585\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4207 - val_loss: 0.4452\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4292 - val_loss: 0.4979\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4214 - val_loss: 0.4399\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3965 - val_loss: 0.4371\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3898 - val_loss: 0.4425\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3928 - val_loss: 0.4349\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3831 - val_loss: 0.4279\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3729 - val_loss: 0.4254\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3713 - val_loss: 0.4695\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3999 - val_loss: 0.4360\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3832 - val_loss: 0.4302\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3776 - val_loss: 0.4817\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3883 - val_loss: 0.4442\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3765 - val_loss: 0.4203\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3695 - val_loss: 0.4210\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3592 - val_loss: 0.4378\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3588 - val_loss: 0.4187\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3511 - val_loss: 0.4262\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3544 - val_loss: 0.4172\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3629 - val_loss: 0.4171\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3507 - val_loss: 0.4215\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3592 - val_loss: 0.4174\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3462 - val_loss: 0.4165\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3494 - val_loss: 0.4170\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3406 - val_loss: 0.4160\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3417 - val_loss: 0.4207\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3404 - val_loss: 0.4158\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3342 - val_loss: 0.4160\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3519 - val_loss: 0.4155\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3402 - val_loss: 0.4227\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3592 - val_loss: 0.4149\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3349 - val_loss: 0.4147\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3361 - val_loss: 0.4221\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3445 - val_loss: 0.4158\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3488 - val_loss: 0.4180\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3325 - val_loss: 0.4141\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3362 - val_loss: 0.4126\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3231 - val_loss: 0.4140\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3462 - val_loss: 0.4124\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3489 - val_loss: 0.4149\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3303 - val_loss: 0.4119\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3344 - val_loss: 0.4129\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3297 - val_loss: 0.4111\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3330 - val_loss: 0.4118\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3372 - val_loss: 0.4113\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3301 - val_loss: 0.4137\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3290 - val_loss: 0.4106\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3295 - val_loss: 0.4124\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3306 - val_loss: 0.4126\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3272 - val_loss: 0.4098\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3257 - val_loss: 0.4130\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3270 - val_loss: 0.4117\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3213 - val_loss: 0.4095\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3287 - val_loss: 0.4113\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3272 - val_loss: 0.4174\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3447 - val_loss: 0.4093\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3382 - val_loss: 0.4204\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3306 - val_loss: 0.4094\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3348 - val_loss: 0.4175\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3271 - val_loss: 0.4105\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3133 - val_loss: 0.4102\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3387 - val_loss: 0.4155\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3145 - val_loss: 0.4117\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3260 - val_loss: 0.4092\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.3231 - val_loss: 0.4086\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.3111 - val_loss: 0.4092\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.3184 - val_loss: 0.4105\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 177ms/step - loss: 0.3130 - val_loss: 0.4084\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.3096 - val_loss: 0.4086\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 177ms/step - loss: 0.3297 - val_loss: 0.4089\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 181ms/step - loss: 0.3093 - val_loss: 0.4095\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 180ms/step - loss: 0.3107 - val_loss: 0.4097\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 0.3186 - val_loss: 0.4092\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 0.3115 - val_loss: 0.4091\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.3262 - val_loss: 0.4092\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.3178 - val_loss: 0.4094\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_09-19:28:35] Training the entire fine-tuned model...\n",
      "[2022_04_09-19:28:43] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4062WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0961s vs `on_train_batch_end` time: 0.1292s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0961s vs `on_train_batch_end` time: 0.1292s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 810ms/step - loss: 0.4062 - val_loss: 0.4308\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3494 - val_loss: 0.4045\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3339 - val_loss: 0.5085\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3541 - val_loss: 0.4135\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.2824 - val_loss: 0.4142\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.2779 - val_loss: 0.4323\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.2685 - val_loss: 0.4362\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.2736 - val_loss: 0.4147\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.2679 - val_loss: 0.4201\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.2579 - val_loss: 0.4092\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_09-19:29:11] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-19:29:11] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:29:11] Validation set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3285WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1293s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1293s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 696ms/step - loss: 0.3285 - val_loss: 0.3996\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>478</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  478  15\n",
       "1   96  26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31901840490797545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_09_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3kx9j6m8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1412... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▁▂▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▂▂▂▁▁▂</td></tr><tr><td>lr</td><td>████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.39965</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32846</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.39965</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fluent-aardvark-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/3kx9j6m8\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/3kx9j6m8</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_192638-3kx9j6m8/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3kx9j6m8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/4mjielba\" target=\"_blank\">bright-dew-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-19:30:05] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:30:05] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:30:05] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 701ms/step - loss: 0.8295 - val_loss: 0.6823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.6669 - val_loss: 0.7001\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5413 - val_loss: 0.5591\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5121 - val_loss: 0.5495\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5137 - val_loss: 0.4636\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4883 - val_loss: 0.4755\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4268 - val_loss: 0.4527\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4272 - val_loss: 0.4561\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4134 - val_loss: 0.4472\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4251 - val_loss: 0.4424\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4287 - val_loss: 0.4689\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4072 - val_loss: 0.4396\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3942 - val_loss: 0.4404\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3839 - val_loss: 0.4384\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3793 - val_loss: 0.4464\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3726 - val_loss: 0.4384\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3663 - val_loss: 0.4644\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-19:30:35] Training the entire fine-tuned model...\n",
      "[2022_04_09-19:30:43] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 847ms/step - loss: 0.3692 - val_loss: 0.4388\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3839 - val_loss: 0.4385\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3770 - val_loss: 0.4377\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3812 - val_loss: 0.4375\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3742 - val_loss: 0.4384\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3756 - val_loss: 0.4387\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3744 - val_loss: 0.4382\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_09-19:31:05] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-19:31:05] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:31:05] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 856ms/step - loss: 0.3772 - val_loss: 0.4374\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>523</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>130</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  523   5\n",
       "1  130  12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1509433962264151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:4mjielba) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2061... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██▁▁▂▂▃▃▄▁</td></tr><tr><td>loss</td><td>█▆▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▄▄▂▂▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.43743</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37719</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43743</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">bright-dew-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/4mjielba\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/4mjielba</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_192949-4mjielba/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:4mjielba). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/220ikt9c\" target=\"_blank\">eternal-tree-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-19:32:00] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:32:00] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:32:00] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 604ms/step - loss: 0.8090 - val_loss: 0.8348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.6132 - val_loss: 0.4847\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4707 - val_loss: 0.4624\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4589 - val_loss: 0.4462\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4370 - val_loss: 0.4526\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4245 - val_loss: 0.4420\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4239 - val_loss: 0.4341\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4137 - val_loss: 0.4775\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4036 - val_loss: 0.4326\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4026 - val_loss: 0.4421\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4075 - val_loss: 0.4225\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3815 - val_loss: 0.4208\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3937 - val_loss: 0.5032\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3994 - val_loss: 0.4182\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3695 - val_loss: 0.4173\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3674 - val_loss: 0.4155\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3626 - val_loss: 0.4409\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3907 - val_loss: 0.4720\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3784 - val_loss: 0.4317\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-19:32:32] Training the entire fine-tuned model...\n",
      "[2022_04_09-19:32:41] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3542WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1046s vs `on_train_batch_end` time: 0.1259s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1046s vs `on_train_batch_end` time: 0.1259s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 684ms/step - loss: 0.3542 - val_loss: 0.4152\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3545 - val_loss: 0.4159\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3555 - val_loss: 0.4149\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3499 - val_loss: 0.4146\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3522 - val_loss: 0.4143\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3542 - val_loss: 0.4147\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3520 - val_loss: 0.4140\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3554 - val_loss: 0.4140\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3309 - val_loss: 0.4141\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3497 - val_loss: 0.4141\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_09-19:33:08] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-19:33:09] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:33:28] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3476WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1005s vs `on_train_batch_end` time: 0.1290s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1005s vs `on_train_batch_end` time: 0.1290s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 695ms/step - loss: 0.3476 - val_loss: 0.4139\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>488</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  488  13\n",
       "1  101  19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:220ikt9c) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2283... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██▁▁▂▂▃▃▃▄▄▅▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▂▂▁▁▂▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.41393</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34757</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41393</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">eternal-tree-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/220ikt9c\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/220ikt9c</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_193144-220ikt9c/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:220ikt9c). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/u7su4okx\" target=\"_blank\">breezy-dream-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-19:34:24] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:34:24] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:34:24] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 720ms/step - loss: 0.8431 - val_loss: 0.6789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.6247 - val_loss: 0.7260\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.5819 - val_loss: 0.4977\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.5154 - val_loss: 0.4903\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4744 - val_loss: 0.4843\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4645 - val_loss: 0.4589\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4472 - val_loss: 0.4850\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4247 - val_loss: 0.4570\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4071 - val_loss: 0.5022\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4328 - val_loss: 0.4404\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4007 - val_loss: 0.4465\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3866 - val_loss: 0.4375\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3828 - val_loss: 0.4453\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3817 - val_loss: 0.4348\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3738 - val_loss: 0.4371\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3716 - val_loss: 0.4329\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3710 - val_loss: 0.4314\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 226ms/step - loss: 0.3676 - val_loss: 0.4544\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3799 - val_loss: 0.4406\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 230ms/step - loss: 0.3727 - val_loss: 0.4385\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-19:34:56] Training the entire fine-tuned model...\n",
      "[2022_04_09-19:35:05] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 829ms/step - loss: 0.3663 - val_loss: 0.4319\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3760 - val_loss: 0.4365\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3683 - val_loss: 0.4327\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3539 - val_loss: 0.4397\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_09-19:35:20] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-19:35:20] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:35:21] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 842ms/step - loss: 0.3752 - val_loss: 0.4321\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>520</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>122</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  520   8\n",
       "1  122  20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23529411764705882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:u7su4okx) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2548... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇█▃▂▂▂▂▂▃▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>16</td></tr><tr><td>best_val_loss</td><td>0.43139</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37523</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43207</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">breezy-dream-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/u7su4okx\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/u7su4okx</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_193407-u7su4okx/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:u7su4okx). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/dfcws4wq\" target=\"_blank\">tough-donkey-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-19:36:15] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:36:15] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:36:15] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 835ms/step - loss: 0.9159 - val_loss: 0.7056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.6047 - val_loss: 0.4681\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.5296 - val_loss: 0.5323\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5111 - val_loss: 0.4530\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4967 - val_loss: 0.4788\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4541 - val_loss: 0.5026\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4757 - val_loss: 0.4416\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4266 - val_loss: 0.4752\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4416 - val_loss: 0.4382\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4207 - val_loss: 0.4725\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4197 - val_loss: 0.5291\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4225 - val_loss: 0.4454\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-19:36:40] Training the entire fine-tuned model...\n",
      "[2022_04_09-19:36:48] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4066WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1023s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1023s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 685ms/step - loss: 0.4066 - val_loss: 0.4317\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3980 - val_loss: 0.4300\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3805 - val_loss: 0.4304\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3834 - val_loss: 0.4289\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3689 - val_loss: 0.4297\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3720 - val_loss: 0.4289\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3630 - val_loss: 0.4274\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3591 - val_loss: 0.4293\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3591 - val_loss: 0.4258\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3481 - val_loss: 0.4253\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3441 - val_loss: 0.4272\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3363 - val_loss: 0.4270\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3304 - val_loss: 0.4354\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_09-19:37:20] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-19:37:20] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:37:21] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3470WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0984s vs `on_train_batch_end` time: 0.1272s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0984s vs `on_train_batch_end` time: 0.1272s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 678ms/step - loss: 0.3470 - val_loss: 0.4270\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>473</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  473  28\n",
       "1   93  27"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30857142857142855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:dfcws4wq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2772... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇▁▂▂▃▃▄▅▅▆▆▇▇█▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▄▂▂▃▁▂▁▂▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>0.42525</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34696</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42704</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">tough-donkey-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/dfcws4wq\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/dfcws4wq</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_193559-dfcws4wq/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:dfcws4wq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/lthmbjq4\" target=\"_blank\">summer-elevator-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-19:38:14] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:38:14] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:38:14] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 705ms/step - loss: 0.7726 - val_loss: 0.6925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5843 - val_loss: 0.5633\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4992 - val_loss: 0.4909\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4640 - val_loss: 0.4737\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4433 - val_loss: 0.4740\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4321 - val_loss: 0.4628\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4314 - val_loss: 0.5019\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4333 - val_loss: 0.4441\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4117 - val_loss: 0.4407\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3999 - val_loss: 0.4745\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3984 - val_loss: 0.4463\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3856 - val_loss: 0.4747\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-19:38:39] Training the entire fine-tuned model...\n",
      "[2022_04_09-19:38:47] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 837ms/step - loss: 0.3930 - val_loss: 0.4410\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3996 - val_loss: 0.4388\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3865 - val_loss: 0.4385\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3847 - val_loss: 0.4456\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3728 - val_loss: 0.4377\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3678 - val_loss: 0.4391\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3661 - val_loss: 0.4379\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3707 - val_loss: 0.4356\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3522 - val_loss: 0.4349\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3502 - val_loss: 0.4345\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3317 - val_loss: 0.4335\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3264 - val_loss: 0.4353\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3156 - val_loss: 0.4371\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3281 - val_loss: 0.4610\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_09-19:39:21] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-19:39:21] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:39:34] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 822ms/step - loss: 0.3326 - val_loss: 0.4343\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>501</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  501  27\n",
       "1  107  35"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3431372549019608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:lthmbjq4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2987... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▄▅▅▆▆▇▁▂▂▃▃▄▄▅▅▆▆▇▇█▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▂▃▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_val_loss</td><td>0.43349</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33257</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.43431</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">summer-elevator-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/lthmbjq4\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/lthmbjq4</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_193759-lthmbjq4/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:lthmbjq4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/i7dk1tus\" target=\"_blank\">magic-sky-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-19:40:26] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:40:26] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:40:26] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 565ms/step - loss: 0.8781 - val_loss: 0.8340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.6624 - val_loss: 0.4905\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5386 - val_loss: 0.5076\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.5014 - val_loss: 0.5206\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4527 - val_loss: 0.4630\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4403 - val_loss: 0.4441\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4414 - val_loss: 0.4453\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4153 - val_loss: 0.4333\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4176 - val_loss: 0.4567\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4119 - val_loss: 0.4543\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4066 - val_loss: 0.4359\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-19:40:49] Training the entire fine-tuned model...\n",
      "[2022_04_09-19:40:57] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4169WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0963s vs `on_train_batch_end` time: 0.1284s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0963s vs `on_train_batch_end` time: 0.1284s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 701ms/step - loss: 0.4169 - val_loss: 0.4535\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.4071 - val_loss: 0.4550\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4114 - val_loss: 0.4277\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3863 - val_loss: 0.4238\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3926 - val_loss: 0.4243\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3803 - val_loss: 0.4236\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3700 - val_loss: 0.4228\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3510 - val_loss: 0.4232\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3453 - val_loss: 0.4367\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3534 - val_loss: 0.4412\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_09-19:41:25] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-19:41:25] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:41:45] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3517WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0989s vs `on_train_batch_end` time: 0.1262s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0989s vs `on_train_batch_end` time: 0.1262s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 678ms/step - loss: 0.3517 - val_loss: 0.4255\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>481</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  481  20\n",
       "1   96  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2926829268292683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:i7dk1tus) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3236... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▅▅▆▇▇█▁▂▂▃▄▅▅▆▇▇▁</td></tr><tr><td>loss</td><td>█▅▄▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▂▁▁▁▂▂▁▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>6</td></tr><tr><td>best_val_loss</td><td>0.42281</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35173</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.42547</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">magic-sky-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/i7dk1tus\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/i7dk1tus</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_194011-i7dk1tus/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:i7dk1tus). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/1ku0s5ra\" target=\"_blank\">icy-sound-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-19:42:40] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:42:40] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:42:40] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 726ms/step - loss: 0.9871 - val_loss: 0.5012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.6622 - val_loss: 0.7740\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.6832 - val_loss: 0.4851\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5213 - val_loss: 0.5444\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4856 - val_loss: 0.4720\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4660 - val_loss: 0.5127\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4656 - val_loss: 0.4716\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4567 - val_loss: 0.5210\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4351 - val_loss: 0.4659\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4330 - val_loss: 0.4805\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4074 - val_loss: 0.4399\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4008 - val_loss: 0.4416\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3957 - val_loss: 0.4417\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4005 - val_loss: 0.4397\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3994 - val_loss: 0.4906\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3985 - val_loss: 0.4497\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4062 - val_loss: 0.4323\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4089 - val_loss: 0.5228\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4117 - val_loss: 0.4811\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3779 - val_loss: 0.4818\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-19:43:12] Training the entire fine-tuned model...\n",
      "[2022_04_09-19:43:38] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 1s/step - loss: 0.4155 - val_loss: 0.6069\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.4281 - val_loss: 0.4913\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3910 - val_loss: 0.4958\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3850 - val_loss: 0.4640\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3713 - val_loss: 0.4651\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3532 - val_loss: 0.4476\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3390 - val_loss: 0.4434\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3254 - val_loss: 0.4425\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3129 - val_loss: 0.4435\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3005 - val_loss: 0.4416\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.2679 - val_loss: 0.4601\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.2452 - val_loss: 0.5279\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3152 - val_loss: 0.6726\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_09-19:44:11] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-19:44:11] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:44:11] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 833ms/step - loss: 0.2869 - val_loss: 0.4402\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>501</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  501  27\n",
       "1  110  32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31840796019900497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1ku0s5ra) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3441... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▂▃▃▄▄▄▅▅▅▁</td></tr><tr><td>loss</td><td>█▅▅▄▃▃▃▃▃▃▃▂▂▂▂▂▃▃▃▂▃▃▂▂▂▂▂▂▂▂▁▁▂▁</td></tr><tr><td>lr</td><td>████████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂█▂▃▂▃▂▃▂▂▁▁▁▁▂▁▁▃▂▂▅▂▂▂▂▁▁▁▁▁▂▃▆▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>16</td></tr><tr><td>best_val_loss</td><td>0.43234</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.28692</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.44023</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">icy-sound-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/1ku0s5ra\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/1ku0s5ra</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_194223-1ku0s5ra/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1ku0s5ra). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/1tl69izy\" target=\"_blank\">earnest-thunder-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-19:45:05] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:45:05] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:45:05] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 593ms/step - loss: 0.8965 - val_loss: 1.4157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.8075 - val_loss: 0.8149\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.6453 - val_loss: 0.6630\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.5783 - val_loss: 0.5256\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4854 - val_loss: 0.4518\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4955 - val_loss: 0.4460\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4258 - val_loss: 0.4424\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4217 - val_loss: 0.4631\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4129 - val_loss: 0.4510\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4114 - val_loss: 0.4415\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4057 - val_loss: 0.4339\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4003 - val_loss: 0.4306\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3952 - val_loss: 0.4394\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4000 - val_loss: 0.4771\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3821 - val_loss: 0.4497\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-19:45:33] Training the entire fine-tuned model...\n",
      "[2022_04_09-19:45:42] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4816WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1271s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1271s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 681ms/step - loss: 0.4816 - val_loss: 0.4355\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4013 - val_loss: 0.4528\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3951 - val_loss: 0.4347\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3740 - val_loss: 0.4404\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3714 - val_loss: 0.4547\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3608 - val_loss: 0.4167\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3355 - val_loss: 0.4190\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.2987 - val_loss: 0.4522\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3071 - val_loss: 0.4462\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_09-19:46:07] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-19:46:07] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:46:08] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3278WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0982s vs `on_train_batch_end` time: 0.1261s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0982s vs `on_train_batch_end` time: 0.1261s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 910ms/step - loss: 0.3278 - val_loss: 0.4179\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>465</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  465  36\n",
       "1   96  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1tl69izy) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3707... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█▁▁▂▃▃▃▄▅▅▁</td></tr><tr><td>loss</td><td>█▇▅▄▃▃▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.41667</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32782</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.41789</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">earnest-thunder-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/1tl69izy\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/1tl69izy</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_194449-1tl69izy/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1tl69izy). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/2iwieknt\" target=\"_blank\">silver-capybara-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-19:47:02] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:47:02] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:47:02] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 713ms/step - loss: 0.8088 - val_loss: 0.7159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.6419 - val_loss: 0.6861\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5238 - val_loss: 0.5081\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.4687 - val_loss: 0.4984\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4392 - val_loss: 0.4590\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4240 - val_loss: 0.4603\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 230ms/step - loss: 0.4121 - val_loss: 0.4531\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4146 - val_loss: 0.4604\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4068 - val_loss: 0.4540\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3998 - val_loss: 0.4430\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3990 - val_loss: 0.4416\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3825 - val_loss: 0.4354\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3918 - val_loss: 0.4335\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3874 - val_loss: 0.4711\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3870 - val_loss: 0.4352\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3631 - val_loss: 0.4453\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3660 - val_loss: 0.4299\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3619 - val_loss: 0.4299\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3568 - val_loss: 0.4370\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3667 - val_loss: 0.4317\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3568 - val_loss: 0.4294\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3646 - val_loss: 0.4293\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3601 - val_loss: 0.4308\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3667 - val_loss: 0.4336\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3561 - val_loss: 0.4332\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3724 - val_loss: 0.4325\n",
      "[2022_04_09-19:47:40] Training the entire fine-tuned model...\n",
      "[2022_04_09-19:47:59] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 843ms/step - loss: 0.3668 - val_loss: 0.4302\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3636 - val_loss: 0.4317\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3708 - val_loss: 0.4309\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3498 - val_loss: 0.4309\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3584 - val_loss: 0.4308\n",
      "[2022_04_09-19:48:17] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-19:48:17] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:48:27] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 855ms/step - loss: 0.3556 - val_loss: 0.4302\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>520</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  520   8\n",
       "1  125  17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20359281437125748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2iwieknt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3929... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▅▄▃▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▃▃▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>21</td></tr><tr><td>best_val_loss</td><td>0.42935</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35562</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43025</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">silver-capybara-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/2iwieknt\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/2iwieknt</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_194646-2iwieknt/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2iwieknt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/22u0y1iq\" target=\"_blank\">charmed-sound-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-19:49:22] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:49:22] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:49:22] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 565ms/step - loss: 0.9259 - val_loss: 1.2911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.8357 - val_loss: 0.7394\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.6009 - val_loss: 0.5427\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5492 - val_loss: 0.4915\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4616 - val_loss: 0.4537\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4418 - val_loss: 0.4474\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4302 - val_loss: 0.4472\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4119 - val_loss: 0.4456\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4153 - val_loss: 0.4375\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4029 - val_loss: 0.4348\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3987 - val_loss: 0.4348\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3906 - val_loss: 0.4373\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3850 - val_loss: 0.4331\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3869 - val_loss: 0.4240\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3680 - val_loss: 0.4237\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3741 - val_loss: 0.4317\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3670 - val_loss: 0.4319\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3821 - val_loss: 0.4224\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3578 - val_loss: 0.4325\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3676 - val_loss: 0.4212\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3476 - val_loss: 0.4218\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3451 - val_loss: 0.4313\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3455 - val_loss: 0.4334\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3492 - val_loss: 0.4266\n",
      "[2022_04_09-19:49:59] Training the entire fine-tuned model...\n",
      "[2022_04_09-19:50:19] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3507WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1016s vs `on_train_batch_end` time: 0.1266s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1016s vs `on_train_batch_end` time: 0.1266s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 697ms/step - loss: 0.3507 - val_loss: 0.4207\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3460 - val_loss: 0.4204\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3517 - val_loss: 0.4208\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3403 - val_loss: 0.4211\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3399 - val_loss: 0.4203\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3441 - val_loss: 0.4204\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3396 - val_loss: 0.4200\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3569 - val_loss: 0.4200\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3375 - val_loss: 0.4198\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3397 - val_loss: 0.4198\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3405 - val_loss: 0.4197\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3434 - val_loss: 0.4196\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3442 - val_loss: 0.4196\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3428 - val_loss: 0.4196\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3523 - val_loss: 0.4196\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3337 - val_loss: 0.4196\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3366 - val_loss: 0.4196\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3448 - val_loss: 0.4196\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "[2022_04_09-19:51:03] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-19:51:03] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:51:03] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3532WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1252s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1252s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 690ms/step - loss: 0.3532 - val_loss: 0.4196\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>488</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  488  13\n",
       "1  101  19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:22u0y1iq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 4184... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▆▆▆▆▇▇▇██▁▁▂▂▃▃▃▃▄▄▄▅▅▅▆▆▁</td></tr><tr><td>loss</td><td>█▇▄▄▃▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.41958</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35325</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41958</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">charmed-sound-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/22u0y1iq\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/22u0y1iq</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_194905-22u0y1iq/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:22u0y1iq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/dwwasdr4\" target=\"_blank\">super-wood-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-19:52:00] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:52:00] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:52:00] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 736ms/step - loss: 0.9087 - val_loss: 0.5858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.6426 - val_loss: 0.6954\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.5375 - val_loss: 0.5347\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5035 - val_loss: 0.5239\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4666 - val_loss: 0.4638\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4772 - val_loss: 0.4571\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4463 - val_loss: 0.4885\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4361 - val_loss: 0.4553\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4222 - val_loss: 0.4937\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4128 - val_loss: 0.4677\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4053 - val_loss: 0.4701\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3970 - val_loss: 0.4391\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3897 - val_loss: 0.4389\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3895 - val_loss: 0.4469\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3928 - val_loss: 0.4389\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3841 - val_loss: 0.4376\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3818 - val_loss: 0.4426\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3775 - val_loss: 0.4399\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3795 - val_loss: 0.4367\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3761 - val_loss: 0.4379\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3921 - val_loss: 0.4398\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3796 - val_loss: 0.4353\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3806 - val_loss: 0.4372\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3801 - val_loss: 0.4351\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3704 - val_loss: 0.4372\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3829 - val_loss: 0.4343\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3762 - val_loss: 0.4365\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3756 - val_loss: 0.4378\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3669 - val_loss: 0.4330\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3670 - val_loss: 0.4375\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3769 - val_loss: 0.4333\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3696 - val_loss: 0.4319\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3669 - val_loss: 0.4337\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3723 - val_loss: 0.4319\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3679 - val_loss: 0.4424\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3582 - val_loss: 0.4349\n",
      "[2022_04_09-19:52:50] Training the entire fine-tuned model...\n",
      "[2022_04_09-19:52:58] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 843ms/step - loss: 0.3619 - val_loss: 0.4332\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3680 - val_loss: 0.4330\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3638 - val_loss: 0.4336\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3526 - val_loss: 0.4379\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3552 - val_loss: 0.4329\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3553 - val_loss: 0.4352\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3433 - val_loss: 0.4341\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3426 - val_loss: 0.4342\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3399 - val_loss: 0.4345\n",
      "[2022_04_09-19:53:24] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-19:53:24] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:53:24] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 850ms/step - loss: 0.3501 - val_loss: 0.4333\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>518</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  518  10\n",
       "1  123  19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2222222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:dwwasdr4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 4505... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▂▃</td></tr><tr><td>loss</td><td>█▅▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅█▄▃▂▂▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>31</td></tr><tr><td>best_val_loss</td><td>0.43193</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35009</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43328</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">super-wood-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/dwwasdr4\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/dwwasdr4</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_195144-dwwasdr4/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:dwwasdr4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/39rbxfdw\" target=\"_blank\">polished-lion-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-19:54:19] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:54:19] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:54:19] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 573ms/step - loss: 1.0787 - val_loss: 0.8341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.6671 - val_loss: 0.7008\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.6254 - val_loss: 0.4938\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5043 - val_loss: 0.4545\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4765 - val_loss: 0.4714\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4343 - val_loss: 0.4876\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4478 - val_loss: 0.4464\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4279 - val_loss: 0.4888\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4373 - val_loss: 0.5052\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4291 - val_loss: 0.4314\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3981 - val_loss: 0.4345\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3941 - val_loss: 0.4267\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3781 - val_loss: 0.4239\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3872 - val_loss: 0.4586\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3888 - val_loss: 0.4196\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3768 - val_loss: 0.4215\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3567 - val_loss: 0.4196\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3626 - val_loss: 0.4202\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3574 - val_loss: 0.4172\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3624 - val_loss: 0.4161\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3449 - val_loss: 0.4175\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3561 - val_loss: 0.4161\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3579 - val_loss: 0.4175\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3493 - val_loss: 0.4165\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3431 - val_loss: 0.4159\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3557 - val_loss: 0.4161\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3501 - val_loss: 0.4158\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3515 - val_loss: 0.4158\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3442 - val_loss: 0.4159\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3497 - val_loss: 0.4158\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3550 - val_loss: 0.4158\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3558 - val_loss: 0.4158\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3361 - val_loss: 0.4158\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3558 - val_loss: 0.4158\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3500 - val_loss: 0.4158\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3403 - val_loss: 0.4158\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3414 - val_loss: 0.4158\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3474 - val_loss: 0.4158\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3519 - val_loss: 0.4158\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3472 - val_loss: 0.4158\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3450 - val_loss: 0.4158\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3541 - val_loss: 0.4158\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3413 - val_loss: 0.4158\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3552 - val_loss: 0.4158\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3435 - val_loss: 0.4158\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3561 - val_loss: 0.4158\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3509 - val_loss: 0.4158\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3441 - val_loss: 0.4158\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3466 - val_loss: 0.4158\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3372 - val_loss: 0.4158\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3467 - val_loss: 0.4158\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3458 - val_loss: 0.4158\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3508 - val_loss: 0.4158\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3449 - val_loss: 0.4158\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3484 - val_loss: 0.4158\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3529 - val_loss: 0.4158\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3476 - val_loss: 0.4158\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3530 - val_loss: 0.4158\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3365 - val_loss: 0.4158\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3328 - val_loss: 0.4158\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3440 - val_loss: 0.4158\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3424 - val_loss: 0.4158\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3457 - val_loss: 0.4158\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3375 - val_loss: 0.4158\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3454 - val_loss: 0.4158\n",
      "[2022_04_09-19:55:40] Training the entire fine-tuned model...\n",
      "[2022_04_09-19:55:57] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3540WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0984s vs `on_train_batch_end` time: 0.1303s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0984s vs `on_train_batch_end` time: 0.1303s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 704ms/step - loss: 0.3540 - val_loss: 0.4290\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3549 - val_loss: 0.4191\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3495 - val_loss: 0.4166\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3432 - val_loss: 0.4144\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3222 - val_loss: 0.4157\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3260 - val_loss: 0.4164\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3192 - val_loss: 0.4194\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3218 - val_loss: 0.4198\n",
      "[2022_04_09-19:56:20] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-19:56:20] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:56:20] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3379WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1015s vs `on_train_batch_end` time: 0.1272s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1015s vs `on_train_batch_end` time: 0.1272s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 693ms/step - loss: 0.3379 - val_loss: 0.4144\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>485</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  485  16\n",
       "1   96  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:39rbxfdw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 4832... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇███▁▁▁▂▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.41439</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33789</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41439</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">polished-lion-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/39rbxfdw\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/39rbxfdw</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_195403-39rbxfdw/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:39rbxfdw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/1bimigx3\" target=\"_blank\">expert-waterfall-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-19:57:19] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:57:19] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:57:19] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 736ms/step - loss: 0.9956 - val_loss: 0.5677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.6381 - val_loss: 0.7067\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.6886 - val_loss: 0.5050\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5608 - val_loss: 0.5125\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.5503 - val_loss: 0.5028\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4819 - val_loss: 0.4569\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4591 - val_loss: 0.4725\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4322 - val_loss: 0.4483\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4417 - val_loss: 0.4469\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4212 - val_loss: 0.4619\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4102 - val_loss: 0.4396\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4130 - val_loss: 0.4656\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3949 - val_loss: 0.4390\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3923 - val_loss: 0.4510\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3831 - val_loss: 0.4341\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3813 - val_loss: 0.4324\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3756 - val_loss: 0.4531\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3851 - val_loss: 0.4383\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3689 - val_loss: 0.4483\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3780 - val_loss: 0.4303\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3645 - val_loss: 0.4297\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3689 - val_loss: 0.4356\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3681 - val_loss: 0.4293\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3609 - val_loss: 0.4305\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3640 - val_loss: 0.4340\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3510 - val_loss: 0.4291\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3632 - val_loss: 0.4289\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3496 - val_loss: 0.4315\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3497 - val_loss: 0.4301\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3445 - val_loss: 0.4288\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3509 - val_loss: 0.4282\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3499 - val_loss: 0.4334\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3545 - val_loss: 0.4283\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3573 - val_loss: 0.4294\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3590 - val_loss: 0.4297\n",
      "[2022_04_09-19:58:09] Training the entire fine-tuned model...\n",
      "[2022_04_09-19:58:16] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 875ms/step - loss: 0.3665 - val_loss: 0.4331\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3375 - val_loss: 0.4340\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3434 - val_loss: 0.4351\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3375 - val_loss: 0.4322\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3263 - val_loss: 0.4393\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3190 - val_loss: 0.4330\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3019 - val_loss: 0.4324\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.2933 - val_loss: 0.4332\n",
      "[2022_04_09-19:58:40] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-19:58:40] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:58:43] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 830ms/step - loss: 0.3173 - val_loss: 0.4334\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>517</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  517  11\n",
       "1  119  23"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26136363636363635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1bimigx3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5311... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇██▁▁▁▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▄▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅█▃▃▃▂▂▂▁▂▂▁▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>30</td></tr><tr><td>best_val_loss</td><td>0.42824</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31734</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.43337</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">expert-waterfall-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/1bimigx3\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/1bimigx3</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_195703-1bimigx3/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1bimigx3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/2gpdbt5f\" target=\"_blank\">lively-plasma-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-19:59:38] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:59:38] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-19:59:38] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 590ms/step - loss: 0.9621 - val_loss: 1.2796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.8230 - val_loss: 0.8074\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.6604 - val_loss: 0.6697\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5224 - val_loss: 0.5211\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4976 - val_loss: 0.4577\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4586 - val_loss: 0.4432\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4295 - val_loss: 0.4474\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4160 - val_loss: 0.4428\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4141 - val_loss: 0.4419\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4218 - val_loss: 0.4435\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4056 - val_loss: 0.4381\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3874 - val_loss: 0.4275\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3918 - val_loss: 0.4389\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3813 - val_loss: 0.4381\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3828 - val_loss: 0.4252\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3735 - val_loss: 0.4324\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3748 - val_loss: 0.4479\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3807 - val_loss: 0.4450\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3837 - val_loss: 0.4216\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3656 - val_loss: 0.4206\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3686 - val_loss: 0.4203\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3541 - val_loss: 0.4186\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3498 - val_loss: 0.4179\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3538 - val_loss: 0.4176\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3552 - val_loss: 0.4193\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3438 - val_loss: 0.4167\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3493 - val_loss: 0.4171\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3501 - val_loss: 0.4163\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3536 - val_loss: 0.4157\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3579 - val_loss: 0.4154\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3480 - val_loss: 0.4152\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3549 - val_loss: 0.4195\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3441 - val_loss: 0.4182\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3407 - val_loss: 0.4144\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3489 - val_loss: 0.4138\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3411 - val_loss: 0.4139\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3425 - val_loss: 0.4139\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3407 - val_loss: 0.4141\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3437 - val_loss: 0.4140\n",
      "[2022_04_09-20:00:32] Training the entire fine-tuned model...\n",
      "[2022_04_09-20:00:40] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3487WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0976s vs `on_train_batch_end` time: 0.1281s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0976s vs `on_train_batch_end` time: 0.1281s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 671ms/step - loss: 0.3487 - val_loss: 0.4159\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3366 - val_loss: 0.4263\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3517 - val_loss: 0.4146\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3243 - val_loss: 0.4147\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3370 - val_loss: 0.4293\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3181 - val_loss: 0.4233\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3131 - val_loss: 0.4180\n",
      "[2022_04_09-20:01:02] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-20:01:02] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:01:02] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3220WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1022s vs `on_train_batch_end` time: 0.1251s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1022s vs `on_train_batch_end` time: 0.1251s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 871ms/step - loss: 0.3220 - val_loss: 0.4162\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>491</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  491  10\n",
       "1  103  17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23129251700680273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2gpdbt5f) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5632... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▆▅▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>34</td></tr><tr><td>best_val_loss</td><td>0.41384</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32198</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.41615</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">lively-plasma-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/2gpdbt5f\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/2gpdbt5f</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_195921-2gpdbt5f/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2gpdbt5f). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/2zpk0zaf\" target=\"_blank\">sage-water-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-20:01:56] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:01:56] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:01:56] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 769ms/step - loss: 1.0296 - val_loss: 0.6222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.6357 - val_loss: 0.6134\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.6476 - val_loss: 0.4977\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.5178 - val_loss: 0.4950\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4785 - val_loss: 0.4674\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4301 - val_loss: 0.4593\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4295 - val_loss: 0.4534\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4336 - val_loss: 0.4756\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4290 - val_loss: 0.4467\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4140 - val_loss: 0.4492\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4198 - val_loss: 0.4645\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4032 - val_loss: 0.4453\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3951 - val_loss: 0.4647\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3914 - val_loss: 0.4369\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3852 - val_loss: 0.4349\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3830 - val_loss: 0.4544\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3727 - val_loss: 0.4378\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3664 - val_loss: 0.4629\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3729 - val_loss: 0.4350\n",
      "[2022_04_09-20:02:28] Training the entire fine-tuned model...\n",
      "[2022_04_09-20:02:53] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 873ms/step - loss: 0.4510 - val_loss: 0.5819\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.4524 - val_loss: 0.4810\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.4290 - val_loss: 0.5073\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3832 - val_loss: 0.4681\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3781 - val_loss: 0.4615\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3682 - val_loss: 0.4436\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3536 - val_loss: 0.4464\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3358 - val_loss: 0.4381\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3296 - val_loss: 0.4453\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3125 - val_loss: 0.4435\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.2929 - val_loss: 0.4496\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.2765 - val_loss: 0.4512\n",
      "[2022_04_09-20:03:25] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-20:03:25] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:03:25] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 850ms/step - loss: 0.3220 - val_loss: 0.4395\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>518</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  518  10\n",
       "1  124  18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21176470588235294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2zpk0zaf) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5988... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██▁▁▂▂▃▃▃▄▄▅▅▅▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▃▃▂▂▂▃▁▂▂▁▂▁▁▂▁▂▁▆▃▄▂▂▁▁▁▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>14</td></tr><tr><td>best_val_loss</td><td>0.43487</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32196</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.43953</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sage-water-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/2zpk0zaf\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/2zpk0zaf</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_200139-2zpk0zaf/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2zpk0zaf). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/2mck5708\" target=\"_blank\">frosty-oath-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-20:04:34] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:04:34] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:04:34] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 564ms/step - loss: 0.8040 - val_loss: 0.8308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.6673 - val_loss: 0.4702\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4947 - val_loss: 0.4917\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4747 - val_loss: 0.4609\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4375 - val_loss: 0.4458\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4208 - val_loss: 0.4502\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4225 - val_loss: 0.4375\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4034 - val_loss: 0.4351\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4133 - val_loss: 0.4334\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3927 - val_loss: 0.4435\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4059 - val_loss: 0.4330\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3959 - val_loss: 0.4951\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4136 - val_loss: 0.4286\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4152 - val_loss: 0.5620\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4303 - val_loss: 0.4259\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3757 - val_loss: 0.4266\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3582 - val_loss: 0.4216\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3460 - val_loss: 0.4231\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3603 - val_loss: 0.4377\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3529 - val_loss: 0.4194\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3518 - val_loss: 0.4181\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3737 - val_loss: 0.4207\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3546 - val_loss: 0.4164\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3429 - val_loss: 0.4161\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3360 - val_loss: 0.4199\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3325 - val_loss: 0.4188\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3390 - val_loss: 0.4141\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3396 - val_loss: 0.5097\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3932 - val_loss: 0.4150\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3479 - val_loss: 0.4873\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3614 - val_loss: 0.4276\n",
      "[2022_04_09-20:05:18] Training the entire fine-tuned model...\n",
      "[2022_04_09-20:05:26] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4704WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0982s vs `on_train_batch_end` time: 0.1281s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0982s vs `on_train_batch_end` time: 0.1281s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 689ms/step - loss: 0.4704 - val_loss: 0.5627\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3979 - val_loss: 0.4192\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3289 - val_loss: 0.4218\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3371 - val_loss: 0.4311\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3108 - val_loss: 0.4175\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.2943 - val_loss: 0.4194\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.2857 - val_loss: 0.4266\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.2801 - val_loss: 0.5505\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3019 - val_loss: 0.4574\n",
      "[2022_04_09-20:05:51] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-20:05:51] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:05:51] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3039WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0990s vs `on_train_batch_end` time: 0.1263s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0990s vs `on_train_batch_end` time: 0.1263s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 683ms/step - loss: 0.3039 - val_loss: 0.4227\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>448</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  448  53\n",
       "1   81  39"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36792452830188677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2mck5708) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6244... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▆▄▄▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▄▃▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████████████▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▂▂▂▁▁▁▁▁▂▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▂▁▃▁▁▁▁▁▁▃▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>26</td></tr><tr><td>best_val_loss</td><td>0.41412</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30393</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.42273</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">frosty-oath-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/2mck5708\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/2mck5708</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_200419-2mck5708/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2mck5708). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/3ilp456q\" target=\"_blank\">feasible-monkey-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-20:06:45] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:06:45] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:06:45] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 724ms/step - loss: 0.7986 - val_loss: 0.6355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.6294 - val_loss: 0.7223\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5491 - val_loss: 0.5513\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4855 - val_loss: 0.5298\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4794 - val_loss: 0.4609\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4366 - val_loss: 0.4610\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4229 - val_loss: 0.4544\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4122 - val_loss: 0.4500\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4125 - val_loss: 0.4974\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4165 - val_loss: 0.4466\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3994 - val_loss: 0.4471\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4017 - val_loss: 0.4884\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4179 - val_loss: 0.4556\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3894 - val_loss: 0.4415\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3830 - val_loss: 0.4545\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3806 - val_loss: 0.4383\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3769 - val_loss: 0.4381\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3693 - val_loss: 0.4435\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3755 - val_loss: 0.4349\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3730 - val_loss: 0.4348\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3749 - val_loss: 0.4402\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3723 - val_loss: 0.4351\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3743 - val_loss: 0.4351\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3681 - val_loss: 0.4369\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3685 - val_loss: 0.4366\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3678 - val_loss: 0.4347\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3714 - val_loss: 0.4343\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3650 - val_loss: 0.4343\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3669 - val_loss: 0.4343\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3678 - val_loss: 0.4347\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3705 - val_loss: 0.4347\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3655 - val_loss: 0.4347\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3608 - val_loss: 0.4347\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3620 - val_loss: 0.4347\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3670 - val_loss: 0.4347\n",
      "[2022_04_09-20:07:34] Training the entire fine-tuned model...\n",
      "[2022_04_09-20:07:43] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 851ms/step - loss: 0.3643 - val_loss: 0.4343\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3630 - val_loss: 0.4357\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3713 - val_loss: 0.4351\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3668 - val_loss: 0.4345\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3707 - val_loss: 0.4345\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3549 - val_loss: 0.4347\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3633 - val_loss: 0.4346\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_09-20:08:04] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-20:08:04] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:08:04] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 866ms/step - loss: 0.3705 - val_loss: 0.4344\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>522</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  522   6\n",
       "1  127  15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18404907975460122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3ilp456q) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6543... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆█▄▃▂▂▁▁▃▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>28</td></tr><tr><td>best_val_loss</td><td>0.43426</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37054</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43436</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">feasible-monkey-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/3ilp456q\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/3ilp456q</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_200629-3ilp456q/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3ilp456q). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/19jvvrdf\" target=\"_blank\">scarlet-morning-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-20:08:58] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:08:58] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:08:58] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 589ms/step - loss: 0.8836 - val_loss: 0.8831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.6218 - val_loss: 0.5365\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5330 - val_loss: 0.4746\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4952 - val_loss: 0.5452\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4877 - val_loss: 0.4831\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4492 - val_loss: 0.4469\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4182 - val_loss: 0.4509\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4118 - val_loss: 0.4537\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4069 - val_loss: 0.4838\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4194 - val_loss: 0.4391\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4092 - val_loss: 0.4318\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3994 - val_loss: 0.4391\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3960 - val_loss: 0.4303\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3911 - val_loss: 0.4356\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3839 - val_loss: 0.4290\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3867 - val_loss: 0.4294\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3844 - val_loss: 0.4272\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3777 - val_loss: 0.4266\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3798 - val_loss: 0.4297\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3806 - val_loss: 0.4262\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3722 - val_loss: 0.4262\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3713 - val_loss: 0.4242\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3728 - val_loss: 0.4245\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3748 - val_loss: 0.4292\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3680 - val_loss: 0.4233\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3653 - val_loss: 0.4236\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3677 - val_loss: 0.4237\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3571 - val_loss: 0.4243\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3564 - val_loss: 0.4224\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3613 - val_loss: 0.4244\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.3667 - val_loss: 0.4230\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.3629 - val_loss: 0.4232\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3613 - val_loss: 0.4228\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3593 - val_loss: 0.4223\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3620 - val_loss: 0.4221\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3584 - val_loss: 0.4221\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3742 - val_loss: 0.4222\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3694 - val_loss: 0.4219\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3620 - val_loss: 0.4219\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3609 - val_loss: 0.4218\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3561 - val_loss: 0.4218\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3638 - val_loss: 0.4218\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3654 - val_loss: 0.4217\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3548 - val_loss: 0.4217\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3613 - val_loss: 0.4218\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3586 - val_loss: 0.4220\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3736 - val_loss: 0.4221\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3569 - val_loss: 0.4222\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3706 - val_loss: 0.4224\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3548 - val_loss: 0.4222\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "[2022_04_09-20:10:04] Training the entire fine-tuned model...\n",
      "[2022_04_09-20:10:37] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3624WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1021s vs `on_train_batch_end` time: 0.1258s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1021s vs `on_train_batch_end` time: 0.1258s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 679ms/step - loss: 0.3624 - val_loss: 0.4221\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3584 - val_loss: 0.4214\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3653 - val_loss: 0.4215\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3656 - val_loss: 0.4215\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3625 - val_loss: 0.4218\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3592 - val_loss: 0.4213\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3543 - val_loss: 0.4210\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3503 - val_loss: 0.4208\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3598 - val_loss: 0.4208\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3556 - val_loss: 0.4209\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3506 - val_loss: 0.4210\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3590 - val_loss: 0.4210\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3588 - val_loss: 0.4209\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3503 - val_loss: 0.4209\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3532 - val_loss: 0.4208\n",
      "[2022_04_09-20:11:14] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-20:11:14] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:11:14] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3506WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0989s vs `on_train_batch_end` time: 0.1258s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0989s vs `on_train_batch_end` time: 0.1258s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 694ms/step - loss: 0.3506 - val_loss: 0.4207\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>492</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  492   9\n",
       "1  107  13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18309859154929578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:19jvvrdf) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6867... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▅▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42075</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35057</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42075</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">scarlet-morning-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/19jvvrdf\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/19jvvrdf</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_200842-19jvvrdf/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:19jvvrdf). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/1nfa5238\" target=\"_blank\">absurd-plasma-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-20:12:18] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:12:18] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:12:18] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 742ms/step - loss: 0.7930 - val_loss: 0.6911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5837 - val_loss: 0.7196\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5687 - val_loss: 0.5227\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4773 - val_loss: 0.5296\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4684 - val_loss: 0.4609\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4388 - val_loss: 0.4539\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4136 - val_loss: 0.4554\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4067 - val_loss: 0.4453\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4029 - val_loss: 0.4587\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4043 - val_loss: 0.4419\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4004 - val_loss: 0.4384\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3990 - val_loss: 0.4604\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3699 - val_loss: 0.4421\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3899 - val_loss: 0.4336\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3705 - val_loss: 0.4438\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3665 - val_loss: 0.4340\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3621 - val_loss: 0.4309\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3830 - val_loss: 0.4527\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3677 - val_loss: 0.4289\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3481 - val_loss: 0.4327\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3563 - val_loss: 0.4453\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3649 - val_loss: 0.4364\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3523 - val_loss: 0.4283\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3390 - val_loss: 0.4271\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3514 - val_loss: 0.4300\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3482 - val_loss: 0.4300\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3369 - val_loss: 0.4272\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3413 - val_loss: 0.4271\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3360 - val_loss: 0.4284\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3469 - val_loss: 0.4288\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-20:13:02] Training the entire fine-tuned model...\n",
      "[2022_04_09-20:13:10] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 852ms/step - loss: 0.3528 - val_loss: 0.4307\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3417 - val_loss: 0.4275\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3379 - val_loss: 0.4312\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3411 - val_loss: 0.4290\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3256 - val_loss: 0.4279\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3373 - val_loss: 0.4280\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3262 - val_loss: 0.4292\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3346 - val_loss: 0.4296\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_09-20:13:33] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-20:13:33] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:13:33] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 845ms/step - loss: 0.3410 - val_loss: 0.4277\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>518</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  518  10\n",
       "1  121  21"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24277456647398843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1nfa5238) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 7299... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▅▅▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇█▃▃▂▂▂▁▂▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>23</td></tr><tr><td>best_val_loss</td><td>0.42708</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34097</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42766</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">absurd-plasma-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/1nfa5238\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/1nfa5238</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_201203-1nfa5238/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1nfa5238). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/3b2obyue\" target=\"_blank\">stellar-sponge-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-20:14:27] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:14:27] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:14:27] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 624ms/step - loss: 0.7961 - val_loss: 0.7846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.6359 - val_loss: 0.4824\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5381 - val_loss: 0.5722\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5507 - val_loss: 0.5653\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4651 - val_loss: 0.4801\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4521 - val_loss: 0.4394\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4422 - val_loss: 0.5042\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4179 - val_loss: 0.4762\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4465 - val_loss: 0.4373\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4132 - val_loss: 0.4645\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4057 - val_loss: 0.4259\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3944 - val_loss: 0.4382\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3830 - val_loss: 0.4245\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3829 - val_loss: 0.4289\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3911 - val_loss: 0.4231\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3912 - val_loss: 0.4456\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3816 - val_loss: 0.4672\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3916 - val_loss: 0.4762\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3660 - val_loss: 0.4248\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3693 - val_loss: 0.4152\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3563 - val_loss: 0.4177\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3492 - val_loss: 0.4195\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3473 - val_loss: 0.4188\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3415 - val_loss: 0.4157\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3382 - val_loss: 0.4144\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3500 - val_loss: 0.4149\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3508 - val_loss: 0.4150\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3444 - val_loss: 0.4146\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3534 - val_loss: 0.4145\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3536 - val_loss: 0.4144\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3483 - val_loss: 0.4143\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3484 - val_loss: 0.4143\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3436 - val_loss: 0.4143\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3452 - val_loss: 0.4143\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3419 - val_loss: 0.4143\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3502 - val_loss: 0.4143\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3456 - val_loss: 0.4143\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3509 - val_loss: 0.4143\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3387 - val_loss: 0.4143\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3424 - val_loss: 0.4143\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "[2022_04_09-20:15:22] Training the entire fine-tuned model...\n",
      "[2022_04_09-20:16:06] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3564WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0974s vs `on_train_batch_end` time: 0.1264s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0974s vs `on_train_batch_end` time: 0.1264s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 686ms/step - loss: 0.3564 - val_loss: 0.4209\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3560 - val_loss: 0.4164\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3441 - val_loss: 0.4130\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3462 - val_loss: 0.4124\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3254 - val_loss: 0.4128\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3269 - val_loss: 0.4124\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3310 - val_loss: 0.4127\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3327 - val_loss: 0.4127\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3262 - val_loss: 0.4132\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3253 - val_loss: 0.4135\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_09-20:16:33] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-20:16:33] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:16:33] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3506WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1017s vs `on_train_batch_end` time: 0.1272s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1017s vs `on_train_batch_end` time: 0.1272s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 687ms/step - loss: 0.3506 - val_loss: 0.4127\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>482</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  482  19\n",
       "1   97  23"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2839506172839506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3b2obyue) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 7590... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▆▄▄▃▃▂▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▄▄▂▃▂▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>0.41235</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35061</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41271</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">stellar-sponge-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/3b2obyue\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/3b2obyue</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_201411-3b2obyue/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3b2obyue). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/3nmfh60w\" target=\"_blank\">sparkling-dawn-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-20:17:46] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:17:47] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:17:47] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 724ms/step - loss: 0.7808 - val_loss: 0.8867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.6417 - val_loss: 0.8105\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.5460 - val_loss: 0.6196\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5169 - val_loss: 0.6096\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.5259 - val_loss: 0.4583\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4393 - val_loss: 0.4624\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4222 - val_loss: 0.4474\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4114 - val_loss: 0.4472\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3991 - val_loss: 0.4415\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3986 - val_loss: 0.4429\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4017 - val_loss: 0.4380\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4035 - val_loss: 0.4383\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3906 - val_loss: 0.4775\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4036 - val_loss: 0.4379\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3854 - val_loss: 0.4408\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3764 - val_loss: 0.4369\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3754 - val_loss: 0.4324\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3743 - val_loss: 0.4367\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3660 - val_loss: 0.4362\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3706 - val_loss: 0.4323\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3650 - val_loss: 0.4311\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3698 - val_loss: 0.4349\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3574 - val_loss: 0.4373\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3715 - val_loss: 0.4335\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3684 - val_loss: 0.4327\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3562 - val_loss: 0.4321\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3587 - val_loss: 0.4318\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_09-20:18:28] Training the entire fine-tuned model...\n",
      "[2022_04_09-20:18:45] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 841ms/step - loss: 0.3874 - val_loss: 0.4524\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3736 - val_loss: 0.4332\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3492 - val_loss: 0.4371\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3561 - val_loss: 0.4336\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3485 - val_loss: 0.4363\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3424 - val_loss: 0.4354\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3490 - val_loss: 0.4359\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3474 - val_loss: 0.4369\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_09-20:19:07] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-20:19:07] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:19:08] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 838ms/step - loss: 0.3597 - val_loss: 0.4324\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>518</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>122</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  518  10\n",
       "1  122  20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23255813953488372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3nmfh60w) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 7952... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▆▄▄▄▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▄▄▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>20</td></tr><tr><td>best_val_loss</td><td>0.43108</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35968</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.43237</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sparkling-dawn-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/3nmfh60w\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/3nmfh60w</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_201731-3nmfh60w/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3nmfh60w). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/1t818kcm\" target=\"_blank\">snowy-morning-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-20:20:03] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:20:03] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:20:03] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 835ms/step - loss: 0.8519 - val_loss: 0.7667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.6737 - val_loss: 0.4967\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5413 - val_loss: 0.5186\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5355 - val_loss: 0.5575\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4882 - val_loss: 0.4974\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4653 - val_loss: 0.4554\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4447 - val_loss: 0.4601\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4272 - val_loss: 0.4501\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4275 - val_loss: 0.4453\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4276 - val_loss: 0.4504\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4207 - val_loss: 0.4435\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4154 - val_loss: 0.4427\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4083 - val_loss: 0.4427\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4157 - val_loss: 0.4430\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4072 - val_loss: 0.4391\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4050 - val_loss: 0.4425\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4004 - val_loss: 0.4381\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3970 - val_loss: 0.4375\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4119 - val_loss: 0.4384\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4124 - val_loss: 0.4369\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4031 - val_loss: 0.4470\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3947 - val_loss: 0.4333\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3892 - val_loss: 0.4323\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3892 - val_loss: 0.4316\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3930 - val_loss: 0.4312\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3893 - val_loss: 0.4296\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3874 - val_loss: 0.4310\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3857 - val_loss: 0.4296\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3713 - val_loss: 0.4281\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3746 - val_loss: 0.4278\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3768 - val_loss: 0.4284\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3809 - val_loss: 0.4262\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3634 - val_loss: 0.4257\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3708 - val_loss: 0.4254\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3696 - val_loss: 0.4248\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3731 - val_loss: 0.4243\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3795 - val_loss: 0.4237\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3700 - val_loss: 0.4269\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3677 - val_loss: 0.4305\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3663 - val_loss: 0.4306\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3677 - val_loss: 0.4248\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3528 - val_loss: 0.4220\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3638 - val_loss: 0.4218\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3531 - val_loss: 0.4221\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3590 - val_loss: 0.4231\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3600 - val_loss: 0.4212\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3523 - val_loss: 0.4211\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3561 - val_loss: 0.4212\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3598 - val_loss: 0.4219\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3502 - val_loss: 0.4211\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3600 - val_loss: 0.4209\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3673 - val_loss: 0.4208\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3587 - val_loss: 0.4208\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3613 - val_loss: 0.4208\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3566 - val_loss: 0.4207\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3584 - val_loss: 0.4208\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3582 - val_loss: 0.4206\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3616 - val_loss: 0.4208\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3526 - val_loss: 0.4208\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3544 - val_loss: 0.4206\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3505 - val_loss: 0.4205\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3544 - val_loss: 0.4205\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3563 - val_loss: 0.4205\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3636 - val_loss: 0.4205\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3491 - val_loss: 0.4205\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3491 - val_loss: 0.4205\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3536 - val_loss: 0.4205\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3646 - val_loss: 0.4205\n",
      "[2022_04_09-20:21:29] Training the entire fine-tuned model...\n",
      "[2022_04_09-20:21:37] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3784WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0997s vs `on_train_batch_end` time: 0.1271s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0997s vs `on_train_batch_end` time: 0.1271s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 697ms/step - loss: 0.3784 - val_loss: 0.4219\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3644 - val_loss: 0.4251\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3489 - val_loss: 0.4233\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3519 - val_loss: 0.4188\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3547 - val_loss: 0.4186\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3427 - val_loss: 0.4183\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3218 - val_loss: 0.4187\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3259 - val_loss: 0.4188\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.2907 - val_loss: 0.4197\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3023 - val_loss: 0.4185\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.2900 - val_loss: 0.4183\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.2926 - val_loss: 0.4194\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.2924 - val_loss: 0.4191\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.2926 - val_loss: 0.4199\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.2976 - val_loss: 0.4196\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.2870 - val_loss: 0.4195\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.2837 - val_loss: 0.4195\n",
      "[2022_04_09-20:22:17] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-20:22:17] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:22:17] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.2975WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0999s vs `on_train_batch_end` time: 0.1265s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0999s vs `on_train_batch_end` time: 0.1265s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 681ms/step - loss: 0.2975 - val_loss: 0.4183\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>476</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  476  25\n",
       "1   89  31"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3522727272727273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1t818kcm) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8227... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▂▃</td></tr><tr><td>loss</td><td>█▄▄▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.41827</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.29749</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.41827</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">snowy-morning-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/1t818kcm\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/1t818kcm</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_201947-1t818kcm/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1t818kcm). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/2u6zmfby\" target=\"_blank\">faithful-blaze-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-20:23:18] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:23:19] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:23:19] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 727ms/step - loss: 0.8528 - val_loss: 0.5165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5778 - val_loss: 0.6595\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5372 - val_loss: 0.5503\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4643 - val_loss: 0.5439\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4677 - val_loss: 0.4687\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4467 - val_loss: 0.4729\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4334 - val_loss: 0.4777\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4371 - val_loss: 0.4702\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4283 - val_loss: 0.4627\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4300 - val_loss: 0.4603\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4255 - val_loss: 0.4599\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4262 - val_loss: 0.4605\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4209 - val_loss: 0.4612\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4216 - val_loss: 0.4611\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4241 - val_loss: 0.4605\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4238 - val_loss: 0.4600\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4203 - val_loss: 0.4596\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4197 - val_loss: 0.4592\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4221 - val_loss: 0.4590\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4194 - val_loss: 0.4593\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4164 - val_loss: 0.4591\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4257 - val_loss: 0.4587\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4200 - val_loss: 0.4588\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4183 - val_loss: 0.4588\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4208 - val_loss: 0.4587\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4209 - val_loss: 0.4586\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4201 - val_loss: 0.4585\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4219 - val_loss: 0.4585\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4246 - val_loss: 0.4585\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4133 - val_loss: 0.4584\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4196 - val_loss: 0.4584\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 230ms/step - loss: 0.4171 - val_loss: 0.4583\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 230ms/step - loss: 0.4163 - val_loss: 0.4583\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4187 - val_loss: 0.4583\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4197 - val_loss: 0.4583\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4213 - val_loss: 0.4582\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4220 - val_loss: 0.4582\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4232 - val_loss: 0.4582\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4211 - val_loss: 0.4582\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4207 - val_loss: 0.4582\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4192 - val_loss: 0.4582\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4257 - val_loss: 0.4582\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4221 - val_loss: 0.4582\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4166 - val_loss: 0.4582\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4145 - val_loss: 0.4582\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4209 - val_loss: 0.4582\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4194 - val_loss: 0.4582\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4188 - val_loss: 0.4582\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4166 - val_loss: 0.4582\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4199 - val_loss: 0.4582\n",
      "[2022_04_09-20:24:23] Training the entire fine-tuned model...\n",
      "[2022_04_09-20:24:33] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 842ms/step - loss: 0.4579 - val_loss: 0.4634\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.4326 - val_loss: 0.4551\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.4186 - val_loss: 0.4560\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.4045 - val_loss: 0.4502\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3862 - val_loss: 0.4437\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3688 - val_loss: 0.4477\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3447 - val_loss: 0.5095\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3628 - val_loss: 0.5087\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3176 - val_loss: 0.4412\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.2974 - val_loss: 0.4451\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.2813 - val_loss: 0.4555\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.2539 - val_loss: 0.4688\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.2433 - val_loss: 0.4775\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.2348 - val_loss: 0.4787\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.2301 - val_loss: 0.4872\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "[2022_04_09-20:25:08] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-20:25:08] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:25:08] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 831ms/step - loss: 0.2943 - val_loss: 0.4426\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>503</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  503  25\n",
       "1  110  32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32160804020100503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2u6zmfby) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8773... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▄▃▃▃▂▂▂▁▁▂</td></tr><tr><td>lr</td><td>███▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▃▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.44115</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.29431</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.44263</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">faithful-blaze-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/2u6zmfby\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/2u6zmfby</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_202303-2u6zmfby/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2u6zmfby). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/2435tvhg\" target=\"_blank\">morning-shadow-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-20:26:03] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:26:03] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:26:03] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 588ms/step - loss: 0.8780 - val_loss: 0.7926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.6509 - val_loss: 0.4762\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5730 - val_loss: 0.5352\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.5225 - val_loss: 0.5475\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4818 - val_loss: 0.4544\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4561 - val_loss: 0.4530\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4571 - val_loss: 0.4802\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4387 - val_loss: 0.4569\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4254 - val_loss: 0.4321\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3951 - val_loss: 0.4304\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3890 - val_loss: 0.4380\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3834 - val_loss: 0.4253\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3909 - val_loss: 0.4267\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3839 - val_loss: 0.4209\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3716 - val_loss: 0.4197\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3706 - val_loss: 0.4247\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3586 - val_loss: 0.4545\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3768 - val_loss: 0.4164\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3675 - val_loss: 0.4161\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3627 - val_loss: 0.4189\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3517 - val_loss: 0.4171\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3388 - val_loss: 0.4143\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3403 - val_loss: 0.4282\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3450 - val_loss: 0.4347\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3545 - val_loss: 0.4688\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3744 - val_loss: 0.4210\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3573 - val_loss: 0.4186\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3576 - val_loss: 0.4291\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_09-20:26:43] Training the entire fine-tuned model...\n",
      "[2022_04_09-20:26:53] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5217WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0990s vs `on_train_batch_end` time: 0.1274s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0990s vs `on_train_batch_end` time: 0.1274s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 683ms/step - loss: 0.5217 - val_loss: 0.4725\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3955 - val_loss: 0.4346\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3617 - val_loss: 0.4152\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3428 - val_loss: 0.4143\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3087 - val_loss: 0.4140\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3046 - val_loss: 0.4256\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.2658 - val_loss: 0.4443\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.2916 - val_loss: 0.4175\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.2291 - val_loss: 0.4304\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.2266 - val_loss: 0.4203\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.2235 - val_loss: 0.4234\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_09-20:27:22] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-20:27:22] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:27:22] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3015WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0999s vs `on_train_batch_end` time: 0.1265s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0999s vs `on_train_batch_end` time: 0.1265s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 889ms/step - loss: 0.3015 - val_loss: 0.4123\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>468</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  468  33\n",
       "1   88  32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34594594594594597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2435tvhg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9230... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▆▅▄▄▃▃▃▃▃▃▃▃▃▃▃▂▃▃▂▂▂▂▂▂▃▂▂▄▃▂▂▂▂▁▂▁▁▁▂</td></tr><tr><td>lr</td><td>█████████████████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▃▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.41233</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30148</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.41233</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">morning-shadow-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/2435tvhg\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/2435tvhg</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_202547-2435tvhg/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2435tvhg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/is1ozecw\" target=\"_blank\">ethereal-dream-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-20:28:16] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:28:16] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:28:16] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 758ms/step - loss: 1.0975 - val_loss: 0.5314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.6248 - val_loss: 0.6720\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.6306 - val_loss: 0.4822\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4930 - val_loss: 0.4987\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4578 - val_loss: 0.4641\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4336 - val_loss: 0.4695\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4240 - val_loss: 0.4531\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4204 - val_loss: 0.4639\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4146 - val_loss: 0.4443\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4151 - val_loss: 0.4422\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3981 - val_loss: 0.4430\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3852 - val_loss: 0.4360\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3861 - val_loss: 0.4394\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3820 - val_loss: 0.4479\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4058 - val_loss: 0.4564\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3943 - val_loss: 0.4881\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3972 - val_loss: 0.4309\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3802 - val_loss: 0.4316\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3662 - val_loss: 0.4457\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3764 - val_loss: 0.4316\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3688 - val_loss: 0.4295\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3671 - val_loss: 0.4382\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3789 - val_loss: 0.4293\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3659 - val_loss: 0.4320\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3638 - val_loss: 0.4288\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3643 - val_loss: 0.4308\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3613 - val_loss: 0.4340\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3659 - val_loss: 0.4284\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3635 - val_loss: 0.4322\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3595 - val_loss: 0.4317\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3529 - val_loss: 0.4276\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3585 - val_loss: 0.4296\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3550 - val_loss: 0.4287\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3544 - val_loss: 0.4268\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3490 - val_loss: 0.4278\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3517 - val_loss: 0.4295\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3548 - val_loss: 0.4266\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3525 - val_loss: 0.4279\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3507 - val_loss: 0.4299\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3519 - val_loss: 0.4271\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3550 - val_loss: 0.4301\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3444 - val_loss: 0.4277\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3487 - val_loss: 0.4257\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3546 - val_loss: 0.4256\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3492 - val_loss: 0.4268\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3582 - val_loss: 0.4302\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3436 - val_loss: 0.4284\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3522 - val_loss: 0.4260\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3393 - val_loss: 0.4260\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3595 - val_loss: 0.4263\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3600 - val_loss: 0.4262\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3449 - val_loss: 0.4266\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_09-20:29:22] Training the entire fine-tuned model...\n",
      "[2022_04_09-20:29:30] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 833ms/step - loss: 0.3455 - val_loss: 0.4261\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3451 - val_loss: 0.4281\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3586 - val_loss: 0.4275\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.3501 - val_loss: 0.4276\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.3407 - val_loss: 0.4267\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3362 - val_loss: 0.4266\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3499 - val_loss: 0.4267\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3376 - val_loss: 0.4269\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3538 - val_loss: 0.4269\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_09-20:29:56] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-20:29:56] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:29:56] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 854ms/step - loss: 0.3519 - val_loss: 0.4262\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>519</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  519   9\n",
       "1  123  19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2235294117647059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:is1ozecw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9553... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▄▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▃▂▂▂▁▁▁▂▃▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>43</td></tr><tr><td>best_val_loss</td><td>0.42559</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35188</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42615</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">ethereal-dream-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/is1ozecw\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/is1ozecw</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_202800-is1ozecw/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:is1ozecw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/1i4mf5pm\" target=\"_blank\">feasible-butterfly-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-20:30:50] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:30:50] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:30:51] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 576ms/step - loss: 0.8863 - val_loss: 1.0380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.7465 - val_loss: 0.6157\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.6045 - val_loss: 0.4699\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5190 - val_loss: 0.4884\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4821 - val_loss: 0.4967\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4548 - val_loss: 0.5377\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4584 - val_loss: 0.4387\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4449 - val_loss: 0.5229\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5089 - val_loss: 0.5482\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4596 - val_loss: 0.4449\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4005 - val_loss: 0.4416\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4199 - val_loss: 0.4317\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3888 - val_loss: 0.4401\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3894 - val_loss: 0.4258\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3873 - val_loss: 0.4243\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3791 - val_loss: 0.4264\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3722 - val_loss: 0.4234\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3781 - val_loss: 0.4230\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3758 - val_loss: 0.4267\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3675 - val_loss: 0.4219\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3797 - val_loss: 0.4219\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3782 - val_loss: 0.4255\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3760 - val_loss: 0.4213\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3728 - val_loss: 0.4210\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3706 - val_loss: 0.4246\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3669 - val_loss: 0.4196\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3703 - val_loss: 0.4188\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3675 - val_loss: 0.4185\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3668 - val_loss: 0.4195\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3697 - val_loss: 0.4178\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3615 - val_loss: 0.4196\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3602 - val_loss: 0.4188\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3629 - val_loss: 0.4205\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3659 - val_loss: 0.4181\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3562 - val_loss: 0.4173\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3554 - val_loss: 0.4169\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3620 - val_loss: 0.4176\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3595 - val_loss: 0.4167\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3500 - val_loss: 0.4169\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3460 - val_loss: 0.4170\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3451 - val_loss: 0.4175\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3553 - val_loss: 0.4165\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3575 - val_loss: 0.4163\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3535 - val_loss: 0.4166\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3504 - val_loss: 0.4167\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3539 - val_loss: 0.4161\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3596 - val_loss: 0.4160\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3483 - val_loss: 0.4159\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3467 - val_loss: 0.4165\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3511 - val_loss: 0.4176\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3558 - val_loss: 0.4157\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3579 - val_loss: 0.4180\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3566 - val_loss: 0.4156\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3542 - val_loss: 0.4158\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3600 - val_loss: 0.4163\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3577 - val_loss: 0.4158\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3501 - val_loss: 0.4155\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3464 - val_loss: 0.4154\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3473 - val_loss: 0.4154\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3533 - val_loss: 0.4154\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3554 - val_loss: 0.4154\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3474 - val_loss: 0.4157\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3485 - val_loss: 0.4157\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3506 - val_loss: 0.4158\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3469 - val_loss: 0.4158\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3488 - val_loss: 0.4158\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3548 - val_loss: 0.4158\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3576 - val_loss: 0.4157\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3542 - val_loss: 0.4157\n",
      "[2022_04_09-20:32:17] Training the entire fine-tuned model...\n",
      "[2022_04_09-20:32:25] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3439WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0975s vs `on_train_batch_end` time: 0.1273s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0975s vs `on_train_batch_end` time: 0.1273s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 684ms/step - loss: 0.3439 - val_loss: 0.4153\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3411 - val_loss: 0.4155\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3411 - val_loss: 0.4155\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3449 - val_loss: 0.4169\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3400 - val_loss: 0.4150\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3431 - val_loss: 0.4153\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3418 - val_loss: 0.4148\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3410 - val_loss: 0.4149\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3383 - val_loss: 0.4147\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3400 - val_loss: 0.4145\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3300 - val_loss: 0.4143\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3383 - val_loss: 0.4143\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3387 - val_loss: 0.4143\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3368 - val_loss: 0.4142\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3350 - val_loss: 0.4141\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3272 - val_loss: 0.4141\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3248 - val_loss: 0.4140\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3316 - val_loss: 0.4140\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3341 - val_loss: 0.4139\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3251 - val_loss: 0.4140\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3244 - val_loss: 0.4141\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3310 - val_loss: 0.4140\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3248 - val_loss: 0.4141\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3285 - val_loss: 0.4141\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3216 - val_loss: 0.4141\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3338 - val_loss: 0.4143\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3300 - val_loss: 0.4144\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_09-20:33:25] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-20:33:25] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:33:25] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3334WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1274s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1274s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 683ms/step - loss: 0.3334 - val_loss: 0.4139\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>487</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  487  14\n",
       "1   98  22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28205128205128205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1i4mf5pm) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9967... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>18</td></tr><tr><td>best_val_loss</td><td>0.41391</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33339</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41393</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">feasible-butterfly-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/1i4mf5pm\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/1i4mf5pm</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_203034-1i4mf5pm/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1i4mf5pm). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/dqv59hqe\" target=\"_blank\">laced-puddle-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-20:34:19] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:34:19] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:34:19] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 752ms/step - loss: 0.9000 - val_loss: 0.6922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.6109 - val_loss: 0.7190\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5492 - val_loss: 0.5219\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4997 - val_loss: 0.5216\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4906 - val_loss: 0.4777\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4432 - val_loss: 0.4548\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4332 - val_loss: 0.4897\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4200 - val_loss: 0.4602\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4245 - val_loss: 0.4476\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4194 - val_loss: 0.4804\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4182 - val_loss: 0.4395\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3836 - val_loss: 0.4527\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3975 - val_loss: 0.4447\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3941 - val_loss: 0.4339\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3800 - val_loss: 0.4335\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3684 - val_loss: 0.4326\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3748 - val_loss: 0.4319\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3758 - val_loss: 0.4813\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3839 - val_loss: 0.4442\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3616 - val_loss: 0.4370\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3628 - val_loss: 0.4296\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3560 - val_loss: 0.4265\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3586 - val_loss: 0.4340\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3543 - val_loss: 0.4309\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3544 - val_loss: 0.4261\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3513 - val_loss: 0.4503\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3484 - val_loss: 0.4226\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3468 - val_loss: 0.4252\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3435 - val_loss: 0.5071\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3528 - val_loss: 0.4268\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3447 - val_loss: 0.4204\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3347 - val_loss: 0.4710\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3419 - val_loss: 0.4187\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3389 - val_loss: 0.4271\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3441 - val_loss: 0.4393\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3309 - val_loss: 0.4403\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3391 - val_loss: 0.4192\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3152 - val_loss: 0.4265\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3252 - val_loss: 0.4213\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3287 - val_loss: 0.4187\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3253 - val_loss: 0.4231\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3260 - val_loss: 0.4232\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3308 - val_loss: 0.4210\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3190 - val_loss: 0.4187\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3179 - val_loss: 0.4186\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3240 - val_loss: 0.4190\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3088 - val_loss: 0.4202\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3329 - val_loss: 0.4204\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3188 - val_loss: 0.4186\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3230 - val_loss: 0.4185\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3175 - val_loss: 0.4188\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3227 - val_loss: 0.4194\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3212 - val_loss: 0.4197\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3346 - val_loss: 0.4197\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3291 - val_loss: 0.4198\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3212 - val_loss: 0.4198\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3235 - val_loss: 0.4198\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3025 - val_loss: 0.4198\n",
      "[2022_04_09-20:35:33] Training the entire fine-tuned model...\n",
      "[2022_04_09-20:35:59] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 865ms/step - loss: 0.3156 - val_loss: 0.4229\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3202 - val_loss: 0.4287\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3267 - val_loss: 0.4205\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3138 - val_loss: 0.4197\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.2964 - val_loss: 0.4278\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.2954 - val_loss: 0.4236\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2951 - val_loss: 0.4274\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.2980 - val_loss: 0.4260\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3008 - val_loss: 0.4263\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.2891 - val_loss: 0.4296\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.2978 - val_loss: 0.4302\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.2841 - val_loss: 0.4284\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_09-20:36:29] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-20:36:29] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:36:30] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 819ms/step - loss: 0.3152 - val_loss: 0.4198\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>499</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  499  29\n",
       "1  105  37"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3557692307692308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:dqv59hqe) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10564... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇█▃▂▂▂▁▂▁▁▂▁▁▁▂▁▃▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>49</td></tr><tr><td>best_val_loss</td><td>0.41854</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31516</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41976</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">laced-puddle-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/dqv59hqe\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/dqv59hqe</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_203403-dqv59hqe/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:dqv59hqe). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/42wnwlts\" target=\"_blank\">rose-capybara-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-20:37:38] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:37:38] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:37:39] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 579ms/step - loss: 0.8560 - val_loss: 0.8710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.6550 - val_loss: 0.5555\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5580 - val_loss: 0.5140\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5647 - val_loss: 0.4976\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5090 - val_loss: 0.5699\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5107 - val_loss: 0.5273\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4544 - val_loss: 0.4483\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4174 - val_loss: 0.4379\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4102 - val_loss: 0.4496\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3998 - val_loss: 0.4348\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3919 - val_loss: 0.4396\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3933 - val_loss: 0.4277\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3940 - val_loss: 0.5005\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4305 - val_loss: 0.4627\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4016 - val_loss: 0.4550\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4137 - val_loss: 0.4815\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3919 - val_loss: 0.4294\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3875 - val_loss: 0.4264\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3545 - val_loss: 0.4364\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3681 - val_loss: 0.4309\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3735 - val_loss: 0.4203\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3608 - val_loss: 0.4218\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3518 - val_loss: 0.4255\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3560 - val_loss: 0.4210\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3573 - val_loss: 0.4188\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3497 - val_loss: 0.4190\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3503 - val_loss: 0.4193\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3603 - val_loss: 0.4201\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3605 - val_loss: 0.4179\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3494 - val_loss: 0.4178\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3594 - val_loss: 0.4191\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3586 - val_loss: 0.4247\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3376 - val_loss: 0.4210\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3534 - val_loss: 0.4167\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3468 - val_loss: 0.4166\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3442 - val_loss: 0.4162\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3467 - val_loss: 0.4170\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3440 - val_loss: 0.4173\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3441 - val_loss: 0.4161\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3303 - val_loss: 0.4161\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3336 - val_loss: 0.4159\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3491 - val_loss: 0.4160\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3507 - val_loss: 0.4159\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3301 - val_loss: 0.4158\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3427 - val_loss: 0.4156\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3407 - val_loss: 0.4156\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3349 - val_loss: 0.4155\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3365 - val_loss: 0.4154\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3492 - val_loss: 0.4157\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3349 - val_loss: 0.4159\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3360 - val_loss: 0.4171\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3389 - val_loss: 0.4161\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3322 - val_loss: 0.4152\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3554 - val_loss: 0.4151\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3370 - val_loss: 0.4151\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3401 - val_loss: 0.4151\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3424 - val_loss: 0.4150\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3364 - val_loss: 0.4151\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3410 - val_loss: 0.4150\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3583 - val_loss: 0.4150\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3334 - val_loss: 0.4149\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3416 - val_loss: 0.4149\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3326 - val_loss: 0.4149\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3332 - val_loss: 0.4149\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3441 - val_loss: 0.4149\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3434 - val_loss: 0.4149\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3372 - val_loss: 0.4149\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3514 - val_loss: 0.4149\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3390 - val_loss: 0.4149\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3275 - val_loss: 0.4149\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3328 - val_loss: 0.4149\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3420 - val_loss: 0.4149\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3467 - val_loss: 0.4149\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3435 - val_loss: 0.4149\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3400 - val_loss: 0.4149\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3460 - val_loss: 0.4149\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3387 - val_loss: 0.4149\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3361 - val_loss: 0.4149\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3323 - val_loss: 0.4149\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3370 - val_loss: 0.4149\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3459 - val_loss: 0.4149\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3331 - val_loss: 0.4149\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3408 - val_loss: 0.4149\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3355 - val_loss: 0.4149\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3486 - val_loss: 0.4149\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3397 - val_loss: 0.4149\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3379 - val_loss: 0.4149\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3528 - val_loss: 0.4149\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3374 - val_loss: 0.4149\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3428 - val_loss: 0.4149\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3366 - val_loss: 0.4149\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3430 - val_loss: 0.4149\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3473 - val_loss: 0.4149\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3379 - val_loss: 0.4149\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3453 - val_loss: 0.4149\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3322 - val_loss: 0.4149\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3457 - val_loss: 0.4149\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3401 - val_loss: 0.4149\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3404 - val_loss: 0.4149\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3356 - val_loss: 0.4149\n",
      "[2022_04_09-20:39:39] Training the entire fine-tuned model...\n",
      "[2022_04_09-20:40:18] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3384WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0993s vs `on_train_batch_end` time: 0.1245s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0993s vs `on_train_batch_end` time: 0.1245s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 687ms/step - loss: 0.3384 - val_loss: 0.4235\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3632 - val_loss: 0.4146\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3350 - val_loss: 0.4125\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3281 - val_loss: 0.4120\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 350ms/step - loss: 0.3214 - val_loss: 0.4123\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3195 - val_loss: 0.4140\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3158 - val_loss: 0.4146\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3179 - val_loss: 0.4136\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3173 - val_loss: 0.4143\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3138 - val_loss: 0.4142\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3141 - val_loss: 0.4157\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3162 - val_loss: 0.4133\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_09-20:40:50] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-20:40:50] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:40:50] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3238WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1267s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1267s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 673ms/step - loss: 0.3238 - val_loss: 0.4124\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>489</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  489  12\n",
       "1  103  17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22818791946308725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:42wnwlts) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 11025... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇███▁▁▁▂▁</td></tr><tr><td>loss</td><td>█▄▄▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>0.41202</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32381</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41239</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">rose-capybara-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/42wnwlts\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/42wnwlts</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_203723-42wnwlts/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:42wnwlts). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/14imk5vz\" target=\"_blank\">driven-bush-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-20:41:44] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:41:44] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:41:45] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 749ms/step - loss: 0.9559 - val_loss: 0.5496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.5853 - val_loss: 0.6920\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.5251 - val_loss: 0.5498\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5275 - val_loss: 0.5177\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4453 - val_loss: 0.4619\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4480 - val_loss: 0.4555\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4174 - val_loss: 0.4603\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4145 - val_loss: 0.4458\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4064 - val_loss: 0.4638\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3969 - val_loss: 0.4407\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3928 - val_loss: 0.4563\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3900 - val_loss: 0.4394\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3829 - val_loss: 0.4518\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3823 - val_loss: 0.4334\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3755 - val_loss: 0.4326\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3761 - val_loss: 0.4617\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3869 - val_loss: 0.4424\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3739 - val_loss: 0.4318\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3761 - val_loss: 0.4327\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3639 - val_loss: 0.4631\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3693 - val_loss: 0.4278\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3773 - val_loss: 0.4355\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3612 - val_loss: 0.4326\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3472 - val_loss: 0.4232\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3552 - val_loss: 0.4241\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3606 - val_loss: 0.4591\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3608 - val_loss: 0.4225\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3659 - val_loss: 0.4297\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3635 - val_loss: 0.4223\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3457 - val_loss: 0.4554\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3328 - val_loss: 0.4365\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3180 - val_loss: 0.4701\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3411 - val_loss: 0.4218\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3258 - val_loss: 0.4208\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3315 - val_loss: 0.4425\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3300 - val_loss: 0.4277\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3403 - val_loss: 0.4333\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3466 - val_loss: 0.4354\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3226 - val_loss: 0.4225\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3253 - val_loss: 0.4229\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3124 - val_loss: 0.4259\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3084 - val_loss: 0.4210\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_09-20:42:41] Training the entire fine-tuned model...\n",
      "[2022_04_09-20:43:24] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 1s/step - loss: 0.3459 - val_loss: 0.4333\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3329 - val_loss: 0.4342\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3271 - val_loss: 0.4258\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3281 - val_loss: 0.4268\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3183 - val_loss: 0.4260\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3002 - val_loss: 0.4390\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.2960 - val_loss: 0.4301\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3152 - val_loss: 0.4314\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.2979 - val_loss: 0.4374\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3098 - val_loss: 0.4332\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2880 - val_loss: 0.4317\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_09-20:43:53] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-20:43:53] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:43:53] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 849ms/step - loss: 0.3202 - val_loss: 0.4252\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>509</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  509  19\n",
       "1  112  30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31413612565445026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:14imk5vz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 11706... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████████████▃▃▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▄▂▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>33</td></tr><tr><td>best_val_loss</td><td>0.42084</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32021</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.42517</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">driven-bush-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/14imk5vz\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/14imk5vz</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_204128-14imk5vz/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:14imk5vz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/3qnt4adh\" target=\"_blank\">distinctive-dew-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-20:45:05] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:45:05] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:45:05] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 564ms/step - loss: 0.9693 - val_loss: 0.7560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5994 - val_loss: 0.5172\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5275 - val_loss: 0.4625\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4635 - val_loss: 0.5469\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4742 - val_loss: 0.4769\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4454 - val_loss: 0.4660\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4197 - val_loss: 0.4427\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4099 - val_loss: 0.4342\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4088 - val_loss: 0.4548\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4162 - val_loss: 0.4385\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3900 - val_loss: 0.5324\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4531 - val_loss: 0.4938\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3996 - val_loss: 0.4397\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3974 - val_loss: 0.4243\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3833 - val_loss: 0.4296\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3674 - val_loss: 0.4307\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3824 - val_loss: 0.4236\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3730 - val_loss: 0.4249\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3694 - val_loss: 0.4232\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3739 - val_loss: 0.4221\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3674 - val_loss: 0.4234\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3654 - val_loss: 0.4215\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3678 - val_loss: 0.4210\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3721 - val_loss: 0.4211\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3621 - val_loss: 0.4210\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3541 - val_loss: 0.4200\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3731 - val_loss: 0.4196\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3670 - val_loss: 0.4192\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3789 - val_loss: 0.4255\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3514 - val_loss: 0.4359\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3555 - val_loss: 0.4194\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3563 - val_loss: 0.4196\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3473 - val_loss: 0.4176\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3517 - val_loss: 0.4175\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3535 - val_loss: 0.4175\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3487 - val_loss: 0.4177\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3419 - val_loss: 0.4174\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3505 - val_loss: 0.4175\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3601 - val_loss: 0.4173\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3554 - val_loss: 0.4172\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3591 - val_loss: 0.4176\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3582 - val_loss: 0.4175\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3505 - val_loss: 0.4175\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3401 - val_loss: 0.4173\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3489 - val_loss: 0.4172\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3487 - val_loss: 0.4171\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3501 - val_loss: 0.4169\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3498 - val_loss: 0.4169\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3521 - val_loss: 0.4170\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3541 - val_loss: 0.4172\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3455 - val_loss: 0.4172\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3459 - val_loss: 0.4172\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3560 - val_loss: 0.4171\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3419 - val_loss: 0.4170\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3481 - val_loss: 0.4169\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3494 - val_loss: 0.4169\n",
      "[2022_04_09-20:46:17] Training the entire fine-tuned model...\n",
      "[2022_04_09-20:46:26] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3486WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1049s vs `on_train_batch_end` time: 0.1263s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1049s vs `on_train_batch_end` time: 0.1263s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 677ms/step - loss: 0.3486 - val_loss: 0.4205\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3431 - val_loss: 0.4257\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3374 - val_loss: 0.4261\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3257 - val_loss: 0.4195\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3234 - val_loss: 0.4138\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3094 - val_loss: 0.4269\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3081 - val_loss: 0.4247\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.2915 - val_loss: 0.4198\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.2878 - val_loss: 0.4239\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.2713 - val_loss: 0.4259\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.2723 - val_loss: 0.4282\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.2655 - val_loss: 0.4273\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.2691 - val_loss: 0.4280\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_09-20:46:58] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-20:46:58] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:47:12] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3160WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0975s vs `on_train_batch_end` time: 0.1292s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0975s vs `on_train_batch_end` time: 0.1292s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 675ms/step - loss: 0.3160 - val_loss: 0.4140\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>475</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  475  26\n",
       "1   89  31"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3502824858757062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3qnt4adh) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 12113... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂</td></tr><tr><td>lr</td><td>███████▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▄▂▁▂▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>4</td></tr><tr><td>best_val_loss</td><td>0.41376</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31603</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.41401</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">distinctive-dew-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/3qnt4adh\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/3qnt4adh</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_204449-3qnt4adh/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3qnt4adh). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/74cepwu2\" target=\"_blank\">deep-music-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-20:48:07] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:48:07] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:48:07] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 722ms/step - loss: 0.7708 - val_loss: 0.8469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.6747 - val_loss: 0.5900\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5104 - val_loss: 0.4789\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4814 - val_loss: 0.4693\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4402 - val_loss: 0.4824\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4619 - val_loss: 0.4693\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4386 - val_loss: 0.4730\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4155 - val_loss: 0.4467\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4009 - val_loss: 0.4678\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3954 - val_loss: 0.4433\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3942 - val_loss: 0.4456\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3977 - val_loss: 0.4482\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3970 - val_loss: 0.4407\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3821 - val_loss: 0.4398\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3830 - val_loss: 0.4347\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3655 - val_loss: 0.4324\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3623 - val_loss: 0.4300\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3698 - val_loss: 0.4298\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3628 - val_loss: 0.4556\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3704 - val_loss: 0.4376\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3639 - val_loss: 0.4348\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3491 - val_loss: 0.4263\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3413 - val_loss: 0.4258\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 227ms/step - loss: 0.3382 - val_loss: 0.4360\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 225ms/step - loss: 0.3554 - val_loss: 0.4453\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3450 - val_loss: 0.4252\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3316 - val_loss: 0.4303\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3371 - val_loss: 0.4249\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3289 - val_loss: 0.4362\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3377 - val_loss: 0.4465\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3413 - val_loss: 0.4274\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3287 - val_loss: 0.4313\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3300 - val_loss: 0.4240\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3200 - val_loss: 0.4234\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3172 - val_loss: 0.4223\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3309 - val_loss: 0.4224\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3217 - val_loss: 0.4259\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3224 - val_loss: 0.4211\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3229 - val_loss: 0.4252\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3280 - val_loss: 0.4214\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3305 - val_loss: 0.4239\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3272 - val_loss: 0.4226\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3215 - val_loss: 0.4226\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3198 - val_loss: 0.4218\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3108 - val_loss: 0.4217\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3222 - val_loss: 0.4215\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-20:49:08] Training the entire fine-tuned model...\n",
      "[2022_04_09-20:49:46] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 828ms/step - loss: 0.4086 - val_loss: 0.4447\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3533 - val_loss: 0.4430\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3168 - val_loss: 0.4347\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3071 - val_loss: 0.4350\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.2954 - val_loss: 0.4357\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.2957 - val_loss: 0.4903\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.2692 - val_loss: 0.4508\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.2498 - val_loss: 0.4465\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.2551 - val_loss: 0.4520\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.2341 - val_loss: 0.4536\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.2273 - val_loss: 0.4561\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_09-20:50:14] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-20:50:14] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:50:14] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 836ms/step - loss: 0.3054 - val_loss: 0.4320\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>509</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  509  19\n",
       "1  116  26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27807486631016043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_09_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:74cepwu2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 12627... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▇▅▄▄▃▃▃▃▃▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▁▁▁▂</td></tr><tr><td>lr</td><td>██████████████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>37</td></tr><tr><td>best_val_loss</td><td>0.42114</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30538</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.43204</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">deep-music-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/74cepwu2\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/74cepwu2</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_204750-74cepwu2/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:74cepwu2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/6jiiwx51\" target=\"_blank\">autumn-durian-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-20:51:07] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:51:07] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:51:07] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 544ms/step - loss: 0.9831 - val_loss: 1.4255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.8638 - val_loss: 0.8197\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.7249 - val_loss: 0.6003\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5183 - val_loss: 0.4873\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4852 - val_loss: 0.4507\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4560 - val_loss: 0.4472\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4213 - val_loss: 0.4445\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4175 - val_loss: 0.4527\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4257 - val_loss: 0.4411\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4143 - val_loss: 0.4380\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3965 - val_loss: 0.4334\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3853 - val_loss: 0.4377\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3897 - val_loss: 0.4288\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3764 - val_loss: 0.4502\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3950 - val_loss: 0.4254\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3685 - val_loss: 0.4222\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3638 - val_loss: 0.4220\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3672 - val_loss: 0.4196\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3753 - val_loss: 0.4827\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3978 - val_loss: 0.4306\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3738 - val_loss: 0.4349\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3595 - val_loss: 0.4434\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3470 - val_loss: 0.4198\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3718 - val_loss: 0.4145\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3582 - val_loss: 0.4237\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3570 - val_loss: 0.4302\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3477 - val_loss: 0.4230\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3410 - val_loss: 0.4138\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3425 - val_loss: 0.4208\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3376 - val_loss: 0.4175\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3405 - val_loss: 0.4162\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3454 - val_loss: 0.4134\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3455 - val_loss: 0.4146\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3382 - val_loss: 0.4150\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3408 - val_loss: 0.4130\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3276 - val_loss: 0.4164\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3380 - val_loss: 0.4146\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3297 - val_loss: 0.4154\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3499 - val_loss: 0.4127\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3385 - val_loss: 0.4127\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3409 - val_loss: 0.4125\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3245 - val_loss: 0.4125\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3355 - val_loss: 0.4121\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3173 - val_loss: 0.4121\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3272 - val_loss: 0.4128\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3341 - val_loss: 0.4124\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3321 - val_loss: 0.4126\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3211 - val_loss: 0.4124\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3251 - val_loss: 0.4123\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3310 - val_loss: 0.4126\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3353 - val_loss: 0.4133\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3437 - val_loss: 0.4132\n",
      "[2022_04_09-20:52:14] Training the entire fine-tuned model...\n",
      "[2022_04_09-20:52:47] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5030WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1024s vs `on_train_batch_end` time: 0.1241s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1024s vs `on_train_batch_end` time: 0.1241s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 699ms/step - loss: 0.5030 - val_loss: 0.5475\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.4026 - val_loss: 0.4241\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3772 - val_loss: 0.4514\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3697 - val_loss: 0.4696\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3642 - val_loss: 0.4209\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3196 - val_loss: 0.4208\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3325 - val_loss: 0.4975\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3491 - val_loss: 0.4329\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.2881 - val_loss: 0.4167\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.2627 - val_loss: 0.4203\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.2560 - val_loss: 0.4205\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.2545 - val_loss: 0.4327\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.2227 - val_loss: 0.4272\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.2036 - val_loss: 0.4332\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.1931 - val_loss: 0.4412\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.1887 - val_loss: 0.4484\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.1749 - val_loss: 0.4520\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_09-20:53:27] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-20:53:27] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:53:27] Validation set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.2693WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0973s vs `on_train_batch_end` time: 0.1277s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0973s vs `on_train_batch_end` time: 0.1277s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 677ms/step - loss: 0.2693 - val_loss: 0.4183\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>475</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  475  26\n",
       "1   98  22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2619047619047619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_09_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:6jiiwx51) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 13024... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▇▄▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▂▂▂▂▁▁▂</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>43</td></tr><tr><td>best_val_loss</td><td>0.41211</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.26925</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.41835</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">autumn-durian-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/6jiiwx51\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/6jiiwx51</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_205052-6jiiwx51/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:6jiiwx51). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/16mp6on2\" target=\"_blank\">vibrant-terrain-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-20:54:22] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:54:22] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:54:22] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 575ms/step - loss: 0.8962 - val_loss: 0.9590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.6648 - val_loss: 0.8300\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5675 - val_loss: 0.5258\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4636 - val_loss: 0.5275\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4339 - val_loss: 0.5144\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4239 - val_loss: 0.5060\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4403 - val_loss: 0.5625\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4196 - val_loss: 0.4766\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3966 - val_loss: 0.4694\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3860 - val_loss: 0.5201\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3899 - val_loss: 0.4649\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3712 - val_loss: 0.4983\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3662 - val_loss: 0.4606\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3606 - val_loss: 0.4890\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3592 - val_loss: 0.4576\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3706 - val_loss: 0.4541\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3770 - val_loss: 0.5592\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3800 - val_loss: 0.4679\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3688 - val_loss: 0.4509\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3524 - val_loss: 0.4547\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3400 - val_loss: 0.4563\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3308 - val_loss: 0.4858\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-20:54:56] Training the entire fine-tuned model...\n",
      "[2022_04_09-20:55:04] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3452WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1035s vs `on_train_batch_end` time: 0.1292s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1035s vs `on_train_batch_end` time: 0.1292s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 714ms/step - loss: 0.3452 - val_loss: 0.4553\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3396 - val_loss: 0.4680\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3378 - val_loss: 0.4653\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3373 - val_loss: 0.4575\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_09-20:55:21] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-20:55:21] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:55:21] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3368WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1012s vs `on_train_batch_end` time: 0.1269s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1012s vs `on_train_batch_end` time: 0.1269s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 699ms/step - loss: 0.3368 - val_loss: 0.4553\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>468</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  468  11\n",
       "1  119  16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19753086419753085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:16mp6on2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 13491... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▁</td></tr><tr><td>loss</td><td>█▅▄▃▂▂▂▂▂▂▂▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▂▂▂▂▃▁▁▂▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>18</td></tr><tr><td>best_val_loss</td><td>0.45091</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33685</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.45529</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">vibrant-terrain-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/16mp6on2\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/16mp6on2</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_205405-16mp6on2/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:16mp6on2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/1nxt666u\" target=\"_blank\">solar-resonance-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-20:56:16] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:56:16] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:56:16] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 704ms/step - loss: 0.9701 - val_loss: 0.8061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 223ms/step - loss: 0.7334 - val_loss: 0.7177\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 219ms/step - loss: 0.6647 - val_loss: 0.7271\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 220ms/step - loss: 0.5883 - val_loss: 0.5606\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 225ms/step - loss: 0.5500 - val_loss: 0.5448\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 219ms/step - loss: 0.4886 - val_loss: 0.4637\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 223ms/step - loss: 0.4742 - val_loss: 0.5095\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 223ms/step - loss: 0.4710 - val_loss: 0.4548\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 225ms/step - loss: 0.4572 - val_loss: 0.4661\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 220ms/step - loss: 0.4246 - val_loss: 0.4211\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 0.4200 - val_loss: 0.4248\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.4066 - val_loss: 0.4206\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 221ms/step - loss: 0.4107 - val_loss: 0.4137\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 254ms/step - loss: 0.4219 - val_loss: 0.4430\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 220ms/step - loss: 0.4212 - val_loss: 0.4619\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 219ms/step - loss: 0.4346 - val_loss: 0.5109\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-20:56:44] Training the entire fine-tuned model...\n",
      "[2022_04_09-20:56:52] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 846ms/step - loss: 0.3996 - val_loss: 0.4165\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3905 - val_loss: 0.4191\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3918 - val_loss: 0.4190\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3901 - val_loss: 0.4153\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3941 - val_loss: 0.4142\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3853 - val_loss: 0.4141\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3891 - val_loss: 0.4146\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3825 - val_loss: 0.4159\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3832 - val_loss: 0.4162\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_09-20:57:17] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-20:57:17] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:57:17] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 819ms/step - loss: 0.3837 - val_loss: 0.4140\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>539</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  539  11\n",
       "1  111  16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2077922077922078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1nxt666u) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 13722... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██▁▁▂▂▃▃▄▄▅▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▇▄▃▂▃▂▂▁▁▁▁▂▂▃▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>12</td></tr><tr><td>best_val_loss</td><td>0.41375</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38369</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41404</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">solar-resonance-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/1nxt666u\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/1nxt666u</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_205600-1nxt666u/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1nxt666u). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/kp1pn3lg\" target=\"_blank\">resilient-mountain-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-20:58:08] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:58:09] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:58:09] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 599ms/step - loss: 0.9205 - val_loss: 0.9632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.6507 - val_loss: 1.0191\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.6337 - val_loss: 0.7776\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.6139 - val_loss: 0.7854\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.5422 - val_loss: 0.5163\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4889 - val_loss: 0.5188\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4570 - val_loss: 0.5657\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4552 - val_loss: 0.4810\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4122 - val_loss: 0.5718\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4271 - val_loss: 0.4654\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3985 - val_loss: 0.4642\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.3867 - val_loss: 0.4640\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3686 - val_loss: 0.4735\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3702 - val_loss: 0.4660\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4038 - val_loss: 0.5146\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-20:58:36] Training the entire fine-tuned model...\n",
      "[2022_04_09-20:58:46] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3798WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1300s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1300s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 680ms/step - loss: 0.3798 - val_loss: 0.4605\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3682 - val_loss: 0.4677\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3676 - val_loss: 0.4685\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3622 - val_loss: 0.4712\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_09-20:59:03] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-20:59:03] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:59:03] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3831WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1293s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1293s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 690ms/step - loss: 0.3831 - val_loss: 0.4614\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>469</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  469  10\n",
       "1  121  14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1761006289308176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:kp1pn3lg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 13950... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█▁▁▂▃▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▂▂▂▂▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇█▅▅▂▂▂▁▂▁▁▁▁▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.46049</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38308</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.4614</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">resilient-mountain-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/kp1pn3lg\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/kp1pn3lg</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_205753-kp1pn3lg/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:kp1pn3lg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/2t7jqs9u\" target=\"_blank\">honest-moon-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-20:59:57] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:59:57] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-20:59:57] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 720ms/step - loss: 0.9316 - val_loss: 0.8240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.6862 - val_loss: 0.5946\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5422 - val_loss: 0.6203\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5349 - val_loss: 0.4620\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5004 - val_loss: 0.4428\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4734 - val_loss: 0.4372\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4461 - val_loss: 0.4305\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4439 - val_loss: 0.4294\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4258 - val_loss: 0.4372\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4128 - val_loss: 0.4248\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4064 - val_loss: 0.4362\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4040 - val_loss: 0.4178\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3984 - val_loss: 0.4322\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3901 - val_loss: 0.4153\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3801 - val_loss: 0.4162\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3746 - val_loss: 0.4186\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3709 - val_loss: 0.4116\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3768 - val_loss: 0.4306\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3693 - val_loss: 0.4128\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3686 - val_loss: 0.4325\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-21:00:31] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:00:56] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 839ms/step - loss: 0.3742 - val_loss: 0.4106\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3655 - val_loss: 0.4172\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3622 - val_loss: 0.4086\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3647 - val_loss: 0.4088\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3537 - val_loss: 0.4139\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3691 - val_loss: 0.4069\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3477 - val_loss: 0.4137\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3486 - val_loss: 0.4069\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3386 - val_loss: 0.4085\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_09-21:01:21] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:01:21] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:01:21] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 842ms/step - loss: 0.3446 - val_loss: 0.4078\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>531</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  531  19\n",
       "1  101  26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3023255813953488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2t7jqs9u) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 14148... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▂▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▅▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.40685</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3446</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.40777</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">honest-moon-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/2t7jqs9u\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/2t7jqs9u</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_205942-2t7jqs9u/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2t7jqs9u). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/xbuh4ze5\" target=\"_blank\">zany-thunder-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-21:02:13] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:02:13] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:02:13] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 563ms/step - loss: 0.8531 - val_loss: 1.0024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.7090 - val_loss: 0.7348\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5419 - val_loss: 0.5056\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4796 - val_loss: 0.4954\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4337 - val_loss: 0.5259\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4183 - val_loss: 0.4742\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4155 - val_loss: 0.4700\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3996 - val_loss: 0.5604\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4125 - val_loss: 0.4663\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3932 - val_loss: 0.4683\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3917 - val_loss: 0.5088\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3772 - val_loss: 0.4579\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3769 - val_loss: 0.4673\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3626 - val_loss: 0.4692\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3733 - val_loss: 0.4600\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-21:02:41] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:02:50] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3794WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0996s vs `on_train_batch_end` time: 0.1273s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0996s vs `on_train_batch_end` time: 0.1273s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 685ms/step - loss: 0.3794 - val_loss: 0.4549\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3660 - val_loss: 0.4539\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3680 - val_loss: 0.4952\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3655 - val_loss: 0.4551\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3501 - val_loss: 0.4757\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_09-21:03:08] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:03:08] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:03:08] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3596WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1003s vs `on_train_batch_end` time: 0.1291s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1003s vs `on_train_batch_end` time: 0.1291s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 688ms/step - loss: 0.3596 - val_loss: 0.4575\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>474</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  474   5\n",
       "1  121  14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18181818181818182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:xbuh4ze5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 14415... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█▁▁▂▃▃▁</td></tr><tr><td>loss</td><td>█▆▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▂▂▂▁▁▂▁▁▂▁▁▁▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.45391</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35955</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.45747</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">zany-thunder-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/xbuh4ze5\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/xbuh4ze5</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_210158-xbuh4ze5/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:xbuh4ze5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/2f518ycx\" target=\"_blank\">crimson-resonance-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-21:04:00] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:04:00] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:04:00] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 731ms/step - loss: 0.9229 - val_loss: 0.7880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.7460 - val_loss: 0.6290\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5914 - val_loss: 0.6437\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5629 - val_loss: 0.5174\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5272 - val_loss: 0.5121\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4944 - val_loss: 0.4369\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4691 - val_loss: 0.4339\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4566 - val_loss: 0.4243\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4468 - val_loss: 0.4212\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4276 - val_loss: 0.4419\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4203 - val_loss: 0.4166\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4113 - val_loss: 0.4285\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4081 - val_loss: 0.4197\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4189 - val_loss: 0.4228\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-21:04:27] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:04:38] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 840ms/step - loss: 0.4113 - val_loss: 0.4151\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.4009 - val_loss: 0.4172\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3943 - val_loss: 0.4155\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3844 - val_loss: 0.4120\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3817 - val_loss: 0.4082\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3844 - val_loss: 0.4105\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3811 - val_loss: 0.4055\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3925 - val_loss: 0.4046\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3652 - val_loss: 0.4333\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3603 - val_loss: 0.4078\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3555 - val_loss: 0.4259\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_09-21:05:06] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:05:06] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:05:06] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 868ms/step - loss: 0.3595 - val_loss: 0.4062\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>538</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  538  12\n",
       "1  107  20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25157232704402516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2f518ycx) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 14609... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▄▅▅▆▆▇▇█▁▂▂▃▃▄▄▅▅▆▆▁</td></tr><tr><td>loss</td><td>█▆▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▅▃▃▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.40462</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35953</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.40616</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">crimson-resonance-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/2f518ycx\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/2f518ycx</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_210345-2f518ycx/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2f518ycx). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/32zo2idh\" target=\"_blank\">electric-pond-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-21:06:02] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:06:02] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:06:02] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 596ms/step - loss: 0.8077 - val_loss: 0.7349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.5647 - val_loss: 0.5694\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4771 - val_loss: 0.5422\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4417 - val_loss: 0.4917\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4135 - val_loss: 0.4820\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4039 - val_loss: 0.4732\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3949 - val_loss: 0.4805\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3916 - val_loss: 0.4651\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3918 - val_loss: 0.5004\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3900 - val_loss: 0.5017\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3869 - val_loss: 0.4908\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-21:06:25] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:06:34] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4322WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1001s vs `on_train_batch_end` time: 0.1294s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1001s vs `on_train_batch_end` time: 0.1294s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 692ms/step - loss: 0.4322 - val_loss: 0.4849\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3815 - val_loss: 0.4604\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3737 - val_loss: 0.4611\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3641 - val_loss: 0.4653\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3743 - val_loss: 0.5318\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_09-21:06:52] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:06:52] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:06:52] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3754WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1013s vs `on_train_batch_end` time: 0.1285s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1013s vs `on_train_batch_end` time: 0.1285s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 694ms/step - loss: 0.3754 - val_loss: 0.4607\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>474</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  474   5\n",
       "1  124  11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1456953642384106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:32zo2idh) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 14829... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▅▅▆▇▇█▁▂▂▃▄▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▁▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▂▁▂▂▂▂▁▁▁▃▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.46041</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37538</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.46072</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">electric-pond-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/32zo2idh\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/32zo2idh</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_210545-32zo2idh/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:32zo2idh). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/1gnzyo8j\" target=\"_blank\">grateful-glitter-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-21:07:47] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:07:47] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:07:47] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 731ms/step - loss: 0.8703 - val_loss: 0.8930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.7138 - val_loss: 0.5408\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5281 - val_loss: 0.4818\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5025 - val_loss: 0.4418\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4819 - val_loss: 0.4430\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4699 - val_loss: 0.4840\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.4585 - val_loss: 0.4499\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-21:08:05] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:08:13] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 840ms/step - loss: 0.4774 - val_loss: 0.4419\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.4657 - val_loss: 0.4819\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.4520 - val_loss: 0.4400\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.4421 - val_loss: 0.4551\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.4192 - val_loss: 0.4224\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3995 - val_loss: 0.4252\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3624 - val_loss: 0.4097\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3365 - val_loss: 0.4244\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3098 - val_loss: 0.5188\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2951 - val_loss: 0.5301\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_09-21:08:41] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:08:41] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:08:41] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 828ms/step - loss: 0.3405 - val_loss: 0.4060\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>516</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  516  34\n",
       "1   99  28"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2962962962962963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1gnzyo8j) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 15011... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▁▂▃▃▄▅▆▆▇█▁</td></tr><tr><td>loss</td><td>█▆▄▄▃▃▃▃▃▃▃▃▂▂▂▁▁▂</td></tr><tr><td>lr</td><td>███████▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▂▂▂▂▁▂▁▁▁▁▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.40604</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34049</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.40604</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">grateful-glitter-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/1gnzyo8j\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/1gnzyo8j</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_210731-1gnzyo8j/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1gnzyo8j). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/1z9wfwla\" target=\"_blank\">smooth-vortex-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-21:09:35] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:09:35] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:09:35] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 571ms/step - loss: 0.8584 - val_loss: 1.0859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.7235 - val_loss: 0.9636\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.6191 - val_loss: 0.5435\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4999 - val_loss: 0.5676\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4301 - val_loss: 0.4822\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4273 - val_loss: 0.4747\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4143 - val_loss: 0.4818\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3959 - val_loss: 0.4715\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3936 - val_loss: 0.4690\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3839 - val_loss: 0.5063\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3856 - val_loss: 0.4719\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4004 - val_loss: 0.4826\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3665 - val_loss: 0.4606\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3650 - val_loss: 0.4669\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3697 - val_loss: 0.4598\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3710 - val_loss: 0.4694\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3584 - val_loss: 0.4589\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3667 - val_loss: 0.4639\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3660 - val_loss: 0.4641\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3740 - val_loss: 0.4554\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3669 - val_loss: 0.4800\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3626 - val_loss: 0.4549\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3529 - val_loss: 0.4621\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3604 - val_loss: 0.4679\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3614 - val_loss: 0.4624\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3512 - val_loss: 0.4608\n",
      "[2022_04_09-21:10:15] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:10:33] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3637WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.1298s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.1298s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 683ms/step - loss: 0.3637 - val_loss: 0.4657\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3639 - val_loss: 0.4597\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3531 - val_loss: 0.4603\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3550 - val_loss: 0.4626\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3546 - val_loss: 0.4606\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3521 - val_loss: 0.4605\n",
      "[2022_04_09-21:10:53] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:10:53] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:10:53] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3584WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1008s vs `on_train_batch_end` time: 0.1272s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1008s vs `on_train_batch_end` time: 0.1272s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 686ms/step - loss: 0.3584 - val_loss: 0.4598\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>476</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  476   3\n",
       "1  121  14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18421052631578946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1z9wfwla) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 15196... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▆▅▃▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>21</td></tr><tr><td>best_val_loss</td><td>0.45491</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35844</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.45985</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">smooth-vortex-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/1z9wfwla\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/1z9wfwla</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_210919-1z9wfwla/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1z9wfwla). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/10ten8ha\" target=\"_blank\">earnest-pine-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-21:11:49] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:11:49] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:11:49] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 1s/step - loss: 0.8301 - val_loss: 0.9221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.7100 - val_loss: 0.5402\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5461 - val_loss: 0.4596\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5090 - val_loss: 0.4463\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4656 - val_loss: 0.4376\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4597 - val_loss: 0.4542\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4306 - val_loss: 0.4255\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4280 - val_loss: 0.4463\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4294 - val_loss: 0.4185\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4035 - val_loss: 0.4408\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4025 - val_loss: 0.4141\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3988 - val_loss: 0.4287\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3928 - val_loss: 0.4146\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3933 - val_loss: 0.4194\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3835 - val_loss: 0.4099\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3819 - val_loss: 0.4292\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3822 - val_loss: 0.4091\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3754 - val_loss: 0.4090\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3711 - val_loss: 0.4278\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3784 - val_loss: 0.4111\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3716 - val_loss: 0.4089\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3729 - val_loss: 0.4103\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3733 - val_loss: 0.4134\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3703 - val_loss: 0.4141\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3618 - val_loss: 0.4128\n",
      "[2022_04_09-21:12:28] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:12:36] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 848ms/step - loss: 0.3685 - val_loss: 0.4121\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3688 - val_loss: 0.4119\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3791 - val_loss: 0.4122\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3645 - val_loss: 0.4113\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3670 - val_loss: 0.4110\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3630 - val_loss: 0.4109\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3685 - val_loss: 0.4110\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3711 - val_loss: 0.4113\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3602 - val_loss: 0.4099\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3591 - val_loss: 0.4102\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3787 - val_loss: 0.4096\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3658 - val_loss: 0.4103\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3634 - val_loss: 0.4103\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3565 - val_loss: 0.4092\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3636 - val_loss: 0.4082\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3666 - val_loss: 0.4101\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3641 - val_loss: 0.4099\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3529 - val_loss: 0.4113\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3616 - val_loss: 0.4104\n",
      "[2022_04_09-21:13:19] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:13:19] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:13:19] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 838ms/step - loss: 0.3664 - val_loss: 0.4081\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>534</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  534  16\n",
       "1  103  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2874251497005988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:10ten8ha) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 15465... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▄▄▄▅▅▅▅▆▆▇▇▇▇██▁▂▂▂▂▃▃▃▄▄▅▅▅▅▆▆▁</td></tr><tr><td>loss</td><td>█▆▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.40812</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36644</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.40812</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">earnest-pine-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/10ten8ha\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/10ten8ha</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_211132-10ten8ha/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:10ten8ha). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/15b99gfh\" target=\"_blank\">glamorous-tree-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-21:14:13] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:14:13] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:14:13] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 590ms/step - loss: 0.8198 - val_loss: 0.7945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.6053 - val_loss: 0.6232\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.5003 - val_loss: 0.5773\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4674 - val_loss: 0.5359\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4627 - val_loss: 0.6059\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4319 - val_loss: 0.4764\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3938 - val_loss: 0.5040\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3943 - val_loss: 0.4655\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3880 - val_loss: 0.4635\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3853 - val_loss: 0.4978\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3736 - val_loss: 0.4800\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3935 - val_loss: 0.4708\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3840 - val_loss: 0.4920\n",
      "[2022_04_09-21:14:38] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:14:50] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3892WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1017s vs `on_train_batch_end` time: 0.1262s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1017s vs `on_train_batch_end` time: 0.1262s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 684ms/step - loss: 0.3892 - val_loss: 0.4742\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3816 - val_loss: 0.4739\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3812 - val_loss: 0.4641\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3689 - val_loss: 0.4744\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3668 - val_loss: 0.4635\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3651 - val_loss: 0.4688\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3594 - val_loss: 0.4799\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3568 - val_loss: 0.4616\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3606 - val_loss: 0.4647\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3483 - val_loss: 0.4797\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3439 - val_loss: 0.4612\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3407 - val_loss: 0.4744\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3420 - val_loss: 0.4631\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3390 - val_loss: 0.4759\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3273 - val_loss: 0.4718\n",
      "[2022_04_09-21:15:28] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:15:28] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:15:28] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3508WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0990s vs `on_train_batch_end` time: 0.1279s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0990s vs `on_train_batch_end` time: 0.1279s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 685ms/step - loss: 0.3508 - val_loss: 0.4636\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>464</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>117</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  464  15\n",
       "1  117  18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21428571428571427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:15b99gfh) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 15787... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▁▁▂▃▃▃▄▅▅▅▆▇▇▇█▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▃▄▁▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_val_loss</td><td>0.46124</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35079</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.46362</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">glamorous-tree-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/15b99gfh\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/15b99gfh</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_211357-15b99gfh/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:15b99gfh). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/nb02lnfq\" target=\"_blank\">vivid-paper-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-21:16:32] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:16:32] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:16:32] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 728ms/step - loss: 0.8572 - val_loss: 0.6954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.6310 - val_loss: 0.4682\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5492 - val_loss: 0.4619\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5250 - val_loss: 0.5059\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4768 - val_loss: 0.4511\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4593 - val_loss: 0.4627\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4383 - val_loss: 0.4280\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4274 - val_loss: 0.4442\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4305 - val_loss: 0.4288\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4145 - val_loss: 0.4184\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4115 - val_loss: 0.4189\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4011 - val_loss: 0.4151\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3909 - val_loss: 0.4206\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3745 - val_loss: 0.4108\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3942 - val_loss: 0.4133\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3868 - val_loss: 0.4160\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3702 - val_loss: 0.4185\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3636 - val_loss: 0.4092\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3641 - val_loss: 0.4115\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3655 - val_loss: 0.4142\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3689 - val_loss: 0.4090\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3688 - val_loss: 0.4076\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3689 - val_loss: 0.4159\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3683 - val_loss: 0.4099\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3653 - val_loss: 0.4082\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3697 - val_loss: 0.4102\n",
      "[2022_04_09-21:17:12] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:17:20] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 851ms/step - loss: 0.3599 - val_loss: 0.4082\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3652 - val_loss: 0.4128\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3615 - val_loss: 0.4104\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3520 - val_loss: 0.4047\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3454 - val_loss: 0.4087\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3474 - val_loss: 0.4068\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3471 - val_loss: 0.4052\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3433 - val_loss: 0.4071\n",
      "[2022_04_09-21:17:42] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:17:42] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:17:46] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 862ms/step - loss: 0.3434 - val_loss: 0.4052\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>537</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  537  13\n",
       "1  103  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2926829268292683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:nb02lnfq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 16038... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>0.40472</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3434</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.40522</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">vivid-paper-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/nb02lnfq\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/nb02lnfq</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_211617-nb02lnfq/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:nb02lnfq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/3t36juu2\" target=\"_blank\">polar-disco-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-21:18:42] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:18:42] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:18:42] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 557ms/step - loss: 1.0019 - val_loss: 0.6626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.6155 - val_loss: 0.8416\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5472 - val_loss: 0.5930\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5015 - val_loss: 0.5563\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4575 - val_loss: 0.5004\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4269 - val_loss: 0.4762\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4078 - val_loss: 0.4725\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4044 - val_loss: 0.4671\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4035 - val_loss: 0.5562\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4147 - val_loss: 0.4605\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3777 - val_loss: 0.4919\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3823 - val_loss: 0.4973\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3895 - val_loss: 0.4603\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3943 - val_loss: 0.4553\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3640 - val_loss: 0.4570\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3534 - val_loss: 0.4530\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3615 - val_loss: 0.4623\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3511 - val_loss: 0.4604\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3421 - val_loss: 0.4507\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3627 - val_loss: 0.4521\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3592 - val_loss: 0.4873\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3455 - val_loss: 0.5093\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3625 - val_loss: 0.4671\n",
      "[2022_04_09-21:19:17] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:19:40] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3453WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.1290s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.1290s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 681ms/step - loss: 0.3453 - val_loss: 0.4505\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3441 - val_loss: 0.4566\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3330 - val_loss: 0.4510\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3391 - val_loss: 0.4653\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3446 - val_loss: 0.4674\n",
      "[2022_04_09-21:19:58] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:19:58] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:19:58] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3372WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.1296s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.1296s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 692ms/step - loss: 0.3372 - val_loss: 0.4507\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>467</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>113</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  467  12\n",
       "1  113  22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2603550295857988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3t36juu2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 16309... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████▃▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅█▄▃▂▁▁▁▃▁▂▂▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.45045</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33717</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.45073</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">polar-disco-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/3t36juu2\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/3t36juu2</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_211825-3t36juu2/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3t36juu2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/1v9kvacz\" target=\"_blank\">deft-haze-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-21:20:53] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:20:53] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:20:53] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 708ms/step - loss: 0.9187 - val_loss: 0.8146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.6734 - val_loss: 0.5437\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5800 - val_loss: 0.4544\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.5467 - val_loss: 0.4469\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4683 - val_loss: 0.4371\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4734 - val_loss: 0.4737\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4515 - val_loss: 0.4292\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4424 - val_loss: 0.4401\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4191 - val_loss: 0.4216\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4142 - val_loss: 0.4308\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4144 - val_loss: 0.4263\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4000 - val_loss: 0.4182\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3980 - val_loss: 0.4377\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3815 - val_loss: 0.4152\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3860 - val_loss: 0.4853\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4057 - val_loss: 0.4121\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3918 - val_loss: 0.4110\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3762 - val_loss: 0.4118\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3705 - val_loss: 0.4160\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3644 - val_loss: 0.4153\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3605 - val_loss: 0.4168\n",
      "[2022_04_09-21:21:26] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:21:34] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 826ms/step - loss: 0.3730 - val_loss: 0.4105\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3671 - val_loss: 0.4093\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3588 - val_loss: 0.4078\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3617 - val_loss: 0.4108\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3518 - val_loss: 0.4045\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3453 - val_loss: 0.4037\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3367 - val_loss: 0.4153\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3413 - val_loss: 0.4050\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3148 - val_loss: 0.4059\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3172 - val_loss: 0.4074\n",
      "[2022_04_09-21:22:00] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:22:00] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:22:00] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 835ms/step - loss: 0.3310 - val_loss: 0.4052\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>512</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  512  38\n",
       "1   97  30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3076923076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1v9kvacz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 16558... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▂▁▂▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.40371</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.331</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.40521</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">deft-haze-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/1v9kvacz\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/1v9kvacz</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_212037-1v9kvacz/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1v9kvacz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/25cowwhl\" target=\"_blank\">glamorous-waterfall-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-21:22:57] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:22:57] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:22:57] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 584ms/step - loss: 0.8360 - val_loss: 0.6519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5151 - val_loss: 0.5856\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4541 - val_loss: 0.5214\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4564 - val_loss: 0.5046\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4426 - val_loss: 0.4893\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4119 - val_loss: 0.5756\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4261 - val_loss: 0.4777\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4010 - val_loss: 0.4749\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3921 - val_loss: 0.4668\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3754 - val_loss: 0.4726\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3778 - val_loss: 0.4668\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3664 - val_loss: 0.4594\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3714 - val_loss: 0.4535\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3694 - val_loss: 0.4561\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4074 - val_loss: 0.5179\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3725 - val_loss: 0.4956\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3489 - val_loss: 0.4527\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3516 - val_loss: 0.4644\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3392 - val_loss: 0.4749\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3506 - val_loss: 0.4549\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3368 - val_loss: 0.4598\n",
      "[2022_04_09-21:23:30] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:23:40] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5317WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1020s vs `on_train_batch_end` time: 0.1283s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1020s vs `on_train_batch_end` time: 0.1283s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 685ms/step - loss: 0.5317 - val_loss: 0.6623\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4567 - val_loss: 0.4727\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3684 - val_loss: 0.4628\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3622 - val_loss: 0.4909\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3336 - val_loss: 0.4622\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3148 - val_loss: 0.4659\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3219 - val_loss: 0.4594\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.2979 - val_loss: 0.5184\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.2904 - val_loss: 0.4844\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.2771 - val_loss: 0.4867\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.2409 - val_loss: 0.4530\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.2409 - val_loss: 0.4586\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.2406 - val_loss: 0.4644\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.2216 - val_loss: 0.4602\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.2064 - val_loss: 0.4694\n",
      "[2022_04_09-21:24:17] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:24:17] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:24:27] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.2418WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0990s vs `on_train_batch_end` time: 0.1303s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0990s vs `on_train_batch_end` time: 0.1303s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 674ms/step - loss: 0.2418 - val_loss: 0.4537\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>443</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  443  36\n",
       "1   94  41"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3867924528301887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:25cowwhl) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 16816... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▁</td></tr><tr><td>loss</td><td>█▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▂▃▂▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▃▂▅▂▂▁▂▁▁▁▁▃▂▁▁▂▁▁█▂▁▂▁▁▁▃▂▂▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>16</td></tr><tr><td>best_val_loss</td><td>0.45266</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.2418</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.45372</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">glamorous-waterfall-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/25cowwhl\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/25cowwhl</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_212239-25cowwhl/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:25cowwhl). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/29pzltl6\" target=\"_blank\">valiant-tree-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-21:25:22] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:25:22] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:25:22] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 734ms/step - loss: 0.9497 - val_loss: 0.6837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.7232 - val_loss: 0.7365\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.7185 - val_loss: 0.6303\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5753 - val_loss: 0.5008\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5114 - val_loss: 0.5139\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4840 - val_loss: 0.4333\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4565 - val_loss: 0.4316\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4534 - val_loss: 0.4269\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4209 - val_loss: 0.4333\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4151 - val_loss: 0.4277\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4068 - val_loss: 0.4184\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4137 - val_loss: 0.4340\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3972 - val_loss: 0.4157\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3948 - val_loss: 0.4162\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 230ms/step - loss: 0.3908 - val_loss: 0.4223\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3805 - val_loss: 0.4117\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3818 - val_loss: 0.4197\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3873 - val_loss: 0.4111\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3706 - val_loss: 0.4330\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3778 - val_loss: 0.4099\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3642 - val_loss: 0.4092\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3703 - val_loss: 0.4189\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3648 - val_loss: 0.4088\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3590 - val_loss: 0.4090\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3539 - val_loss: 0.4278\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3719 - val_loss: 0.4346\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3814 - val_loss: 0.4168\n",
      "[2022_04_09-21:26:01] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:26:19] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 838ms/step - loss: 0.5997 - val_loss: 0.4959\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.4197 - val_loss: 0.4382\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.4051 - val_loss: 0.4161\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3866 - val_loss: 0.4248\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3769 - val_loss: 0.4126\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3710 - val_loss: 0.4218\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3456 - val_loss: 0.4052\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3338 - val_loss: 0.4241\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3492 - val_loss: 0.4058\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3311 - val_loss: 0.4051\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2980 - val_loss: 0.4048\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3080 - val_loss: 0.4153\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3029 - val_loss: 0.4060\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.2970 - val_loss: 0.4060\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.2950 - val_loss: 0.4073\n",
      "[2022_04_09-21:26:56] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:26:56] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:26:56] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 856ms/step - loss: 0.2942 - val_loss: 0.4081\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>514</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  514  36\n",
       "1   96  31"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31958762886597936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:29pzltl6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 17095... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇▇██▁▂▂▂▂▃▃▃▃▄▄▄▅▁</td></tr><tr><td>loss</td><td>█▆▆▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████████▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇█▆▃▃▂▂▁▂▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_val_loss</td><td>0.4048</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.29417</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.4081</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">valiant-tree-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/29pzltl6\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/29pzltl6</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_212506-29pzltl6/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:29pzltl6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/17csflmg\" target=\"_blank\">sage-rain-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-21:28:01] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:28:01] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:28:01] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 599ms/step - loss: 0.8255 - val_loss: 0.6075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5255 - val_loss: 0.4999\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4579 - val_loss: 0.6208\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4758 - val_loss: 0.4837\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4423 - val_loss: 0.4802\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4050 - val_loss: 0.4797\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3974 - val_loss: 0.4803\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3937 - val_loss: 0.5014\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3889 - val_loss: 0.4736\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3805 - val_loss: 0.4996\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3734 - val_loss: 0.4934\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3882 - val_loss: 0.4563\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3789 - val_loss: 0.4551\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3610 - val_loss: 0.5131\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3555 - val_loss: 0.4529\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3508 - val_loss: 0.4496\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3636 - val_loss: 0.4666\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4062 - val_loss: 0.5410\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3474 - val_loss: 0.4923\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3521 - val_loss: 0.4479\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3481 - val_loss: 0.4486\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3475 - val_loss: 0.4648\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3485 - val_loss: 0.4424\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3506 - val_loss: 0.4700\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3418 - val_loss: 0.4493\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3364 - val_loss: 0.4483\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3369 - val_loss: 0.4547\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3296 - val_loss: 0.4507\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3273 - val_loss: 0.4469\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-21:28:44] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:29:00] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3353WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1007s vs `on_train_batch_end` time: 0.1302s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1007s vs `on_train_batch_end` time: 0.1302s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 670ms/step - loss: 0.3353 - val_loss: 0.4514\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3386 - val_loss: 0.4564\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3331 - val_loss: 0.4473\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3221 - val_loss: 0.4470\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3315 - val_loss: 0.4533\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3320 - val_loss: 0.4573\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3307 - val_loss: 0.4488\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3180 - val_loss: 0.4476\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3403 - val_loss: 0.4480\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3416 - val_loss: 0.4495\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_09-21:29:27] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:29:27] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:29:27] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3405WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1289s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1289s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 690ms/step - loss: 0.3405 - val_loss: 0.4473\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>470</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  470   9\n",
       "1  114  21"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2545454545454545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:17csflmg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 17407... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██▁▁▁▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇▃█▃▂▂▂▃▂▃▃▂▁▄▁▁▂▅▃▁▁▂▁▂▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>22</td></tr><tr><td>best_val_loss</td><td>0.4424</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34047</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.44732</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sage-rain-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/17csflmg\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/17csflmg</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_212745-17csflmg/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:17csflmg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/tgc08epb\" target=\"_blank\">ethereal-grass-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-21:30:22] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:30:22] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:30:22] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 735ms/step - loss: 0.8273 - val_loss: 0.9115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.7359 - val_loss: 0.5179\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5828 - val_loss: 0.4493\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5326 - val_loss: 0.4515\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4823 - val_loss: 0.4423\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4523 - val_loss: 0.4632\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4503 - val_loss: 0.4382\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4365 - val_loss: 0.4626\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4242 - val_loss: 0.4212\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4120 - val_loss: 0.4422\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4124 - val_loss: 0.4191\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3924 - val_loss: 0.4176\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3888 - val_loss: 0.4514\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3955 - val_loss: 0.4136\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3857 - val_loss: 0.4127\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3971 - val_loss: 0.4541\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3569 - val_loss: 0.4231\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3765 - val_loss: 0.4358\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3715 - val_loss: 0.4111\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3831 - val_loss: 0.4103\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3662 - val_loss: 0.4242\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3706 - val_loss: 0.4151\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3627 - val_loss: 0.4098\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3522 - val_loss: 0.4178\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3591 - val_loss: 0.4129\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3716 - val_loss: 0.4133\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3587 - val_loss: 0.4116\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3605 - val_loss: 0.4114\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3554 - val_loss: 0.4109\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-21:31:04] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:31:13] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 826ms/step - loss: 0.3678 - val_loss: 0.4141\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3665 - val_loss: 0.4146\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3574 - val_loss: 0.4117\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3584 - val_loss: 0.4103\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3655 - val_loss: 0.4115\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3528 - val_loss: 0.4117\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3651 - val_loss: 0.4112\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3559 - val_loss: 0.4110\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3589 - val_loss: 0.4107\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3598 - val_loss: 0.4110\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_09-21:31:40] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:31:40] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:31:40] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 11s 857ms/step - loss: 0.3670 - val_loss: 0.4105\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  536  14\n",
       "1  107  20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2484472049689441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:tgc08epb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 17711... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██▁▁▁▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▇▄▄▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▁▂▁▂▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>22</td></tr><tr><td>best_val_loss</td><td>0.40979</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36702</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41048</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">ethereal-grass-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/tgc08epb\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/tgc08epb</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_213006-tgc08epb/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:tgc08epb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/13h0y0d3\" target=\"_blank\">expert-dawn-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-21:32:34] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:32:34] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:32:34] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 578ms/step - loss: 0.8381 - val_loss: 0.8166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5842 - val_loss: 0.6829\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5238 - val_loss: 0.5303\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4529 - val_loss: 0.4818\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4203 - val_loss: 0.4831\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4089 - val_loss: 0.5509\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4043 - val_loss: 0.4906\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4139 - val_loss: 0.5107\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4022 - val_loss: 0.4746\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3935 - val_loss: 0.4673\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3922 - val_loss: 0.5029\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3916 - val_loss: 0.4741\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3902 - val_loss: 0.4711\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3746 - val_loss: 0.4755\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3824 - val_loss: 0.4793\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3779 - val_loss: 0.4783\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-21:33:02] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:33:11] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3923WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1001s vs `on_train_batch_end` time: 0.1296s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1001s vs `on_train_batch_end` time: 0.1296s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 679ms/step - loss: 0.3923 - val_loss: 0.5150\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3872 - val_loss: 0.4647\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3884 - val_loss: 0.4712\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3816 - val_loss: 0.4920\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3725 - val_loss: 0.4621\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3661 - val_loss: 0.4808\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3641 - val_loss: 0.4759\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3619 - val_loss: 0.4672\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3526 - val_loss: 0.4741\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3573 - val_loss: 0.4794\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3533 - val_loss: 0.4776\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_09-21:33:40] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:33:40] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:33:40] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3712WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0996s vs `on_train_batch_end` time: 0.1288s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0996s vs `on_train_batch_end` time: 0.1288s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 685ms/step - loss: 0.3712 - val_loss: 0.4618\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>468</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  468  11\n",
       "1  118  17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2085889570552147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:13h0y0d3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 18006... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██▁▁▂▂▃▃▄▄▅▅▆▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▂▁▁▃▂▂▁▁▂▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.46182</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37115</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.46182</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">expert-dawn-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/13h0y0d3\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/13h0y0d3</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_213218-13h0y0d3/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:13h0y0d3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/317ez2da\" target=\"_blank\">glorious-cherry-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-21:34:35] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:34:35] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:34:35] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 717ms/step - loss: 0.8017 - val_loss: 0.7666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.6175 - val_loss: 0.4695\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5178 - val_loss: 0.4572\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5015 - val_loss: 0.5169\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.4678 - val_loss: 0.4391\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4474 - val_loss: 0.4343\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4473 - val_loss: 0.4517\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4382 - val_loss: 0.4412\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4175 - val_loss: 0.4461\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4418 - val_loss: 0.4178\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4116 - val_loss: 0.4618\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4086 - val_loss: 0.4189\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4180 - val_loss: 0.4168\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3986 - val_loss: 0.4399\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3963 - val_loss: 0.4151\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4039 - val_loss: 0.4173\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3970 - val_loss: 0.4170\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3846 - val_loss: 0.4191\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3938 - val_loss: 0.4176\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3788 - val_loss: 0.4162\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3858 - val_loss: 0.4152\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-21:35:08] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:35:34] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 836ms/step - loss: 0.3927 - val_loss: 0.4198\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3981 - val_loss: 0.4154\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3972 - val_loss: 0.4131\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3832 - val_loss: 0.4180\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3797 - val_loss: 0.4122\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3813 - val_loss: 0.4108\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3818 - val_loss: 0.4132\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3744 - val_loss: 0.4128\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3749 - val_loss: 0.4056\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3725 - val_loss: 0.4205\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3565 - val_loss: 0.4041\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3507 - val_loss: 0.4089\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3546 - val_loss: 0.4126\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3549 - val_loss: 0.4043\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3449 - val_loss: 0.4036\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3405 - val_loss: 0.4099\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3404 - val_loss: 0.4097\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3351 - val_loss: 0.4071\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3487 - val_loss: 0.4058\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3339 - val_loss: 0.4053\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3295 - val_loss: 0.4051\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "[2022_04_09-21:36:20] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:36:20] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:36:20] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 11s 1s/step - loss: 0.3404 - val_loss: 0.4049\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>522</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  522  28\n",
       "1   96  31"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:317ez2da) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 18243... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▂▃▃▄▄▅▅▅▆▆▆▇▇▇█▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>14</td></tr><tr><td>best_val_loss</td><td>0.40364</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34041</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.40488</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">glorious-cherry-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/317ez2da\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/317ez2da</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_213419-317ez2da/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:317ez2da). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/2p4yrgz3\" target=\"_blank\">pleasant-glade-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-21:37:20] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:37:20] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:37:20] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 562ms/step - loss: 0.9216 - val_loss: 0.7132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5795 - val_loss: 0.7283\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4958 - val_loss: 0.5107\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4678 - val_loss: 0.5082\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4374 - val_loss: 0.5826\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4552 - val_loss: 0.5218\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4624 - val_loss: 0.5655\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4155 - val_loss: 0.4701\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4148 - val_loss: 0.4737\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3970 - val_loss: 0.5098\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3876 - val_loss: 0.4667\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3993 - val_loss: 0.4692\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3888 - val_loss: 0.4918\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3898 - val_loss: 0.4638\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3899 - val_loss: 0.4811\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3920 - val_loss: 0.4731\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3887 - val_loss: 0.4610\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3906 - val_loss: 0.4912\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3781 - val_loss: 0.4587\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3829 - val_loss: 0.4748\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3703 - val_loss: 0.4667\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3744 - val_loss: 0.4610\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3731 - val_loss: 0.4622\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3727 - val_loss: 0.4658\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3639 - val_loss: 0.4670\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-21:37:57] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:38:05] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3864WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0964s vs `on_train_batch_end` time: 0.1295s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0964s vs `on_train_batch_end` time: 0.1295s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 678ms/step - loss: 0.3864 - val_loss: 0.4592\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3739 - val_loss: 0.4813\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3779 - val_loss: 0.4560\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3591 - val_loss: 0.4710\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3527 - val_loss: 0.4631\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3491 - val_loss: 0.4598\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3479 - val_loss: 0.4578\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3450 - val_loss: 0.4668\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3415 - val_loss: 0.4690\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_09-21:38:30] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:38:30] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:38:32] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3576WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0992s vs `on_train_batch_end` time: 0.1307s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0992s vs `on_train_batch_end` time: 0.1307s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 686ms/step - loss: 0.3576 - val_loss: 0.4583\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>472</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  472   7\n",
       "1  120  15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1910828025477707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2p4yrgz3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 18566... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▂▂▄▃▄▁▁▂▁▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_val_loss</td><td>0.456</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35762</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.45835</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">pleasant-glade-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/2p4yrgz3\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/2p4yrgz3</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_213704-2p4yrgz3/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2p4yrgz3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/15dysewj\" target=\"_blank\">autumn-mountain-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-21:39:26] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:39:26] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:39:26] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 722ms/step - loss: 1.0053 - val_loss: 0.6901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.6612 - val_loss: 0.6547\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5953 - val_loss: 0.7429\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5771 - val_loss: 0.5554\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5432 - val_loss: 0.5566\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4893 - val_loss: 0.4582\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4531 - val_loss: 0.4806\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4589 - val_loss: 0.4238\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4387 - val_loss: 0.4218\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4461 - val_loss: 0.4481\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4329 - val_loss: 0.4289\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4164 - val_loss: 0.4403\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4016 - val_loss: 0.4165\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4067 - val_loss: 0.4145\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3927 - val_loss: 0.4276\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3943 - val_loss: 0.4191\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3946 - val_loss: 0.4132\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3885 - val_loss: 0.4189\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3971 - val_loss: 0.4193\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3840 - val_loss: 0.4142\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3828 - val_loss: 0.4152\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3861 - val_loss: 0.4157\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3872 - val_loss: 0.4152\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-21:40:01] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:40:09] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 847ms/step - loss: 0.4067 - val_loss: 0.4167\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3855 - val_loss: 0.4213\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3865 - val_loss: 0.4147\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3740 - val_loss: 0.4075\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3642 - val_loss: 0.4160\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3557 - val_loss: 0.4093\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3633 - val_loss: 0.4064\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3450 - val_loss: 0.4126\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3470 - val_loss: 0.4052\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3244 - val_loss: 0.4120\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3158 - val_loss: 0.4413\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3096 - val_loss: 0.4223\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3243 - val_loss: 0.4122\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3055 - val_loss: 0.4235\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3064 - val_loss: 0.4100\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_09-21:40:46] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:40:46] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:40:46] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 859ms/step - loss: 0.3354 - val_loss: 0.4048\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>516</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  516  34\n",
       "1   96  31"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3229166666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:15dysewj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 18838... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇▆█▄▄▂▃▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.40485</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33544</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.40485</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">autumn-mountain-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/15dysewj\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/15dysewj</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_213910-15dysewj/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:15dysewj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/2eiebd75\" target=\"_blank\">desert-sunset-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-21:41:41] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:41:41] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:41:41] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 604ms/step - loss: 0.8771 - val_loss: 0.8390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.6198 - val_loss: 0.7152\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4961 - val_loss: 0.4943\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4367 - val_loss: 0.4908\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4160 - val_loss: 0.5015\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4107 - val_loss: 0.4808\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3924 - val_loss: 0.4737\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4038 - val_loss: 0.4945\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3932 - val_loss: 0.5073\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3866 - val_loss: 0.4776\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3772 - val_loss: 0.4627\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3775 - val_loss: 0.4604\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3667 - val_loss: 0.4646\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3697 - val_loss: 0.4644\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3660 - val_loss: 0.4582\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3745 - val_loss: 0.4709\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3730 - val_loss: 0.4614\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3721 - val_loss: 0.4590\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3640 - val_loss: 0.4651\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3552 - val_loss: 0.4643\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3612 - val_loss: 0.4642\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-21:42:15] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:42:23] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4090WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0975s vs `on_train_batch_end` time: 0.1322s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0975s vs `on_train_batch_end` time: 0.1322s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 693ms/step - loss: 0.4090 - val_loss: 0.4568\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4096 - val_loss: 0.4637\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3748 - val_loss: 0.4840\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3488 - val_loss: 0.4499\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3401 - val_loss: 0.4580\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3139 - val_loss: 0.4568\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.2951 - val_loss: 0.4607\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.2661 - val_loss: 0.4846\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.2636 - val_loss: 0.5152\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.2586 - val_loss: 0.4743\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_09-21:42:51] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:42:51] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:43:10] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3393WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1007s vs `on_train_batch_end` time: 0.1282s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1007s vs `on_train_batch_end` time: 0.1282s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 672ms/step - loss: 0.3393 - val_loss: 0.4538\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>467</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  467  12\n",
       "1  114  21"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2eiebd75) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19141... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▃▃▂▂▂▂▁▁▁▁▂</td></tr><tr><td>lr</td><td>██████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>0.44985</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33931</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.45379</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">desert-sunset-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/2eiebd75\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/2eiebd75</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_214125-2eiebd75/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2eiebd75). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/280ipsqs\" target=\"_blank\">apricot-elevator-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-21:44:06] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:44:06] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:44:06] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 739ms/step - loss: 0.9153 - val_loss: 0.8569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.7013 - val_loss: 0.5377\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.5807 - val_loss: 0.4609\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.5089 - val_loss: 0.4425\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4882 - val_loss: 0.4428\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4809 - val_loss: 0.4605\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.4492 - val_loss: 0.4274\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 229ms/step - loss: 0.4235 - val_loss: 0.4318\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4309 - val_loss: 0.4352\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4126 - val_loss: 0.4197\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4161 - val_loss: 0.4532\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4090 - val_loss: 0.4206\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4062 - val_loss: 0.4294\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3877 - val_loss: 0.4155\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3883 - val_loss: 0.4138\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3855 - val_loss: 0.4158\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3936 - val_loss: 0.4220\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3907 - val_loss: 0.4132\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3732 - val_loss: 0.4140\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3794 - val_loss: 0.4160\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3832 - val_loss: 0.4129\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3828 - val_loss: 0.4248\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3819 - val_loss: 0.4146\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3744 - val_loss: 0.4111\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3757 - val_loss: 0.4206\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3753 - val_loss: 0.4108\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3744 - val_loss: 0.4116\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3732 - val_loss: 0.4195\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3744 - val_loss: 0.4107\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3682 - val_loss: 0.4101\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3799 - val_loss: 0.4129\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3698 - val_loss: 0.4124\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3713 - val_loss: 0.4147\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3681 - val_loss: 0.4143\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3733 - val_loss: 0.4130\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3669 - val_loss: 0.4122\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_09-21:44:55] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:45:05] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 856ms/step - loss: 0.4339 - val_loss: 0.4113\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3821 - val_loss: 0.4062\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3783 - val_loss: 0.4109\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3594 - val_loss: 0.4165\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3501 - val_loss: 0.4338\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3377 - val_loss: 0.4011\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3195 - val_loss: 0.3994\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3212 - val_loss: 0.4051\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3222 - val_loss: 0.4017\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3056 - val_loss: 0.4053\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.2963 - val_loss: 0.4025\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.2962 - val_loss: 0.4016\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.2848 - val_loss: 0.4029\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "[2022_04_09-21:45:36] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:45:36] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:45:36] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 855ms/step - loss: 0.3122 - val_loss: 0.4033\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>512</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  512  38\n",
       "1   97  30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3076923076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:280ipsqs) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19399... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▆▄▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>6</td></tr><tr><td>best_val_loss</td><td>0.39944</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31218</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.40329</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">apricot-elevator-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/280ipsqs\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/280ipsqs</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_214350-280ipsqs/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:280ipsqs). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/1hwclfkd\" target=\"_blank\">hearty-plasma-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-21:46:30] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:46:30] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:46:30] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 574ms/step - loss: 0.8328 - val_loss: 0.6595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5426 - val_loss: 0.5658\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4653 - val_loss: 0.5713\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4727 - val_loss: 0.4902\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4213 - val_loss: 0.5099\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4096 - val_loss: 0.5062\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4067 - val_loss: 0.4669\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4047 - val_loss: 0.4666\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4012 - val_loss: 0.6017\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4457 - val_loss: 0.5971\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4536 - val_loss: 0.5063\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4271 - val_loss: 0.4719\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3687 - val_loss: 0.4595\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3608 - val_loss: 0.4891\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3729 - val_loss: 0.4639\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3557 - val_loss: 0.4600\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3678 - val_loss: 0.4771\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3603 - val_loss: 0.4703\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3535 - val_loss: 0.4626\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3547 - val_loss: 0.4598\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3591 - val_loss: 0.4607\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-21:47:04] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:47:12] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3730WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1312s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1312s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 848ms/step - loss: 0.3730 - val_loss: 0.4664\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3596 - val_loss: 0.4723\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3574 - val_loss: 0.4698\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3578 - val_loss: 0.4688\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3556 - val_loss: 0.4659\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3577 - val_loss: 0.4630\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3609 - val_loss: 0.4636\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3599 - val_loss: 0.4622\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3579 - val_loss: 0.4619\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3545 - val_loss: 0.4622\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3611 - val_loss: 0.4654\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 329ms/step - loss: 0.3540 - val_loss: 0.4729\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3556 - val_loss: 0.4710\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3606 - val_loss: 0.4677\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3633 - val_loss: 0.4651\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3567 - val_loss: 0.4631\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3538 - val_loss: 0.4620\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_09-21:47:53] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:47:53] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:47:59] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3542WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.1303s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.1303s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 708ms/step - loss: 0.3542 - val_loss: 0.4620\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>474</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  474   5\n",
       "1  120  15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1935483870967742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1hwclfkd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19747... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▅▂▃▃▁▁▆▆▃▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>12</td></tr><tr><td>best_val_loss</td><td>0.45947</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35418</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.46203</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">hearty-plasma-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/1hwclfkd\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/1hwclfkd</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_214614-1hwclfkd/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1hwclfkd). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/2ndd907l\" target=\"_blank\">scarlet-elevator-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-21:48:53] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:48:53] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:48:53] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 710ms/step - loss: 0.9610 - val_loss: 0.6473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.6649 - val_loss: 0.6377\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.6107 - val_loss: 0.7251\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.5921 - val_loss: 0.5133\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5110 - val_loss: 0.5091\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.5004 - val_loss: 0.4286\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4460 - val_loss: 0.4309\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4539 - val_loss: 0.4427\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4243 - val_loss: 0.4212\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4146 - val_loss: 0.4208\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4030 - val_loss: 0.4181\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4004 - val_loss: 0.4150\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3956 - val_loss: 0.4199\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3960 - val_loss: 0.4199\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3894 - val_loss: 0.4140\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4087 - val_loss: 0.4243\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3784 - val_loss: 0.4111\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3900 - val_loss: 0.4106\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3760 - val_loss: 0.4312\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3702 - val_loss: 0.4140\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3615 - val_loss: 0.4108\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3634 - val_loss: 0.4278\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3636 - val_loss: 0.4143\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3594 - val_loss: 0.4096\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3594 - val_loss: 0.4121\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3424 - val_loss: 0.4115\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3495 - val_loss: 0.4123\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3529 - val_loss: 0.4114\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3491 - val_loss: 0.4116\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3448 - val_loss: 0.4109\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3458 - val_loss: 0.4118\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3450 - val_loss: 0.4122\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-21:49:38] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:49:52] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 833ms/step - loss: 0.3593 - val_loss: 0.4154\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3579 - val_loss: 0.4105\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3520 - val_loss: 0.4087\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3517 - val_loss: 0.4106\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3564 - val_loss: 0.4133\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3553 - val_loss: 0.4143\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3537 - val_loss: 0.4104\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3451 - val_loss: 0.4096\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3461 - val_loss: 0.4099\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3583 - val_loss: 0.4107\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3522 - val_loss: 0.4110\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_09-21:50:20] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:50:20] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:50:20] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 11s 859ms/step - loss: 0.3489 - val_loss: 0.4088\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>531</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  531  19\n",
       "1  103  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2823529411764706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2ndd907l) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 20037... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆▆█▃▃▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_val_loss</td><td>0.40873</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34888</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.40882</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">scarlet-elevator-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/2ndd907l\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/2ndd907l</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_214838-2ndd907l/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2ndd907l). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/3nhn7a8q\" target=\"_blank\">cool-monkey-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-21:51:18] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:51:18] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:51:18] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 596ms/step - loss: 0.8633 - val_loss: 0.7112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5816 - val_loss: 0.5973\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4866 - val_loss: 0.5401\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4373 - val_loss: 0.4884\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4249 - val_loss: 0.4994\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4112 - val_loss: 0.4818\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3981 - val_loss: 0.4967\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3967 - val_loss: 0.4637\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3760 - val_loss: 0.4989\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3916 - val_loss: 0.4636\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3633 - val_loss: 0.4571\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3687 - val_loss: 0.4591\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3593 - val_loss: 0.4776\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3577 - val_loss: 0.4661\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3633 - val_loss: 0.4495\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3931 - val_loss: 0.5415\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3650 - val_loss: 0.5175\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3528 - val_loss: 0.4506\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3737 - val_loss: 0.4499\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3282 - val_loss: 0.4739\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3451 - val_loss: 0.4582\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3457 - val_loss: 0.4517\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3315 - val_loss: 0.4623\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_09-21:51:53] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:52:15] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3615WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1022s vs `on_train_batch_end` time: 0.1273s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1022s vs `on_train_batch_end` time: 0.1273s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 685ms/step - loss: 0.3615 - val_loss: 0.4616\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3501 - val_loss: 0.4566\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3660 - val_loss: 0.4721\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3404 - val_loss: 0.4464\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3472 - val_loss: 0.4679\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3368 - val_loss: 0.4608\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3311 - val_loss: 0.4519\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3293 - val_loss: 0.4697\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3302 - val_loss: 0.4663\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3287 - val_loss: 0.4553\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3306 - val_loss: 0.4515\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3238 - val_loss: 0.4524\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_09-21:52:45] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:52:45] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:53:01] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3411WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0987s vs `on_train_batch_end` time: 0.1315s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0987s vs `on_train_batch_end` time: 0.1315s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 678ms/step - loss: 0.3411 - val_loss: 0.4466\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>462</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  462  17\n",
       "1  109  26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29213483146067415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3nhn7a8q) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 20372... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▄▅▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▁▁▂▂▂▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▂▂▁▂▁▁▁▂▂▁▄▃▁▁▂▁▁▁▁▁▂▁▂▁▁▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>0.44644</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34113</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.44663</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">cool-monkey-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/3nhn7a8q\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/3nhn7a8q</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_215100-3nhn7a8q/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3nhn7a8q). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/1dyjdtbf\" target=\"_blank\">comic-cherry-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-21:53:56] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:53:56] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:53:56] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 699ms/step - loss: 0.9193 - val_loss: 0.8106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.6344 - val_loss: 0.5619\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.5911 - val_loss: 0.4881\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5014 - val_loss: 0.4429\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4583 - val_loss: 0.4384\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4452 - val_loss: 0.4376\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4405 - val_loss: 0.4279\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4314 - val_loss: 0.4782\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4365 - val_loss: 0.4413\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4320 - val_loss: 0.4366\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4094 - val_loss: 0.4247\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4047 - val_loss: 0.4309\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4201 - val_loss: 0.4458\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4002 - val_loss: 0.4167\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4177 - val_loss: 0.4694\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4204 - val_loss: 0.5031\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3981 - val_loss: 0.4245\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3913 - val_loss: 0.4094\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3834 - val_loss: 0.4330\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3691 - val_loss: 0.4085\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3688 - val_loss: 0.4065\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3666 - val_loss: 0.4149\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3594 - val_loss: 0.4112\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3720 - val_loss: 0.4463\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3712 - val_loss: 0.4101\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3494 - val_loss: 0.4134\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3643 - val_loss: 0.4090\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3659 - val_loss: 0.4067\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3466 - val_loss: 0.4200\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_09-21:54:38] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:54:55] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 851ms/step - loss: 0.3618 - val_loss: 0.4174\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3423 - val_loss: 0.4065\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3464 - val_loss: 0.4121\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3494 - val_loss: 0.4051\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3552 - val_loss: 0.4053\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3539 - val_loss: 0.4123\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3460 - val_loss: 0.4054\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3340 - val_loss: 0.4091\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3351 - val_loss: 0.4081\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3393 - val_loss: 0.4068\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3259 - val_loss: 0.4060\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3264 - val_loss: 0.4082\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_09-21:55:25] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:55:25] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:55:25] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 846ms/step - loss: 0.3504 - val_loss: 0.4052\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>533</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  533  17\n",
       "1  102  25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2958579881656805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1dyjdtbf) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 20648... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▇▇▇▇▇██▁▁▁▂▂▂▃▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▂▁▂▂▂▁▁▂▁▂▃▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>0.40511</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35044</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.40516</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">comic-cherry-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/1dyjdtbf\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/1dyjdtbf</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_215340-1dyjdtbf/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1dyjdtbf). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/2d1rawpv\" target=\"_blank\">legendary-dragon-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-21:56:35] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:56:35] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:56:35] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 575ms/step - loss: 0.7436 - val_loss: 0.6105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.6135 - val_loss: 0.5015\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5379 - val_loss: 0.7730\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5369 - val_loss: 0.5750\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4800 - val_loss: 0.5787\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4661 - val_loss: 0.5748\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4672 - val_loss: 0.5480\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4143 - val_loss: 0.4819\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4183 - val_loss: 0.4835\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4000 - val_loss: 0.4954\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3988 - val_loss: 0.4739\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3924 - val_loss: 0.4770\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3867 - val_loss: 0.4860\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3851 - val_loss: 0.4692\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3846 - val_loss: 0.4757\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3863 - val_loss: 0.4765\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3836 - val_loss: 0.4678\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3858 - val_loss: 0.4762\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3834 - val_loss: 0.4656\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3812 - val_loss: 0.4781\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3827 - val_loss: 0.4653\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3730 - val_loss: 0.4733\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3774 - val_loss: 0.4679\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3768 - val_loss: 0.4604\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3763 - val_loss: 0.4772\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3667 - val_loss: 0.4623\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3743 - val_loss: 0.4640\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3640 - val_loss: 0.4791\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3686 - val_loss: 0.4707\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3610 - val_loss: 0.4602\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3649 - val_loss: 0.4582\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3647 - val_loss: 0.4634\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3682 - val_loss: 0.4706\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3594 - val_loss: 0.4662\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3637 - val_loss: 0.4626\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3664 - val_loss: 0.4608\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3701 - val_loss: 0.4610\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3687 - val_loss: 0.4621\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3647 - val_loss: 0.4635\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_09-21:57:28] Training the entire fine-tuned model...\n",
      "[2022_04_09-21:58:15] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3678WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1010s vs `on_train_batch_end` time: 0.1299s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1010s vs `on_train_batch_end` time: 0.1299s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 699ms/step - loss: 0.3678 - val_loss: 0.4736\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3776 - val_loss: 0.4549\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3796 - val_loss: 0.4707\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3620 - val_loss: 0.4710\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3442 - val_loss: 0.4536\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3462 - val_loss: 0.4705\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3343 - val_loss: 0.4578\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3371 - val_loss: 0.4764\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3310 - val_loss: 0.4615\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3261 - val_loss: 0.4631\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3204 - val_loss: 0.4688\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3254 - val_loss: 0.4651\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3167 - val_loss: 0.4625\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_09-21:58:47] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-21:58:47] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:58:47] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3531WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0970s vs `on_train_batch_end` time: 0.1319s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0970s vs `on_train_batch_end` time: 0.1319s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 680ms/step - loss: 0.3531 - val_loss: 0.4561\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>471</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  471   8\n",
       "1  120  15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.189873417721519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2d1rawpv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 20955... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▆▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂</td></tr><tr><td>lr</td><td>█████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄▂█▄▄▃▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>4</td></tr><tr><td>best_val_loss</td><td>0.45364</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35306</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.45613</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">legendary-dragon-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/2d1rawpv\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/2d1rawpv</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_215619-2d1rawpv/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2d1rawpv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/2lpzuvhn\" target=\"_blank\">vague-bird-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-21:59:43] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:59:43] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-21:59:43] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 715ms/step - loss: 1.0048 - val_loss: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.6851 - val_loss: 0.6668\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.6216 - val_loss: 0.6983\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5650 - val_loss: 0.5106\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4905 - val_loss: 0.5529\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4713 - val_loss: 0.4540\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4569 - val_loss: 0.4994\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4380 - val_loss: 0.4440\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4331 - val_loss: 0.4825\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4307 - val_loss: 0.4273\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4148 - val_loss: 0.4387\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4010 - val_loss: 0.4186\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3992 - val_loss: 0.4508\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4051 - val_loss: 0.4127\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3997 - val_loss: 0.4171\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3833 - val_loss: 0.4282\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3804 - val_loss: 0.4135\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3774 - val_loss: 0.4210\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3808 - val_loss: 0.4092\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3725 - val_loss: 0.4088\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3725 - val_loss: 0.4168\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3685 - val_loss: 0.4126\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3699 - val_loss: 0.4082\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3639 - val_loss: 0.4172\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3675 - val_loss: 0.4123\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3537 - val_loss: 0.4083\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3693 - val_loss: 0.4145\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3641 - val_loss: 0.4103\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3611 - val_loss: 0.4089\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3592 - val_loss: 0.4085\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3621 - val_loss: 0.4097\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-22:00:26] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:00:34] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 834ms/step - loss: 0.3722 - val_loss: 0.4085\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3677 - val_loss: 0.4215\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3569 - val_loss: 0.4073\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3473 - val_loss: 0.4073\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3424 - val_loss: 0.4061\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3533 - val_loss: 0.4050\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3459 - val_loss: 0.4281\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3461 - val_loss: 0.4136\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3370 - val_loss: 0.4138\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3342 - val_loss: 0.4036\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3276 - val_loss: 0.4112\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3007 - val_loss: 0.4049\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3003 - val_loss: 0.4083\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.2962 - val_loss: 0.4162\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.2898 - val_loss: 0.4110\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.2845 - val_loss: 0.4112\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.2836 - val_loss: 0.4173\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.2729 - val_loss: 0.4144\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_09-22:01:15] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:01:15] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:01:15] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 857ms/step - loss: 0.3165 - val_loss: 0.4042\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>516</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  516  34\n",
       "1   98  29"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30526315789473685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2lpzuvhn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 21328... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇██▁▁▁▂▂▂▃▃▃▄▄▄▅▅▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▆▃▂▃▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>0.4036</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31649</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.40417</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">vague-bird-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/2lpzuvhn\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/2lpzuvhn</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_215926-2lpzuvhn/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2lpzuvhn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/363tn5v0\" target=\"_blank\">whole-capybara-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-22:02:10] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:02:10] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:02:10] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 556ms/step - loss: 0.8890 - val_loss: 0.7448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.6358 - val_loss: 0.6931\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4756 - val_loss: 0.4971\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4872 - val_loss: 0.4880\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4474 - val_loss: 0.5812\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4505 - val_loss: 0.4842\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4055 - val_loss: 0.4749\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3958 - val_loss: 0.4733\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3920 - val_loss: 0.4706\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3782 - val_loss: 0.4975\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3806 - val_loss: 0.4614\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3699 - val_loss: 0.5150\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3752 - val_loss: 0.4613\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3649 - val_loss: 0.4533\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3573 - val_loss: 0.4545\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3576 - val_loss: 0.4483\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3687 - val_loss: 0.4491\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3642 - val_loss: 0.5325\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3633 - val_loss: 0.4681\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3511 - val_loss: 0.4715\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3333 - val_loss: 0.4558\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3389 - val_loss: 0.4546\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3449 - val_loss: 0.4523\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3319 - val_loss: 0.4600\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_09-22:02:46] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:03:09] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4179WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0982s vs `on_train_batch_end` time: 0.1314s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0982s vs `on_train_batch_end` time: 0.1314s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 665ms/step - loss: 0.4179 - val_loss: 0.6196\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4215 - val_loss: 0.4492\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3790 - val_loss: 0.4522\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3539 - val_loss: 0.4638\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3297 - val_loss: 0.4584\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3180 - val_loss: 0.4500\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3060 - val_loss: 0.4581\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.2972 - val_loss: 0.4576\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.2847 - val_loss: 0.4608\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.2808 - val_loss: 0.4603\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_09-22:03:35] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:03:35] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:03:36] Validation set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3604WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1015s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1015s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 680ms/step - loss: 0.3604 - val_loss: 0.4498\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>469</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>117</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  469  10\n",
       "1  117  18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22085889570552147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_09_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:363tn5v0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 21695... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▂▂▂▁▁▁▁▁▂</td></tr><tr><td>lr</td><td>████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▂▂▄▂▂▂▂▂▁▃▁▁▁▁▁▃▁▂▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>15</td></tr><tr><td>best_val_loss</td><td>0.44835</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36039</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.44979</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">whole-capybara-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/363tn5v0\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/363tn5v0</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_220154-363tn5v0/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:363tn5v0). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/3mmrtvsw\" target=\"_blank\">youthful-snowflake-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-22:04:30] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:04:31] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:04:31] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 725ms/step - loss: 0.8374 - val_loss: 0.9910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.6716 - val_loss: 0.6121\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.6261 - val_loss: 0.5062\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.5445 - val_loss: 0.4511\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5057 - val_loss: 0.4363\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5043 - val_loss: 0.4394\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4584 - val_loss: 0.4361\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4670 - val_loss: 0.4560\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4343 - val_loss: 0.4340\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4319 - val_loss: 0.4511\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4091 - val_loss: 0.4178\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4021 - val_loss: 0.4162\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4003 - val_loss: 0.4162\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3881 - val_loss: 0.4241\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3856 - val_loss: 0.4129\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3957 - val_loss: 0.4174\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3823 - val_loss: 0.4146\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3811 - val_loss: 0.4136\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3710 - val_loss: 0.4101\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3597 - val_loss: 0.4131\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3566 - val_loss: 0.4136\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 228ms/step - loss: 0.3556 - val_loss: 0.4116\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3674 - val_loss: 0.4260\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3626 - val_loss: 0.4130\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3569 - val_loss: 0.4070\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3557 - val_loss: 0.4140\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3488 - val_loss: 0.4107\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3480 - val_loss: 0.4067\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3449 - val_loss: 0.4099\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3543 - val_loss: 0.4099\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3394 - val_loss: 0.4098\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3491 - val_loss: 0.4078\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3430 - val_loss: 0.4090\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3451 - val_loss: 0.4090\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3436 - val_loss: 0.4077\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3408 - val_loss: 0.4092\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-22:05:21] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:05:29] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 836ms/step - loss: 0.6316 - val_loss: 0.5893\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.4835 - val_loss: 0.5403\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.4348 - val_loss: 0.4644\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3941 - val_loss: 0.4644\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3799 - val_loss: 0.4086\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3693 - val_loss: 0.4069\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3466 - val_loss: 0.4059\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3395 - val_loss: 0.4019\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3315 - val_loss: 0.4089\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3357 - val_loss: 0.4000\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3188 - val_loss: 0.4106\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3122 - val_loss: 0.4019\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3100 - val_loss: 0.4295\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.2845 - val_loss: 0.4057\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.2851 - val_loss: 0.4083\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.2700 - val_loss: 0.4143\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.2694 - val_loss: 0.4080\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.2582 - val_loss: 0.4083\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_09-22:06:10] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:06:10] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:06:10] Validation set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 845ms/step - loss: 0.2992 - val_loss: 0.4030\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>513</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  513  37\n",
       "1   97  30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30927835051546393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_09_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3mmrtvsw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 21967... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇▇██▁▁▁▂▂▂▃▃▃▃▄▄▄▁</td></tr><tr><td>loss</td><td>█▆▅▄▄▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▅▄▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▃▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>0.39995</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.29922</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.40303</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">youthful-snowflake-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/3mmrtvsw\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/3mmrtvsw</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_220414-3mmrtvsw/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3mmrtvsw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/2k6wj6ft\" target=\"_blank\">good-totem-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-22:07:11] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:07:11] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:07:11] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 733ms/step - loss: 0.9593 - val_loss: 0.9806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.7330 - val_loss: 0.6684\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.7176 - val_loss: 0.7366\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.6169 - val_loss: 0.5019\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5701 - val_loss: 0.5527\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.5417 - val_loss: 0.4315\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4931 - val_loss: 0.4767\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4931 - val_loss: 0.4004\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4780 - val_loss: 0.4067\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4673 - val_loss: 0.3981\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4480 - val_loss: 0.3895\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4455 - val_loss: 0.4193\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4448 - val_loss: 0.3858\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4394 - val_loss: 0.3840\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4411 - val_loss: 0.4077\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4298 - val_loss: 0.3807\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4202 - val_loss: 0.3874\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4125 - val_loss: 0.3772\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4103 - val_loss: 0.3748\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4176 - val_loss: 0.3821\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4250 - val_loss: 0.4346\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4330 - val_loss: 0.3743\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3886 - val_loss: 0.3692\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4136 - val_loss: 0.3937\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3904 - val_loss: 0.3676\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3815 - val_loss: 0.4019\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3917 - val_loss: 0.3726\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3956 - val_loss: 0.3662\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3859 - val_loss: 0.3880\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3806 - val_loss: 0.3741\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3826 - val_loss: 0.3834\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-22:07:55] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:08:08] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 859ms/step - loss: 0.3898 - val_loss: 0.3654\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3800 - val_loss: 0.3646\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3681 - val_loss: 0.3655\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3756 - val_loss: 0.3652\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.3710 - val_loss: 0.3635\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3811 - val_loss: 0.3627\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.3829 - val_loss: 0.3649\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.3746 - val_loss: 0.3676\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3739 - val_loss: 0.3637\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_09-22:08:32] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:08:32] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:08:32] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 861ms/step - loss: 0.3617 - val_loss: 0.3627\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>537</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  537   7\n",
       "1   90  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3310344827586207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2k6wj6ft) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 22338... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▅▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████████████████▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▅▃▃▂▂▁▁▁▁▂▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.36273</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36166</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.36275</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">good-totem-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/2k6wj6ft\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/2k6wj6ft</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_220654-2k6wj6ft/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2k6wj6ft). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/2rbjm661\" target=\"_blank\">dark-firefly-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-22:09:28] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:09:28] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:09:28] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 589ms/step - loss: 0.8618 - val_loss: 0.5970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4916 - val_loss: 0.5653\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4283 - val_loss: 0.6461\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4237 - val_loss: 0.5923\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4440 - val_loss: 0.5847\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-22:09:44] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:09:56] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4240WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1191s vs `on_train_batch_end` time: 0.1232s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1191s vs `on_train_batch_end` time: 0.1232s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 727ms/step - loss: 0.4240 - val_loss: 0.5535\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4184 - val_loss: 0.5451\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4187 - val_loss: 0.5396\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4116 - val_loss: 0.5383\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4124 - val_loss: 0.5399\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4123 - val_loss: 0.5385\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4151 - val_loss: 0.5371\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4104 - val_loss: 0.5343\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4116 - val_loss: 0.5320\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4137 - val_loss: 0.5316\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4097 - val_loss: 0.5351\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4069 - val_loss: 0.5386\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4052 - val_loss: 0.5411\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_09-22:10:29] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:10:29] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:10:29] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4117WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0978s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0978s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 679ms/step - loss: 0.4117 - val_loss: 0.5316\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  485  0\n",
       "1  148  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2rbjm661) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 22649... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▁▂▂▃▃▄▅▅▆▆▇▇█▁</td></tr><tr><td>loss</td><td>█▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅▃█▅▄▂▂▁▁▂▁▁▁▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>0.53161</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.41166</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.53164</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dark-firefly-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/2rbjm661\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/2rbjm661</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_220912-2rbjm661/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2rbjm661). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/1rdi8qrb\" target=\"_blank\">stellar-haze-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-22:11:24] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:11:24] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:11:24] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 733ms/step - loss: 0.9701 - val_loss: 0.9027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.7210 - val_loss: 0.5614\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.6215 - val_loss: 0.5865\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5683 - val_loss: 0.4300\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.5107 - val_loss: 0.4415\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4959 - val_loss: 0.4084\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4864 - val_loss: 0.4017\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4684 - val_loss: 0.4292\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4812 - val_loss: 0.4093\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4830 - val_loss: 0.4069\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-22:11:46] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:12:02] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 845ms/step - loss: 0.4686 - val_loss: 0.4138\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.4593 - val_loss: 0.4007\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.4604 - val_loss: 0.4011\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.4528 - val_loss: 0.4019\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.4500 - val_loss: 0.3939\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.4439 - val_loss: 0.3957\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.4383 - val_loss: 0.3901\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.4310 - val_loss: 0.3908\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.4382 - val_loss: 0.3888\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.4280 - val_loss: 0.3833\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.4181 - val_loss: 0.3809\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.4115 - val_loss: 0.3824\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.4063 - val_loss: 0.3772\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.4052 - val_loss: 0.3749\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.4024 - val_loss: 0.3712\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3935 - val_loss: 0.3716\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3872 - val_loss: 0.3755\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3697 - val_loss: 0.3711\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3668 - val_loss: 0.3772\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.3418 - val_loss: 0.3695\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.3407 - val_loss: 0.3744\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3218 - val_loss: 0.3725\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.3152 - val_loss: 0.3793\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_09-22:12:52] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:12:52] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:12:52] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 853ms/step - loss: 0.3379 - val_loss: 0.3701\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>526</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  526  18\n",
       "1   77  37"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4378698224852071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1rdi8qrb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 22841... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▄▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>19</td></tr><tr><td>best_val_loss</td><td>0.36947</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33786</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.3701</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">stellar-haze-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/1rdi8qrb\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/1rdi8qrb</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_221108-1rdi8qrb/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1rdi8qrb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/2usq2zah\" target=\"_blank\">cool-paper-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-22:13:47] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:13:47] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:13:47] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 565ms/step - loss: 0.9727 - val_loss: 0.5614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5698 - val_loss: 0.8887\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5123 - val_loss: 0.5402\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4400 - val_loss: 0.6405\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4335 - val_loss: 0.5466\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4321 - val_loss: 0.5247\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4258 - val_loss: 0.6852\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4311 - val_loss: 0.5284\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3795 - val_loss: 0.5366\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-22:14:07] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:14:15] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3703WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1245s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1245s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 696ms/step - loss: 0.3703 - val_loss: 0.5442\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3667 - val_loss: 0.5071\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3675 - val_loss: 0.5227\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3667 - val_loss: 0.5192\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3481 - val_loss: 0.5000\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 3s 335ms/step - loss: 0.3589 - val_loss: 0.5071\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3556 - val_loss: 0.5346\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3473 - val_loss: 0.4971\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3384 - val_loss: 0.5153\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3421 - val_loss: 0.5252\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3260 - val_loss: 0.4969\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3209 - val_loss: 0.5062\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3195 - val_loss: 0.5040\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3149 - val_loss: 0.5016\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_09-22:14:51] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:14:51] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:14:51] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3261WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0958s vs `on_train_batch_end` time: 0.1269s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0958s vs `on_train_batch_end` time: 0.1269s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 702ms/step - loss: 0.3261 - val_loss: 0.4971\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>461</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  461  24\n",
       "1  125  23"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2358974358974359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2usq2zah) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 23114... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▄▅▅▁▂▂▃▃▄▄▅▅▆▆▇▇█▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂█▂▄▂▁▄▂▂▂▁▁▁▁▁▂▁▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_val_loss</td><td>0.49695</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32613</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.49709</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">cool-paper-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/2usq2zah\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/2usq2zah</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_221331-2usq2zah/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2usq2zah). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/27v3rpvb\" target=\"_blank\">tough-durian-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-22:15:44] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:15:44] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:15:44] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 732ms/step - loss: 0.8837 - val_loss: 1.1025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.7483 - val_loss: 0.5821\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.6710 - val_loss: 0.5706\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.5562 - val_loss: 0.4337\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5144 - val_loss: 0.4537\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.5291 - val_loss: 0.4080\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4890 - val_loss: 0.4025\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4763 - val_loss: 0.4289\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4654 - val_loss: 0.3962\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4521 - val_loss: 0.4044\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4402 - val_loss: 0.3902\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4258 - val_loss: 0.4098\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4433 - val_loss: 0.3873\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4363 - val_loss: 0.3814\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4218 - val_loss: 0.3880\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4197 - val_loss: 0.3781\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3976 - val_loss: 0.3917\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4065 - val_loss: 0.3755\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4116 - val_loss: 0.3873\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3982 - val_loss: 0.3730\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4044 - val_loss: 0.3729\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3929 - val_loss: 0.3719\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3885 - val_loss: 0.3718\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3886 - val_loss: 0.3794\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3857 - val_loss: 0.3681\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3762 - val_loss: 0.3664\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3724 - val_loss: 0.3757\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3780 - val_loss: 0.3627\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3732 - val_loss: 0.3630\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3719 - val_loss: 0.3614\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3763 - val_loss: 0.3947\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3771 - val_loss: 0.3847\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3956 - val_loss: 0.3612\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3884 - val_loss: 0.3997\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3910 - val_loss: 0.3702\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3815 - val_loss: 0.3663\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-22:16:33] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:17:24] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 839ms/step - loss: 0.3887 - val_loss: 0.3615\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3716 - val_loss: 0.3686\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3684 - val_loss: 0.3573\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3526 - val_loss: 0.3582\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3582 - val_loss: 0.3559\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3499 - val_loss: 0.3573\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.3492 - val_loss: 0.3530\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3343 - val_loss: 0.3608\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3274 - val_loss: 0.3523\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3329 - val_loss: 0.3669\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3470 - val_loss: 0.3553\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3204 - val_loss: 0.3554\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_09-22:17:54] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:17:54] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:18:11] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 832ms/step - loss: 0.3258 - val_loss: 0.3526\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>515</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  515  29\n",
       "1   73  41"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44565217391304346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:27v3rpvb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 23328... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▆▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████████████▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.35226</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32581</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.35256</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">tough-durian-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/27v3rpvb\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/27v3rpvb</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_221528-27v3rpvb/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:27v3rpvb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/g0w7m51f\" target=\"_blank\">comic-breeze-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-22:19:04] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:19:04] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:19:04] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 582ms/step - loss: 0.8720 - val_loss: 0.8884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.7383 - val_loss: 1.0240\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5438 - val_loss: 0.6527\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4869 - val_loss: 0.7630\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4528 - val_loss: 0.5720\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4177 - val_loss: 0.5973\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3871 - val_loss: 0.5160\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3939 - val_loss: 0.5146\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3589 - val_loss: 0.5153\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3501 - val_loss: 0.5185\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3649 - val_loss: 0.5423\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-22:19:28] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:19:42] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3662WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1251s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1251s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 679ms/step - loss: 0.3662 - val_loss: 0.5250\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3557 - val_loss: 0.5092\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3554 - val_loss: 0.5178\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3422 - val_loss: 0.5065\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3374 - val_loss: 0.5413\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3237 - val_loss: 0.5009\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3218 - val_loss: 0.5374\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3127 - val_loss: 0.5105\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.2964 - val_loss: 0.5194\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_09-22:20:07] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:20:07] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:20:07] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3285WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1221s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1221s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 682ms/step - loss: 0.3285 - val_loss: 0.5010\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>458</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>117</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  458  27\n",
       "1  117  31"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30097087378640774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:g0w7m51f) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 23670... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▅▅▆▇▇█▁▂▂▃▄▅▅▆▇▁</td></tr><tr><td>loss</td><td>█▆▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆█▃▅▂▂▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.50093</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32847</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.50096</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">comic-breeze-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/g0w7m51f\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/g0w7m51f</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_221848-g0w7m51f/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:g0w7m51f). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/28wdht3w\" target=\"_blank\">electric-aardvark-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-22:21:02] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:21:02] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:21:02] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 735ms/step - loss: 0.8889 - val_loss: 0.7060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.6489 - val_loss: 0.4384\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5662 - val_loss: 0.4518\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5575 - val_loss: 0.4977\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5059 - val_loss: 0.4160\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4864 - val_loss: 0.4293\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4739 - val_loss: 0.4001\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4767 - val_loss: 0.4080\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4792 - val_loss: 0.4882\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4871 - val_loss: 0.3877\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4535 - val_loss: 0.3851\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4356 - val_loss: 0.4216\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4355 - val_loss: 0.3845\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4213 - val_loss: 0.3954\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4238 - val_loss: 0.3862\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4088 - val_loss: 0.3773\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4070 - val_loss: 0.3752\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4039 - val_loss: 0.3889\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4097 - val_loss: 0.3726\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3957 - val_loss: 0.3757\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3932 - val_loss: 0.3866\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3957 - val_loss: 0.3699\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3902 - val_loss: 0.3954\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 230ms/step - loss: 0.3834 - val_loss: 0.3666\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 227ms/step - loss: 0.3773 - val_loss: 0.3670\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 226ms/step - loss: 0.3765 - val_loss: 0.3841\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3818 - val_loss: 0.3716\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-22:21:41] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:21:49] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 849ms/step - loss: 0.6906 - val_loss: 0.5345\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.5328 - val_loss: 0.4200\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.4771 - val_loss: 0.3741\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.4222 - val_loss: 0.3956\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.4104 - val_loss: 0.3726\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.4082 - val_loss: 0.4072\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.3842 - val_loss: 0.3669\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3532 - val_loss: 0.3807\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3701 - val_loss: 0.3599\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3460 - val_loss: 0.3667\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3480 - val_loss: 0.3724\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3178 - val_loss: 0.3740\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_09-22:22:20] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:22:20] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:22:20] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 839ms/step - loss: 0.3656 - val_loss: 0.3566\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  536   8\n",
       "1   90  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3287671232876712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:28wdht3w) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 23877... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▆▄▃▂▂▂▂▁▂▁▁▁▂</td></tr><tr><td>lr</td><td>███████████████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▄▂▂▂▂▄▂▂▂▂▂▂▁▁▂▁▁▂▁▂▁▁▂▁▅▂▁▂▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.35659</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36558</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.35659</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">electric-aardvark-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/28wdht3w\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/28wdht3w</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_222046-28wdht3w/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:28wdht3w). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/15x3e1cz\" target=\"_blank\">giddy-totem-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-22:23:16] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:23:16] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:23:16] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 570ms/step - loss: 0.8803 - val_loss: 0.5872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.5272 - val_loss: 0.5977\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4470 - val_loss: 0.5972\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4106 - val_loss: 0.5255\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4150 - val_loss: 0.5229\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3704 - val_loss: 0.5154\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3626 - val_loss: 0.5528\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3717 - val_loss: 0.5126\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3440 - val_loss: 0.5888\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3608 - val_loss: 0.5220\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3677 - val_loss: 0.5073\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3476 - val_loss: 0.5009\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3427 - val_loss: 0.5112\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3305 - val_loss: 0.5027\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3555 - val_loss: 0.5108\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-22:23:42] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:23:52] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3870WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0974s vs `on_train_batch_end` time: 0.1267s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0974s vs `on_train_batch_end` time: 0.1267s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 680ms/step - loss: 0.3870 - val_loss: 0.6064\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3655 - val_loss: 0.5169\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3681 - val_loss: 0.4808\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3238 - val_loss: 0.5071\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3258 - val_loss: 0.5298\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3175 - val_loss: 0.4966\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_09-22:24:14] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:24:14] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:24:14] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3182WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1257s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1257s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 695ms/step - loss: 0.3182 - val_loss: 0.4928\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>481</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  481   4\n",
       "1  137  11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13496932515337423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:15x3e1cz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 24171... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█▁▁▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▁▂▂▁▁▁▁▂▂▂▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇█▇▃▃▃▅▃▇▃▂▂▃▂▃█▃▁▂▄▂▂</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_val_loss</td><td>0.48085</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31824</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.49282</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">giddy-totem-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/15x3e1cz\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/15x3e1cz</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_222300-15x3e1cz/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:15x3e1cz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/2mhh7cp2\" target=\"_blank\">dandy-breeze-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-22:25:06] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:25:06] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:25:06] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 738ms/step - loss: 0.9147 - val_loss: 0.8273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.6954 - val_loss: 0.4833\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.6186 - val_loss: 0.4305\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.5355 - val_loss: 0.4519\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.5057 - val_loss: 0.4207\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4905 - val_loss: 0.4570\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4757 - val_loss: 0.4090\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4754 - val_loss: 0.4174\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4702 - val_loss: 0.4104\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4426 - val_loss: 0.4032\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4616 - val_loss: 0.4840\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4644 - val_loss: 0.4170\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4714 - val_loss: 0.3863\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4237 - val_loss: 0.3935\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4174 - val_loss: 0.3783\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4209 - val_loss: 0.3775\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4091 - val_loss: 0.3922\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4093 - val_loss: 0.3749\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4071 - val_loss: 0.3762\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4097 - val_loss: 0.3729\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3956 - val_loss: 0.3800\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3908 - val_loss: 0.3743\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4023 - val_loss: 0.3826\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3871 - val_loss: 0.3679\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3900 - val_loss: 0.3707\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3872 - val_loss: 0.3721\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3749 - val_loss: 0.3692\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3839 - val_loss: 0.3669\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 227ms/step - loss: 0.3976 - val_loss: 0.3671\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 229ms/step - loss: 0.3824 - val_loss: 0.3697\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3785 - val_loss: 0.3716\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3850 - val_loss: 0.3715\n",
      "[2022_04_09-22:25:51] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:26:00] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 856ms/step - loss: 0.3860 - val_loss: 0.3707\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3755 - val_loss: 0.3735\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3817 - val_loss: 0.3674\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.3936 - val_loss: 0.3663\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3823 - val_loss: 0.3678\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3801 - val_loss: 0.3672\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3735 - val_loss: 0.3661\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3760 - val_loss: 0.3675\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.3728 - val_loss: 0.3662\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3663 - val_loss: 0.3666\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3665 - val_loss: 0.3663\n",
      "[2022_04_09-22:26:29] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:26:29] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:26:29] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 11s 1s/step - loss: 0.3752 - val_loss: 0.3660\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  536   8\n",
       "1   91  23"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31724137931034485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2mhh7cp2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 24377... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.36597</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37519</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.36597</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dandy-breeze-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/2mhh7cp2\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/2mhh7cp2</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_222451-2mhh7cp2/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2mhh7cp2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/1b8fnzjy\" target=\"_blank\">soft-planet-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-22:27:42] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:27:42] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:27:42] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 536ms/step - loss: 0.8211 - val_loss: 0.6985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5722 - val_loss: 0.7918\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4781 - val_loss: 0.5495\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4041 - val_loss: 0.5969\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3898 - val_loss: 0.5470\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4100 - val_loss: 0.5200\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3845 - val_loss: 0.5562\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3603 - val_loss: 0.5247\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3609 - val_loss: 0.5162\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3418 - val_loss: 0.5025\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3479 - val_loss: 0.5103\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3602 - val_loss: 0.5838\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3457 - val_loss: 0.5213\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3542 - val_loss: 0.5320\n",
      "[2022_04_09-22:28:08] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:28:16] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3473WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0993s vs `on_train_batch_end` time: 0.1245s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0993s vs `on_train_batch_end` time: 0.1245s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 678ms/step - loss: 0.3473 - val_loss: 0.5100\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3384 - val_loss: 0.5127\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3487 - val_loss: 0.5086\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3461 - val_loss: 0.5045\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3328 - val_loss: 0.5058\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3446 - val_loss: 0.5065\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3431 - val_loss: 0.5041\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 3s 316ms/step - loss: 0.3301 - val_loss: 0.5130\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3318 - val_loss: 0.5126\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3294 - val_loss: 0.5055\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3352 - val_loss: 0.5054\n",
      "[2022_04_09-22:28:46] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:28:46] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:29:03] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3357WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1001s vs `on_train_batch_end` time: 0.1242s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1001s vs `on_train_batch_end` time: 0.1242s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 686ms/step - loss: 0.3357 - val_loss: 0.5044\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>477</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  477   8\n",
       "1  130  18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20689655172413793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1b8fnzjy) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 24696... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▄▅▅▆▆▇▇█▁▂▂▃▃▄▄▅▅▆▆▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆█▂▃▂▁▂▂▁▁▁▃▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>0.50252</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33575</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.50438</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">soft-planet-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/1b8fnzjy\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/1b8fnzjy</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_222725-1b8fnzjy/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1b8fnzjy). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/1bunpn3d\" target=\"_blank\">devout-plasma-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-22:29:57] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:29:57] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:29:57] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 773ms/step - loss: 0.9617 - val_loss: 1.0383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.7654 - val_loss: 0.5844\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.6018 - val_loss: 0.6556\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.5547 - val_loss: 0.4738\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.5459 - val_loss: 0.4929\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.5299 - val_loss: 0.4086\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4875 - val_loss: 0.4025\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4770 - val_loss: 0.4146\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4586 - val_loss: 0.3987\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4503 - val_loss: 0.3958\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4335 - val_loss: 0.3895\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4369 - val_loss: 0.3922\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4365 - val_loss: 0.3898\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4327 - val_loss: 0.4085\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4245 - val_loss: 0.3834\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4304 - val_loss: 0.3828\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4150 - val_loss: 0.3935\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4174 - val_loss: 0.3823\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4147 - val_loss: 0.3836\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4197 - val_loss: 0.3832\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4217 - val_loss: 0.3816\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4142 - val_loss: 0.3849\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4118 - val_loss: 0.3784\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4136 - val_loss: 0.3901\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4123 - val_loss: 0.3783\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4151 - val_loss: 0.3784\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4103 - val_loss: 0.3792\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4089 - val_loss: 0.3810\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4090 - val_loss: 0.3815\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-22:30:39] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:30:56] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 868ms/step - loss: 0.4078 - val_loss: 0.3771\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.4010 - val_loss: 0.3835\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.4037 - val_loss: 0.3731\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.4107 - val_loss: 0.3729\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3890 - val_loss: 0.3848\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3867 - val_loss: 0.3701\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3886 - val_loss: 0.3800\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3815 - val_loss: 0.3695\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3754 - val_loss: 0.3660\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3718 - val_loss: 0.3788\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3557 - val_loss: 0.3644\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3663 - val_loss: 0.3765\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.3535 - val_loss: 0.3610\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3538 - val_loss: 0.3629\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3529 - val_loss: 0.3613\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3288 - val_loss: 0.3610\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.3264 - val_loss: 0.3587\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3247 - val_loss: 0.3588\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3292 - val_loss: 0.3601\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3297 - val_loss: 0.3613\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3316 - val_loss: 0.3620\n",
      "[2022_04_09-22:31:43] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:31:43] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:31:43] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 850ms/step - loss: 0.3202 - val_loss: 0.3587\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>523</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  523  21\n",
       "1   79  35"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4117647058823529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1bunpn3d) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 24936... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▇▇▇▇█▁▁▂▂▂▃▃▃▃▄▄▅▅▅▅▆▁</td></tr><tr><td>loss</td><td>█▆▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▄▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.35866</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32019</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.35866</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">devout-plasma-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/1bunpn3d\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/1bunpn3d</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_222941-1bunpn3d/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1bunpn3d). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/26cq1qjg\" target=\"_blank\">comfy-planet-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-22:32:38] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:32:38] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:32:38] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 893ms/step - loss: 0.8563 - val_loss: 0.5774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4874 - val_loss: 0.5369\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4177 - val_loss: 0.5202\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3949 - val_loss: 0.5585\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3786 - val_loss: 0.5264\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3818 - val_loss: 0.5356\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3882 - val_loss: 0.5500\n",
      "[2022_04_09-22:32:57] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:33:06] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4044WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1239s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1239s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 704ms/step - loss: 0.4044 - val_loss: 0.5315\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4029 - val_loss: 0.5159\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3878 - val_loss: 0.5426\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3825 - val_loss: 0.5236\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3805 - val_loss: 0.5120\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3699 - val_loss: 0.5253\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3657 - val_loss: 0.5138\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3567 - val_loss: 0.5164\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3527 - val_loss: 0.5182\n",
      "[2022_04_09-22:33:31] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:33:31] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:33:31] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3717WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0972s vs `on_train_batch_end` time: 0.1269s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0972s vs `on_train_batch_end` time: 0.1269s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 669ms/step - loss: 0.3717 - val_loss: 0.5114\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>483</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>147</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  483  2\n",
       "1  147  1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013245033112582781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:26cq1qjg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 25295... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▄▅▅▆▁▂▃▄▅▅▆▇█▁</td></tr><tr><td>loss</td><td>█▃▂▂▁▁▁▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▆▃▄▅▃▁▄▂▁▂▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.51136</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3717</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.51136</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">comfy-planet-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/26cq1qjg\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/26cq1qjg</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_223221-26cq1qjg/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:26cq1qjg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/2bupipe3\" target=\"_blank\">radiant-dream-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-22:34:23] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:34:23] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:34:23] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 699ms/step - loss: 1.0631 - val_loss: 0.5197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.7355 - val_loss: 0.5806\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.6794 - val_loss: 0.5867\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5976 - val_loss: 0.4461\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5281 - val_loss: 0.5243\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5127 - val_loss: 0.4192\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5118 - val_loss: 0.4297\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4885 - val_loss: 0.4063\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4684 - val_loss: 0.4016\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4905 - val_loss: 0.4637\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4827 - val_loss: 0.3937\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4429 - val_loss: 0.4008\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4367 - val_loss: 0.3925\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4276 - val_loss: 0.3873\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4253 - val_loss: 0.3851\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4177 - val_loss: 0.4085\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4271 - val_loss: 0.3842\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4207 - val_loss: 0.3875\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4066 - val_loss: 0.3762\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4012 - val_loss: 0.3774\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4067 - val_loss: 0.3732\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3966 - val_loss: 0.3724\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3864 - val_loss: 0.3821\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4104 - val_loss: 0.3765\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4026 - val_loss: 0.4171\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4058 - val_loss: 0.3676\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4032 - val_loss: 0.3685\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3960 - val_loss: 0.3992\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3901 - val_loss: 0.3669\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3962 - val_loss: 0.3667\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3934 - val_loss: 0.3874\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3952 - val_loss: 0.3687\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3930 - val_loss: 0.3665\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3796 - val_loss: 0.3785\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3755 - val_loss: 0.3664\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3901 - val_loss: 0.3679\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3694 - val_loss: 0.3683\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3724 - val_loss: 0.3671\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3734 - val_loss: 0.3670\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-22:35:17] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:36:03] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 1s/step - loss: 0.3963 - val_loss: 0.3747\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3880 - val_loss: 0.3611\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3741 - val_loss: 0.3590\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3634 - val_loss: 0.3795\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3468 - val_loss: 0.3581\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3442 - val_loss: 0.3601\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 389ms/step - loss: 0.3393 - val_loss: 0.3722\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3380 - val_loss: 0.3557\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3268 - val_loss: 0.3569\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.3125 - val_loss: 0.3610\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3036 - val_loss: 0.3537\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.2854 - val_loss: 0.3571\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.2796 - val_loss: 0.3606\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.2785 - val_loss: 0.3673\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 393ms/step - loss: 0.2519 - val_loss: 0.3687\n",
      "[2022_04_09-22:36:39] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:36:39] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:36:50] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 863ms/step - loss: 0.2900 - val_loss: 0.3581\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>508</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  508  36\n",
       "1   70  44"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4536082474226804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2bupipe3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 25475... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▅▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆██▆▃▃▂▄▂▂▂▃▂▂▂▂▂▂▃▁▂▁▂▁▂▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_val_loss</td><td>0.35373</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.29002</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.35809</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">radiant-dream-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/2bupipe3\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/2bupipe3</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_223407-2bupipe3/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2bupipe3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/2mpbvlvl\" target=\"_blank\">resilient-yogurt-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-22:37:45] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:37:45] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:37:45] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 571ms/step - loss: 0.9119 - val_loss: 0.5810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5652 - val_loss: 0.8968\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5267 - val_loss: 0.5685\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4638 - val_loss: 0.5658\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4080 - val_loss: 0.5349\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3898 - val_loss: 0.5142\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3732 - val_loss: 0.5278\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3600 - val_loss: 0.5246\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3821 - val_loss: 0.5570\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3523 - val_loss: 0.5054\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3543 - val_loss: 0.5064\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3414 - val_loss: 0.5195\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3480 - val_loss: 0.5114\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3446 - val_loss: 0.5114\n",
      "[2022_04_09-22:38:10] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:38:23] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3621WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1226s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1226s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 686ms/step - loss: 0.3621 - val_loss: 0.5066\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3420 - val_loss: 0.5347\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3435 - val_loss: 0.4970\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3382 - val_loss: 0.5020\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3262 - val_loss: 0.5052\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3245 - val_loss: 0.4967\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3485 - val_loss: 0.4935\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3171 - val_loss: 0.5233\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3138 - val_loss: 0.4910\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.2985 - val_loss: 0.5096\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3036 - val_loss: 0.4937\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.2929 - val_loss: 0.5230\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.2863 - val_loss: 0.5070\n",
      "[2022_04_09-22:38:56] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:38:56] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:38:56] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3031WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0977s vs `on_train_batch_end` time: 0.1245s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0977s vs `on_train_batch_end` time: 0.1245s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 674ms/step - loss: 0.3031 - val_loss: 0.4917\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>464</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>122</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  464  21\n",
       "1  122  26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2mpbvlvl) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 25847... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▄▅▅▆▆▇▇█▁▂▂▃▃▄▄▅▅▆▆▇▇▁</td></tr><tr><td>loss</td><td>█▄▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▂▂▂▁▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.49098</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30307</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.49172</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">resilient-yogurt-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/2mpbvlvl\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/2mpbvlvl</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_223728-2mpbvlvl/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2mpbvlvl). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/26nnesrx\" target=\"_blank\">devout-surf-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-22:39:51] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:39:51] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:39:51] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 1s/step - loss: 0.9140 - val_loss: 1.1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.8215 - val_loss: 0.5534\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.5944 - val_loss: 0.6057\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5467 - val_loss: 0.4367\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5201 - val_loss: 0.4504\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4849 - val_loss: 0.4082\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4656 - val_loss: 0.4220\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4696 - val_loss: 0.3996\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4668 - val_loss: 0.4116\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4571 - val_loss: 0.4006\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4472 - val_loss: 0.3916\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4435 - val_loss: 0.4186\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4401 - val_loss: 0.3892\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4486 - val_loss: 0.3822\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4299 - val_loss: 0.3840\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 227ms/step - loss: 0.4153 - val_loss: 0.3844\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.4127 - val_loss: 0.3786\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4069 - val_loss: 0.3921\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4099 - val_loss: 0.3755\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4102 - val_loss: 0.3745\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4038 - val_loss: 0.3802\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4154 - val_loss: 0.3893\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4035 - val_loss: 0.3792\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3926 - val_loss: 0.3723\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3736 - val_loss: 0.3683\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3929 - val_loss: 0.3771\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3941 - val_loss: 0.3698\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3800 - val_loss: 0.3683\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3838 - val_loss: 0.3690\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3870 - val_loss: 0.3718\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3815 - val_loss: 0.3730\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3844 - val_loss: 0.3720\n",
      "[2022_04_09-22:40:37] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:40:45] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 853ms/step - loss: 0.5560 - val_loss: 0.5514\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.4975 - val_loss: 0.5274\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.4453 - val_loss: 0.4030\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.4370 - val_loss: 0.4152\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.4152 - val_loss: 0.3734\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3840 - val_loss: 0.3826\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3788 - val_loss: 0.3583\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 391ms/step - loss: 0.3609 - val_loss: 0.3564\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3444 - val_loss: 0.3587\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3324 - val_loss: 0.3519\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3239 - val_loss: 0.3522\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.3150 - val_loss: 0.3580\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3038 - val_loss: 0.3771\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 389ms/step - loss: 0.2882 - val_loss: 0.3567\n",
      "[2022_04_09-22:41:19] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:41:19] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:41:19] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 11s 861ms/step - loss: 0.3254 - val_loss: 0.3516\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>524</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  524  20\n",
       "1   80  34"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40476190476190477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:26nnesrx) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 26116... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▇▇▇███▁▁▂▂▂▂▃▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▇▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.3516</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32545</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.3516</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">devout-surf-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/26nnesrx\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/26nnesrx</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_223934-26nnesrx/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:26nnesrx). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/2lygf0yb\" target=\"_blank\">ethereal-eon-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-22:42:26] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:42:26] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:42:26] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 602ms/step - loss: 0.9222 - val_loss: 0.5986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.5685 - val_loss: 0.6719\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4625 - val_loss: 0.5687\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4345 - val_loss: 0.5177\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4058 - val_loss: 0.6131\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4181 - val_loss: 0.5163\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3595 - val_loss: 0.5870\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3800 - val_loss: 0.5134\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3666 - val_loss: 0.5112\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3563 - val_loss: 0.5436\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3610 - val_loss: 0.5273\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3663 - val_loss: 0.5377\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3628 - val_loss: 0.5227\n",
      "[2022_04_09-22:42:50] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:43:02] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4615WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0988s vs `on_train_batch_end` time: 0.1235s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0988s vs `on_train_batch_end` time: 0.1235s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 663ms/step - loss: 0.4615 - val_loss: 0.5527\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3962 - val_loss: 0.4941\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3765 - val_loss: 0.5244\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3458 - val_loss: 0.4922\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3363 - val_loss: 0.5339\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3384 - val_loss: 0.4975\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3237 - val_loss: 0.5212\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3032 - val_loss: 0.4982\n",
      "[2022_04_09-22:43:25] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:43:25] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:43:25] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3466WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0948s vs `on_train_batch_end` time: 0.1266s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0948s vs `on_train_batch_end` time: 0.1266s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 663ms/step - loss: 0.3466 - val_loss: 0.4923\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>476</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  476   9\n",
       "1  137  11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13095238095238096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2lygf0yb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 26466... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█▁▂▂▃▃▄▅▅▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▂▃▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅█▄▂▆▂▅▂▂▃▂▃▂▃▁▂▁▃▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>0.49219</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34657</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.49233</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">ethereal-eon-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/2lygf0yb\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/2lygf0yb</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_224210-2lygf0yb/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2lygf0yb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/3v43fr0x\" target=\"_blank\">vocal-wood-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-22:44:23] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:44:23] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:44:23] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 722ms/step - loss: 0.8974 - val_loss: 1.2427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.8434 - val_loss: 0.6753\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.6881 - val_loss: 0.7376\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.6552 - val_loss: 0.4820\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.6007 - val_loss: 0.4774\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.5008 - val_loss: 0.4176\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5088 - val_loss: 0.4194\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4787 - val_loss: 0.3995\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4604 - val_loss: 0.4082\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4532 - val_loss: 0.3949\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4437 - val_loss: 0.3934\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4340 - val_loss: 0.3916\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4347 - val_loss: 0.3870\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4359 - val_loss: 0.3917\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4386 - val_loss: 0.4076\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4292 - val_loss: 0.3817\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4217 - val_loss: 0.3809\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4159 - val_loss: 0.3918\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4082 - val_loss: 0.3796\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3979 - val_loss: 0.3778\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4025 - val_loss: 0.3735\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3914 - val_loss: 0.3806\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4035 - val_loss: 0.3732\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3956 - val_loss: 0.3736\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3845 - val_loss: 0.3731\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3985 - val_loss: 0.3664\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3918 - val_loss: 0.3660\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3857 - val_loss: 0.4123\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3968 - val_loss: 0.3652\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3851 - val_loss: 0.3636\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3704 - val_loss: 0.4189\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4034 - val_loss: 0.3622\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3791 - val_loss: 0.3738\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3770 - val_loss: 0.3775\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3638 - val_loss: 0.3640\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3617 - val_loss: 0.3689\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3606 - val_loss: 0.3719\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3623 - val_loss: 0.3613\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3647 - val_loss: 0.3630\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3748 - val_loss: 0.3686\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3560 - val_loss: 0.3617\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3624 - val_loss: 0.3615\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3668 - val_loss: 0.3639\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3707 - val_loss: 0.3641\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-22:45:21] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:45:29] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 852ms/step - loss: 0.3628 - val_loss: 0.3648\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3660 - val_loss: 0.3670\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3524 - val_loss: 0.3656\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3597 - val_loss: 0.3626\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3558 - val_loss: 0.3615\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3611 - val_loss: 0.3604\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3562 - val_loss: 0.3610\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3647 - val_loss: 0.3624\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3577 - val_loss: 0.3634\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3618 - val_loss: 0.3624\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3380 - val_loss: 0.3619\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3554 - val_loss: 0.3611\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_09-22:45:59] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:45:59] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:45:59] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 832ms/step - loss: 0.3583 - val_loss: 0.3603\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>532</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  532  12\n",
       "1   84  30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38461538461538464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3v43fr0x) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 26676... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▇▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.36033</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35832</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.36033</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">vocal-wood-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/3v43fr0x\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/3v43fr0x</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_224407-3v43fr0x/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3v43fr0x). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/b0ek8ade\" target=\"_blank\">fluent-armadillo-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-22:47:11] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:47:11] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:47:11] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 580ms/step - loss: 1.0096 - val_loss: 0.5388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.6727 - val_loss: 1.0582\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.7552 - val_loss: 0.5482\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5024 - val_loss: 0.6248\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4432 - val_loss: 0.5907\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4068 - val_loss: 0.5286\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4060 - val_loss: 0.5295\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3947 - val_loss: 0.5705\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3964 - val_loss: 0.5370\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3830 - val_loss: 0.5247\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3810 - val_loss: 0.5208\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3858 - val_loss: 0.5246\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3819 - val_loss: 0.5304\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3804 - val_loss: 0.5329\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3764 - val_loss: 0.5326\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3746 - val_loss: 0.5312\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3785 - val_loss: 0.5293\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_09-22:47:40] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:47:48] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3862WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0983s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0983s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 672ms/step - loss: 0.3862 - val_loss: 0.5257\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3807 - val_loss: 0.5315\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3722 - val_loss: 0.5287\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3776 - val_loss: 0.5240\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3790 - val_loss: 0.5231\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3725 - val_loss: 0.5277\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3748 - val_loss: 0.5278\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3708 - val_loss: 0.5275\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3723 - val_loss: 0.5284\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3739 - val_loss: 0.5287\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3671 - val_loss: 0.5273\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_09-22:48:17] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:48:17] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:48:17] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3741WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1228s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1228s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 683ms/step - loss: 0.3741 - val_loss: 0.5229\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>483</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  483  2\n",
       "1  144  4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05194805194805195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:b0ek8ade) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 27070... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██▁▁▂▂▃▃▄▄▅▅▅▁</td></tr><tr><td>loss</td><td>█▄▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁█▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_val_loss</td><td>0.5208</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37409</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.52291</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fluent-armadillo-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/b0ek8ade\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/b0ek8ade</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_224655-b0ek8ade/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:b0ek8ade). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/1n6bxz1i\" target=\"_blank\">pleasant-water-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-22:49:11] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:49:11] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:49:11] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 734ms/step - loss: 1.0339 - val_loss: 0.7894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.6951 - val_loss: 0.5356\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.6197 - val_loss: 0.5306\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5438 - val_loss: 0.4307\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5177 - val_loss: 0.4277\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4877 - val_loss: 0.4114\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4810 - val_loss: 0.4038\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4598 - val_loss: 0.4572\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4685 - val_loss: 0.4137\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4690 - val_loss: 0.4387\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4491 - val_loss: 0.3949\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4410 - val_loss: 0.3936\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4367 - val_loss: 0.4033\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4382 - val_loss: 0.3969\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4314 - val_loss: 0.3942\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4258 - val_loss: 0.3943\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4362 - val_loss: 0.3968\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4372 - val_loss: 0.3959\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-22:49:41] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:50:08] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 832ms/step - loss: 0.4335 - val_loss: 0.4005\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.4353 - val_loss: 0.3982\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.4334 - val_loss: 0.3936\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.4354 - val_loss: 0.3982\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.4265 - val_loss: 0.3883\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.4286 - val_loss: 0.3882\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.4194 - val_loss: 0.3944\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.4210 - val_loss: 0.3853\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.4150 - val_loss: 0.3855\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.4156 - val_loss: 0.3836\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 389ms/step - loss: 0.4038 - val_loss: 0.3861\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.4014 - val_loss: 0.3778\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3979 - val_loss: 0.3934\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3966 - val_loss: 0.3740\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3952 - val_loss: 0.3832\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3780 - val_loss: 0.3715\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 389ms/step - loss: 0.3758 - val_loss: 0.3726\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.3582 - val_loss: 0.3701\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3677 - val_loss: 0.3686\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.3549 - val_loss: 0.3746\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3503 - val_loss: 0.3658\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 2s 391ms/step - loss: 0.3385 - val_loss: 0.3687\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 3s 782ms/step - loss: 0.3424 - val_loss: 0.3681\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3242 - val_loss: 0.3698\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.3279 - val_loss: 0.3708\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 2s 392ms/step - loss: 0.3186 - val_loss: 0.3746\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3126 - val_loss: 0.3725\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_09-22:51:07] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:51:07] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:51:07] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 841ms/step - loss: 0.3453 - val_loss: 0.3657\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>535</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  535   9\n",
       "1   92  22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30344827586206896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1n6bxz1i) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 27323... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▆▁▁▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▄▂▂▂▂▂▂▁▁▂▂▁▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.36567</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34534</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.36567</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">pleasant-water-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/1n6bxz1i\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/1n6bxz1i</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_224856-1n6bxz1i/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1n6bxz1i). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/2o7e4x7q\" target=\"_blank\">efficient-plant-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-22:52:00] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:52:00] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:52:00] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 586ms/step - loss: 0.9605 - val_loss: 0.5580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.5861 - val_loss: 0.9771\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5779 - val_loss: 0.6623\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4884 - val_loss: 0.7199\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4639 - val_loss: 0.5381\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3933 - val_loss: 0.5286\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3862 - val_loss: 0.5512\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3923 - val_loss: 0.5569\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3799 - val_loss: 0.5224\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3809 - val_loss: 0.5231\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3731 - val_loss: 0.5238\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3734 - val_loss: 0.5257\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3704 - val_loss: 0.5208\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3701 - val_loss: 0.5232\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3668 - val_loss: 0.5264\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3695 - val_loss: 0.5252\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3700 - val_loss: 0.5245\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3669 - val_loss: 0.5239\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3652 - val_loss: 0.5235\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_09-22:52:31] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:52:39] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3718WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0945s vs `on_train_batch_end` time: 0.1273s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0945s vs `on_train_batch_end` time: 0.1273s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 668ms/step - loss: 0.3718 - val_loss: 0.5215\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3611 - val_loss: 0.5253\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3615 - val_loss: 0.5153\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3516 - val_loss: 0.5254\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3577 - val_loss: 0.5173\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3533 - val_loss: 0.5119\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3464 - val_loss: 0.5263\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3491 - val_loss: 0.5218\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3361 - val_loss: 0.5067\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3305 - val_loss: 0.5253\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3334 - val_loss: 0.5105\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3266 - val_loss: 0.5074\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3161 - val_loss: 0.5132\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3170 - val_loss: 0.5236\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3185 - val_loss: 0.5122\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_09-22:53:17] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:53:17] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:53:17] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3384WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1253s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1253s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 694ms/step - loss: 0.3384 - val_loss: 0.5070\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>454</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  454  31\n",
       "1  116  32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3033175355450237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2o7e4x7q) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 27656... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂█▃▄▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.50667</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33844</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.50697</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">efficient-plant-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/2o7e4x7q\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/2o7e4x7q</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_225145-2o7e4x7q/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2o7e4x7q). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/1mjlxazz\" target=\"_blank\">icy-jazz-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-22:54:12] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:54:12] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:54:12] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 722ms/step - loss: 0.8823 - val_loss: 0.7653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.6657 - val_loss: 0.4531\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5660 - val_loss: 0.4365\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.5516 - val_loss: 0.4993\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5081 - val_loss: 0.4191\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4975 - val_loss: 0.4330\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4849 - val_loss: 0.4135\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4706 - val_loss: 0.3961\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4542 - val_loss: 0.4417\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4606 - val_loss: 0.3980\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4383 - val_loss: 0.3877\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4306 - val_loss: 0.3889\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4163 - val_loss: 0.4062\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4402 - val_loss: 0.3802\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4177 - val_loss: 0.3876\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4090 - val_loss: 0.3750\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3992 - val_loss: 0.3842\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4155 - val_loss: 0.3774\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4106 - val_loss: 0.3950\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4354 - val_loss: 0.3819\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4077 - val_loss: 0.3808\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3962 - val_loss: 0.3697\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3944 - val_loss: 0.3742\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4004 - val_loss: 0.3768\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3895 - val_loss: 0.3681\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3943 - val_loss: 0.3737\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3998 - val_loss: 0.3716\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3932 - val_loss: 0.3694\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3865 - val_loss: 0.3685\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3893 - val_loss: 0.3693\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3923 - val_loss: 0.3736\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-22:54:55] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:55:10] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 842ms/step - loss: 0.3966 - val_loss: 0.3682\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3943 - val_loss: 0.3676\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3939 - val_loss: 0.3700\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3657 - val_loss: 0.3622\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3565 - val_loss: 0.3669\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3666 - val_loss: 0.3605\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3618 - val_loss: 0.3606\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3709 - val_loss: 0.3706\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3479 - val_loss: 0.3585\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3425 - val_loss: 0.3604\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3214 - val_loss: 0.3820\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3208 - val_loss: 0.3670\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3330 - val_loss: 0.3613\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3227 - val_loss: 0.3709\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.3155 - val_loss: 0.3600\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_09-22:55:46] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:55:46] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:55:46] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 845ms/step - loss: 0.3271 - val_loss: 0.3582\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>528</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  528  16\n",
       "1   82  32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3950617283950617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1mjlxazz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 27927... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇███▁▁▂▂▂▂▃▃▃▄▄▄▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.35824</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32711</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.35824</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">icy-jazz-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/1mjlxazz\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/1mjlxazz</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_225355-1mjlxazz/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1mjlxazz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/24kp1aec\" target=\"_blank\">splendid-glitter-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-22:56:41] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:56:41] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:56:41] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 577ms/step - loss: 0.9658 - val_loss: 0.5966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.6429 - val_loss: 1.0432\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.6131 - val_loss: 0.6423\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.5249 - val_loss: 0.7146\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4482 - val_loss: 0.5367\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 175ms/step - loss: 0.3965 - val_loss: 0.5324\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4033 - val_loss: 0.5587\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4072 - val_loss: 0.5677\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3846 - val_loss: 0.5164\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3914 - val_loss: 0.5273\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3883 - val_loss: 0.5343\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3711 - val_loss: 0.5118\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3737 - val_loss: 0.5366\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3740 - val_loss: 0.5273\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3696 - val_loss: 0.5084\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.3662 - val_loss: 0.5258\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3555 - val_loss: 0.5072\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3686 - val_loss: 0.5186\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.3604 - val_loss: 0.5190\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.3645 - val_loss: 0.5053\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3640 - val_loss: 0.5186\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3661 - val_loss: 0.5300\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.3562 - val_loss: 0.5047\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3575 - val_loss: 0.5139\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3555 - val_loss: 0.5092\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3492 - val_loss: 0.5057\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3499 - val_loss: 0.5075\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3462 - val_loss: 0.5086\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3478 - val_loss: 0.5061\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-22:57:24] Training the entire fine-tuned model...\n",
      "[2022_04_09-22:57:32] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3630WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1248s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1248s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 678ms/step - loss: 0.3630 - val_loss: 0.5006\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3470 - val_loss: 0.4996\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3440 - val_loss: 0.5071\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3591 - val_loss: 0.5009\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3407 - val_loss: 0.4881\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3210 - val_loss: 0.5068\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3291 - val_loss: 0.4901\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3019 - val_loss: 0.4952\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.2970 - val_loss: 0.4928\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3081 - val_loss: 0.5000\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.2963 - val_loss: 0.5050\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_09-22:58:00] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-22:58:00] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:58:19] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3338WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0932s vs `on_train_batch_end` time: 0.1271s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0932s vs `on_train_batch_end` time: 0.1271s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 653ms/step - loss: 0.3327 - val_loss: 0.4883\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>464</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  464  21\n",
       "1  124  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24870466321243523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:24kp1aec) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 28269... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██▁▁▁▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂█▃▄▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>4</td></tr><tr><td>best_val_loss</td><td>0.48808</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3327</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.48829</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">splendid-glitter-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/24kp1aec\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/24kp1aec</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_225625-24kp1aec/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:24kp1aec). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/147qkl9i\" target=\"_blank\">magic-dawn-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-22:59:12] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:59:12] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-22:59:12] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 706ms/step - loss: 0.8608 - val_loss: 1.2758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 220ms/step - loss: 0.7746 - val_loss: 0.6416\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 0.6767 - val_loss: 0.7020\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 223ms/step - loss: 0.5734 - val_loss: 0.4642\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 227ms/step - loss: 0.5529 - val_loss: 0.4958\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 220ms/step - loss: 0.5294 - val_loss: 0.4087\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 223ms/step - loss: 0.5172 - val_loss: 0.4025\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 0.5000 - val_loss: 0.4019\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 218ms/step - loss: 0.4781 - val_loss: 0.3952\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 223ms/step - loss: 0.4660 - val_loss: 0.4243\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 223ms/step - loss: 0.4533 - val_loss: 0.3935\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.4485 - val_loss: 0.4410\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.4506 - val_loss: 0.3893\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 254ms/step - loss: 0.4386 - val_loss: 0.4169\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 220ms/step - loss: 0.4428 - val_loss: 0.3898\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 0.4258 - val_loss: 0.3815\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 220ms/step - loss: 0.4249 - val_loss: 0.3849\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 220ms/step - loss: 0.4198 - val_loss: 0.3921\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 220ms/step - loss: 0.4073 - val_loss: 0.3751\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 221ms/step - loss: 0.4028 - val_loss: 0.4354\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 0.4354 - val_loss: 0.3781\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 220ms/step - loss: 0.4241 - val_loss: 0.3741\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.4230 - val_loss: 0.3927\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 219ms/step - loss: 0.3956 - val_loss: 0.3680\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 225ms/step - loss: 0.3949 - val_loss: 0.3677\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 230ms/step - loss: 0.3881 - val_loss: 0.3900\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3980 - val_loss: 0.3692\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 230ms/step - loss: 0.4112 - val_loss: 0.3702\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3964 - val_loss: 0.3661\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 230ms/step - loss: 0.3923 - val_loss: 0.3851\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3821 - val_loss: 0.3641\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 230ms/step - loss: 0.3800 - val_loss: 0.3638\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3881 - val_loss: 0.3785\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 230ms/step - loss: 0.3725 - val_loss: 0.3645\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3805 - val_loss: 0.3645\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3783 - val_loss: 0.3645\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3658 - val_loss: 0.3655\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3716 - val_loss: 0.3664\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-23:00:01] Training the entire fine-tuned model...\n",
      "[2022_04_09-23:00:50] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 816ms/step - loss: 0.8252 - val_loss: 0.7106\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.6079 - val_loss: 0.5772\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.4873 - val_loss: 0.4069\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.4469 - val_loss: 0.4038\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.4173 - val_loss: 0.3646\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.4040 - val_loss: 0.3630\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3908 - val_loss: 0.3625\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3746 - val_loss: 0.3565\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3634 - val_loss: 0.3620\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3548 - val_loss: 0.3543\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3626 - val_loss: 0.3662\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3328 - val_loss: 0.3547\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3429 - val_loss: 0.3587\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.2858 - val_loss: 0.3525\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.2917 - val_loss: 0.3533\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2864 - val_loss: 0.3600\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.2849 - val_loss: 0.3542\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.2922 - val_loss: 0.3543\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.2890 - val_loss: 0.3563\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.2773 - val_loss: 0.3572\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "[2022_04_09-23:01:34] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-23:01:34] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:01:34] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 852ms/step - loss: 0.2985 - val_loss: 0.3532\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>516</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  516  28\n",
       "1   77  37"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4134078212290503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:147qkl9i) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 28572... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▃▄▄▄▄▁</td></tr><tr><td>loss</td><td>█▇▆▄▄▄▃▃▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂█▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▄▂▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>13</td></tr><tr><td>best_val_loss</td><td>0.35248</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.29847</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.35318</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">magic-dawn-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/147qkl9i\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/147qkl9i</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_225856-147qkl9i/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:147qkl9i). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/2nuakcdv\" target=\"_blank\">hardy-terrain-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-23:02:28] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:02:28] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:02:28] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 608ms/step - loss: 0.8272 - val_loss: 0.8419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 177ms/step - loss: 0.6216 - val_loss: 0.8348\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 180ms/step - loss: 0.5527 - val_loss: 0.5548\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 180ms/step - loss: 0.5296 - val_loss: 0.5354\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 176ms/step - loss: 0.4878 - val_loss: 0.7282\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.5042 - val_loss: 0.5483\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 177ms/step - loss: 0.4442 - val_loss: 0.6946\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 177ms/step - loss: 0.4378 - val_loss: 0.5700\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 180ms/step - loss: 0.3730 - val_loss: 0.5318\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 180ms/step - loss: 0.3819 - val_loss: 0.5305\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 177ms/step - loss: 0.3516 - val_loss: 0.5497\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.3617 - val_loss: 0.5198\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 176ms/step - loss: 0.3689 - val_loss: 0.5204\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 176ms/step - loss: 0.3611 - val_loss: 0.5281\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.3500 - val_loss: 0.5174\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.3504 - val_loss: 0.5202\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 180ms/step - loss: 0.3514 - val_loss: 0.5197\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 176ms/step - loss: 0.3459 - val_loss: 0.5096\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 180ms/step - loss: 0.3547 - val_loss: 0.5116\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 0.3539 - val_loss: 0.5412\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 176ms/step - loss: 0.3448 - val_loss: 0.5060\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 176ms/step - loss: 0.3489 - val_loss: 0.5120\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.3505 - val_loss: 0.5193\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 180ms/step - loss: 0.3426 - val_loss: 0.5049\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 180ms/step - loss: 0.3413 - val_loss: 0.5097\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 180ms/step - loss: 0.3348 - val_loss: 0.5105\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 181ms/step - loss: 0.3402 - val_loss: 0.5012\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 177ms/step - loss: 0.3388 - val_loss: 0.5136\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 181ms/step - loss: 0.3263 - val_loss: 0.4990\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 181ms/step - loss: 0.3315 - val_loss: 0.5037\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 0.3404 - val_loss: 0.5107\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 180ms/step - loss: 0.3283 - val_loss: 0.4985\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3355 - val_loss: 0.5024\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3292 - val_loss: 0.4996\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.3289 - val_loss: 0.5126\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 0.3177 - val_loss: 0.5026\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 177ms/step - loss: 0.3293 - val_loss: 0.4994\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.3201 - val_loss: 0.5018\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-23:03:17] Training the entire fine-tuned model...\n",
      "[2022_04_09-23:03:25] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4438WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0967s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0967s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 684ms/step - loss: 0.4438 - val_loss: 0.5692\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3590 - val_loss: 0.4944\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3386 - val_loss: 0.5250\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3310 - val_loss: 0.4754\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.3141 - val_loss: 0.4850\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3138 - val_loss: 0.5277\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.3070 - val_loss: 0.4812\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.2832 - val_loss: 0.5081\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.2830 - val_loss: 0.4836\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.2680 - val_loss: 0.4902\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_09-23:03:52] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-23:03:52] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:03:52] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3214WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1249s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1249s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 677ms/step - loss: 0.3214 - val_loss: 0.4812\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>479</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>134</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  479   6\n",
       "1  134  14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2nuakcdv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 28996... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▅▄▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▃▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▃▂▆▅▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂▁▃▁▂▁▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>0.47539</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32142</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.4812</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">hardy-terrain-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/2nuakcdv\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/2nuakcdv</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_230213-2nuakcdv/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2nuakcdv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/2mvsxrrg\" target=\"_blank\">playful-waterfall-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-23:04:45] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:04:45] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:04:45] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 697ms/step - loss: 0.9819 - val_loss: 0.9459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 229ms/step - loss: 0.7009 - val_loss: 0.5293\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 220ms/step - loss: 0.6378 - val_loss: 0.4792\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 0.5221 - val_loss: 0.4260\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.4999 - val_loss: 0.4383\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 225ms/step - loss: 0.4881 - val_loss: 0.4096\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 221ms/step - loss: 0.4584 - val_loss: 0.4181\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 223ms/step - loss: 0.4593 - val_loss: 0.3994\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.4541 - val_loss: 0.3955\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 221ms/step - loss: 0.4417 - val_loss: 0.4010\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.4574 - val_loss: 0.3912\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 228ms/step - loss: 0.4312 - val_loss: 0.4159\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.4357 - val_loss: 0.3832\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4369 - val_loss: 0.3843\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 230ms/step - loss: 0.4200 - val_loss: 0.4213\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 230ms/step - loss: 0.4259 - val_loss: 0.3934\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 229ms/step - loss: 0.4171 - val_loss: 0.3981\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4444 - val_loss: 0.3815\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.4154 - val_loss: 0.4044\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4040 - val_loss: 0.3787\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 229ms/step - loss: 0.4141 - val_loss: 0.3843\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4116 - val_loss: 0.3841\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 229ms/step - loss: 0.4019 - val_loss: 0.3761\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 228ms/step - loss: 0.4066 - val_loss: 0.3785\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4094 - val_loss: 0.3774\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3946 - val_loss: 0.3768\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3964 - val_loss: 0.3767\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 229ms/step - loss: 0.3977 - val_loss: 0.3774\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3932 - val_loss: 0.3775\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 229ms/step - loss: 0.3978 - val_loss: 0.3748\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 227ms/step - loss: 0.3896 - val_loss: 0.3754\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.3923 - val_loss: 0.3751\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 0.3933 - val_loss: 0.3750\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 220ms/step - loss: 0.3998 - val_loss: 0.3752\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 228ms/step - loss: 0.3847 - val_loss: 0.3752\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 221ms/step - loss: 0.3980 - val_loss: 0.3752\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 219ms/step - loss: 0.3910 - val_loss: 0.3751\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 219ms/step - loss: 0.3975 - val_loss: 0.3752\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_09-23:05:35] Training the entire fine-tuned model...\n",
      "[2022_04_09-23:05:43] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 829ms/step - loss: 0.4024 - val_loss: 0.3771\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3950 - val_loss: 0.3745\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.4006 - val_loss: 0.3753\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.4021 - val_loss: 0.3759\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3977 - val_loss: 0.3740\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3963 - val_loss: 0.3742\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3894 - val_loss: 0.3733\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3906 - val_loss: 0.3730\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3796 - val_loss: 0.3731\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.4007 - val_loss: 0.3737\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3905 - val_loss: 0.3727\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3838 - val_loss: 0.3711\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3876 - val_loss: 0.3718\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.3764 - val_loss: 0.3720\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3891 - val_loss: 0.3709\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3883 - val_loss: 0.3719\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3928 - val_loss: 0.3713\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3899 - val_loss: 0.3703\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3867 - val_loss: 0.3687\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3876 - val_loss: 0.3698\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3836 - val_loss: 0.3694\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3756 - val_loss: 0.3707\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3766 - val_loss: 0.3693\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3774 - val_loss: 0.3689\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3848 - val_loss: 0.3689\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.3803 - val_loss: 0.3684\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.3849 - val_loss: 0.3683\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3771 - val_loss: 0.3684\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3715 - val_loss: 0.3686\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3712 - val_loss: 0.3684\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 2s 389ms/step - loss: 0.3859 - val_loss: 0.3683\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3778 - val_loss: 0.3683\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3747 - val_loss: 0.3683\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3682 - val_loss: 0.3682\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3791 - val_loss: 0.3683\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3725 - val_loss: 0.3682\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3697 - val_loss: 0.3682\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3694 - val_loss: 0.3682\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3633 - val_loss: 0.3683\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3825 - val_loss: 0.3683\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3728 - val_loss: 0.3682\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3790 - val_loss: 0.3682\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "[2022_04_09-23:07:07] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-23:07:07] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:07:07] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 877ms/step - loss: 0.3813 - val_loss: 0.3682\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>534</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  534  10\n",
       "1   89  25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33557046979865773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2mvsxrrg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 29338... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▆▇▇▇▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▇▇▇██▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>33</td></tr><tr><td>best_val_loss</td><td>0.36818</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38135</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.36823</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">playful-waterfall-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/2mvsxrrg\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/2mvsxrrg</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_230431-2mvsxrrg/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2mvsxrrg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/284dh2t4\" target=\"_blank\">still-monkey-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-23:08:08] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:08:08] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:08:08] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 579ms/step - loss: 0.9294 - val_loss: 0.7386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.6084 - val_loss: 1.0496\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.6188 - val_loss: 0.6141\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4893 - val_loss: 0.6357\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4213 - val_loss: 0.5164\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4092 - val_loss: 0.5157\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3817 - val_loss: 0.5427\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3733 - val_loss: 0.5269\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3690 - val_loss: 0.5301\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3728 - val_loss: 0.5086\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3521 - val_loss: 0.5285\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3493 - val_loss: 0.5076\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3449 - val_loss: 0.5300\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3693 - val_loss: 0.5055\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3630 - val_loss: 0.5362\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3319 - val_loss: 0.5070\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3343 - val_loss: 0.5205\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3230 - val_loss: 0.5051\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3163 - val_loss: 0.5756\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3301 - val_loss: 0.4963\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3325 - val_loss: 0.4910\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3288 - val_loss: 0.4886\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3245 - val_loss: 0.5212\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3088 - val_loss: 0.4925\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.2990 - val_loss: 0.5506\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3190 - val_loss: 0.4915\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3167 - val_loss: 0.4998\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3060 - val_loss: 0.4913\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.2963 - val_loss: 0.5099\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3149 - val_loss: 0.5092\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_09-23:08:51] Training the entire fine-tuned model...\n",
      "[2022_04_09-23:09:05] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3145WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0975s vs `on_train_batch_end` time: 0.1251s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0975s vs `on_train_batch_end` time: 0.1251s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 678ms/step - loss: 0.3145 - val_loss: 0.4958\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3242 - val_loss: 0.4958\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3137 - val_loss: 0.4906\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3076 - val_loss: 0.4891\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3099 - val_loss: 0.4919\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.2989 - val_loss: 0.4969\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3059 - val_loss: 0.4977\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3122 - val_loss: 0.4897\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3137 - val_loss: 0.4896\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3153 - val_loss: 0.4897\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3028 - val_loss: 0.4905\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3101 - val_loss: 0.4914\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_09-23:09:35] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-23:09:35] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:09:35] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3219WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1252s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1252s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 711ms/step - loss: 0.3219 - val_loss: 0.4894\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>457</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  457  28\n",
       "1  119  29"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28292682926829266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:284dh2t4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 29841... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▁▂▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▄▅▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▃▃▁▁▂▁▂▁▁▁▂▂▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>21</td></tr><tr><td>best_val_loss</td><td>0.48861</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32187</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.48941</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">still-monkey-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/284dh2t4\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/284dh2t4</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_230751-284dh2t4/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:284dh2t4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/2m8tpmir\" target=\"_blank\">devoted-hill-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-23:10:33] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:10:33] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:10:33] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 727ms/step - loss: 0.8982 - val_loss: 0.9512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.7269 - val_loss: 0.4799\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5918 - val_loss: 0.4289\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.5373 - val_loss: 0.4355\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4940 - val_loss: 0.4113\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4866 - val_loss: 0.4353\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4670 - val_loss: 0.4015\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4630 - val_loss: 0.3972\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4516 - val_loss: 0.4182\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4576 - val_loss: 0.4011\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4550 - val_loss: 0.4139\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4690 - val_loss: 0.4467\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4442 - val_loss: 0.3858\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4318 - val_loss: 0.3844\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4340 - val_loss: 0.4091\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4151 - val_loss: 0.3831\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4248 - val_loss: 0.3844\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4200 - val_loss: 0.3888\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4284 - val_loss: 0.3839\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4155 - val_loss: 0.3844\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4118 - val_loss: 0.3847\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4064 - val_loss: 0.3846\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4126 - val_loss: 0.3835\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4105 - val_loss: 0.3830\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4204 - val_loss: 0.3831\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4147 - val_loss: 0.3832\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4118 - val_loss: 0.3832\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4166 - val_loss: 0.3832\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4208 - val_loss: 0.3831\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4207 - val_loss: 0.3831\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4056 - val_loss: 0.3831\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4121 - val_loss: 0.3831\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "[2022_04_09-23:11:18] Training the entire fine-tuned model...\n",
      "[2022_04_09-23:11:26] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 849ms/step - loss: 0.4139 - val_loss: 0.3815\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.4137 - val_loss: 0.3840\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.4034 - val_loss: 0.3761\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.4086 - val_loss: 0.3838\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.4003 - val_loss: 0.3767\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.4005 - val_loss: 0.3742\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.4000 - val_loss: 0.3757\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3938 - val_loss: 0.3711\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3883 - val_loss: 0.3759\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 389ms/step - loss: 0.3811 - val_loss: 0.3710\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3800 - val_loss: 0.3750\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3750 - val_loss: 0.3661\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 390ms/step - loss: 0.3783 - val_loss: 0.3708\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3690 - val_loss: 0.3646\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3602 - val_loss: 0.3641\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3453 - val_loss: 0.3643\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 390ms/step - loss: 0.3492 - val_loss: 0.3623\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.3430 - val_loss: 0.3651\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3307 - val_loss: 0.3607\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3427 - val_loss: 0.3666\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 2s 389ms/step - loss: 0.3328 - val_loss: 0.3597\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3302 - val_loss: 0.3610\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.3142 - val_loss: 0.3637\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.3159 - val_loss: 0.3621\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.3128 - val_loss: 0.3729\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.2936 - val_loss: 0.3650\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3025 - val_loss: 0.3658\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.3066 - val_loss: 0.3648\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2979 - val_loss: 0.3674\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_09-23:12:29] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-23:12:29] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:12:29] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 860ms/step - loss: 0.3299 - val_loss: 0.3602\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>523</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  523  21\n",
       "1   77  37"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43023255813953487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2m8tpmir) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 30163... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▆▆▆▇▇▇██▁▁▂▂▃▃▃▃▄▄▅▅▅▆▆▆▇▇▁</td></tr><tr><td>loss</td><td>█▆▄▃▃▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▂▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>20</td></tr><tr><td>best_val_loss</td><td>0.35972</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32986</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.36024</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">devoted-hill-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/2m8tpmir\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/2m8tpmir</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_231016-2m8tpmir/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2m8tpmir). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/28bs5dl4\" target=\"_blank\">peachy-glitter-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-23:13:23] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:13:23] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:13:23] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 580ms/step - loss: 0.8382 - val_loss: 0.9025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.6637 - val_loss: 0.9681\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.5549 - val_loss: 0.5626\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4437 - val_loss: 0.5879\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4043 - val_loss: 0.5614\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4203 - val_loss: 0.5358\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4086 - val_loss: 0.7317\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4593 - val_loss: 0.5569\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3810 - val_loss: 0.5882\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3632 - val_loss: 0.5056\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3595 - val_loss: 0.5043\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3471 - val_loss: 0.5146\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3435 - val_loss: 0.5083\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3535 - val_loss: 0.5008\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3408 - val_loss: 0.5194\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3435 - val_loss: 0.5085\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3300 - val_loss: 0.4916\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3171 - val_loss: 0.5191\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3260 - val_loss: 0.4867\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3324 - val_loss: 0.4924\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3296 - val_loss: 0.5588\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3320 - val_loss: 0.4917\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3355 - val_loss: 0.4931\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3080 - val_loss: 0.5163\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3255 - val_loss: 0.4992\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3024 - val_loss: 0.4876\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3079 - val_loss: 0.4997\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_09-23:14:02] Training the entire fine-tuned model...\n",
      "[2022_04_09-23:14:22] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3285WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1254s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1254s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 662ms/step - loss: 0.3285 - val_loss: 0.4992\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3060 - val_loss: 0.4868\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3218 - val_loss: 0.4935\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3075 - val_loss: 0.4850\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3099 - val_loss: 0.4965\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.2919 - val_loss: 0.4904\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3006 - val_loss: 0.5013\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.2940 - val_loss: 0.4881\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.2845 - val_loss: 0.4979\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.2854 - val_loss: 0.5116\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.2877 - val_loss: 0.5030\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.2786 - val_loss: 0.4904\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_09-23:14:52] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-23:14:52] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:14:52] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3044WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0986s vs `on_train_batch_end` time: 0.1250s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0986s vs `on_train_batch_end` time: 0.1250s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 686ms/step - loss: 0.3044 - val_loss: 0.4844\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>463</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  463  22\n",
       "1  119  29"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2914572864321608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:28bs5dl4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 30579... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▆▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇█▂▂▂▂▅▂▃▁▁▁▁▁▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.48437</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30439</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.48437</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">peachy-glitter-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/28bs5dl4\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/28bs5dl4</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_231306-28bs5dl4/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:28bs5dl4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/1mstket1\" target=\"_blank\">desert-tree-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-23:15:46] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:15:46] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:15:46] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 731ms/step - loss: 0.9861 - val_loss: 0.8841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.7133 - val_loss: 0.5392\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.6238 - val_loss: 0.5239\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5829 - val_loss: 0.4217\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5365 - val_loss: 0.4135\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5151 - val_loss: 0.4271\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4893 - val_loss: 0.4054\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4780 - val_loss: 0.4238\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4541 - val_loss: 0.3962\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4535 - val_loss: 0.3942\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4439 - val_loss: 0.4137\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4399 - val_loss: 0.3864\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4323 - val_loss: 0.4097\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4386 - val_loss: 0.3937\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4179 - val_loss: 0.3836\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4291 - val_loss: 0.3836\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4066 - val_loss: 0.3767\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4033 - val_loss: 0.3794\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4124 - val_loss: 0.3771\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3978 - val_loss: 0.3726\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3937 - val_loss: 0.3866\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4060 - val_loss: 0.3697\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4061 - val_loss: 0.3774\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3870 - val_loss: 0.3795\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4109 - val_loss: 0.3704\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3904 - val_loss: 0.3665\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3828 - val_loss: 0.4009\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3968 - val_loss: 0.3644\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3821 - val_loss: 0.3649\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3590 - val_loss: 0.3845\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3818 - val_loss: 0.3755\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3719 - val_loss: 0.3656\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3699 - val_loss: 0.3710\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3763 - val_loss: 0.3666\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3724 - val_loss: 0.3633\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3608 - val_loss: 0.3643\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3687 - val_loss: 0.3630\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3625 - val_loss: 0.3654\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3591 - val_loss: 0.3616\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3642 - val_loss: 0.3640\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3566 - val_loss: 0.3625\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3663 - val_loss: 0.3612\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3567 - val_loss: 0.3686\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3615 - val_loss: 0.3604\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3672 - val_loss: 0.3685\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3637 - val_loss: 0.3613\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3571 - val_loss: 0.3609\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3520 - val_loss: 0.3642\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3628 - val_loss: 0.3626\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3610 - val_loss: 0.3629\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3562 - val_loss: 0.3625\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3382 - val_loss: 0.3645\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-23:16:52] Training the entire fine-tuned model...\n",
      "[2022_04_09-23:17:26] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 842ms/step - loss: 0.3717 - val_loss: 0.3785\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3689 - val_loss: 0.3758\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3733 - val_loss: 0.3597\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 392ms/step - loss: 0.3740 - val_loss: 0.3570\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.3617 - val_loss: 0.3607\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 389ms/step - loss: 0.3535 - val_loss: 0.3576\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 389ms/step - loss: 0.3324 - val_loss: 0.3526\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3320 - val_loss: 0.3654\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3375 - val_loss: 0.3518\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 389ms/step - loss: 0.3309 - val_loss: 0.3590\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 389ms/step - loss: 0.3045 - val_loss: 0.3504\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3057 - val_loss: 0.3509\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3133 - val_loss: 0.3509\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 389ms/step - loss: 0.2825 - val_loss: 0.3552\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3020 - val_loss: 0.3525\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.2794 - val_loss: 0.3576\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.2751 - val_loss: 0.3595\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.2791 - val_loss: 0.3531\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.2699 - val_loss: 0.3547\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_09-23:18:09] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-23:18:09] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:18:09] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 841ms/step - loss: 0.3113 - val_loss: 0.3519\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>512</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  512  32\n",
       "1   73  41"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4385026737967914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1mstket1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 30874... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_val_loss</td><td>0.35039</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31129</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.35188</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">desert-tree-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/1mstket1\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/1mstket1</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_231530-1mstket1/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1mstket1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/27mcx20e\" target=\"_blank\">valiant-dawn-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-23:19:08] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:19:08] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:19:08] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 602ms/step - loss: 0.8780 - val_loss: 0.6369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.6063 - val_loss: 0.7857\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4857 - val_loss: 0.5237\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4047 - val_loss: 0.5518\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3830 - val_loss: 0.5555\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4184 - val_loss: 0.6102\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3785 - val_loss: 0.5476\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3869 - val_loss: 0.5395\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3792 - val_loss: 0.5242\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3640 - val_loss: 0.5103\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3589 - val_loss: 0.5397\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3689 - val_loss: 0.5123\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3542 - val_loss: 0.5090\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3549 - val_loss: 0.5125\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3518 - val_loss: 0.5123\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3465 - val_loss: 0.5081\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3476 - val_loss: 0.5087\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3536 - val_loss: 0.5047\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3458 - val_loss: 0.5225\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3407 - val_loss: 0.4997\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3499 - val_loss: 0.5054\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3578 - val_loss: 0.5185\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3459 - val_loss: 0.4981\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3528 - val_loss: 0.5026\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3389 - val_loss: 0.5027\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3379 - val_loss: 0.5116\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3390 - val_loss: 0.4984\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3330 - val_loss: 0.5006\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3392 - val_loss: 0.5136\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3342 - val_loss: 0.5106\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3323 - val_loss: 0.5024\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-23:19:52] Training the entire fine-tuned model...\n",
      "[2022_04_09-23:20:07] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3492WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1229s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1229s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 676ms/step - loss: 0.3492 - val_loss: 0.5034\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3367 - val_loss: 0.5155\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3397 - val_loss: 0.5084\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3292 - val_loss: 0.5058\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3187 - val_loss: 0.5039\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3137 - val_loss: 0.5083\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3101 - val_loss: 0.5023\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3124 - val_loss: 0.5006\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3084 - val_loss: 0.5091\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3129 - val_loss: 0.5091\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3020 - val_loss: 0.5021\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3039 - val_loss: 0.5109\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3105 - val_loss: 0.5088\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3020 - val_loss: 0.5025\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3034 - val_loss: 0.5004\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.2988 - val_loss: 0.5017\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3043 - val_loss: 0.5037\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3048 - val_loss: 0.5078\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3007 - val_loss: 0.5108\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3024 - val_loss: 0.5107\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3001 - val_loss: 0.5103\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3043 - val_loss: 0.5083\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.2962 - val_loss: 0.5065\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "[2022_04_09-23:20:58] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-23:20:58] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:20:58] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3087WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0966s vs `on_train_batch_end` time: 0.1248s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0966s vs `on_train_batch_end` time: 0.1248s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 675ms/step - loss: 0.3087 - val_loss: 0.5015\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>452</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>115</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  452  33\n",
       "1  115  33"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.308411214953271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:27mcx20e) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 31340... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▁</td></tr><tr><td>loss</td><td>█▅▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▂▂▄▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>22</td></tr><tr><td>best_val_loss</td><td>0.49809</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30871</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.50149</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">valiant-dawn-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/27mcx20e\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/27mcx20e</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_231850-27mcx20e/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:27mcx20e). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/zkmau5te\" target=\"_blank\">hopeful-totem-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-23:21:54] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:21:55] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:21:55] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 713ms/step - loss: 0.9525 - val_loss: 0.8522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.6538 - val_loss: 0.4665\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5961 - val_loss: 0.4269\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.5403 - val_loss: 0.4369\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.5077 - val_loss: 0.4136\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4973 - val_loss: 0.4388\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4791 - val_loss: 0.4034\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4590 - val_loss: 0.4015\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4551 - val_loss: 0.4165\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4512 - val_loss: 0.4012\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4481 - val_loss: 0.3895\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4504 - val_loss: 0.4084\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4528 - val_loss: 0.3991\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4245 - val_loss: 0.3821\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4185 - val_loss: 0.4128\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4269 - val_loss: 0.3762\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4127 - val_loss: 0.3770\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4333 - val_loss: 0.4383\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4336 - val_loss: 0.3747\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4047 - val_loss: 0.3707\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4068 - val_loss: 0.4115\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3992 - val_loss: 0.3733\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3927 - val_loss: 0.3735\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3843 - val_loss: 0.3673\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3802 - val_loss: 0.3732\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4002 - val_loss: 0.3807\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3928 - val_loss: 0.3627\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3801 - val_loss: 0.3833\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.3750 - val_loss: 0.3615\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3661 - val_loss: 0.3857\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3819 - val_loss: 0.3672\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3810 - val_loss: 0.3691\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3862 - val_loss: 0.3894\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3719 - val_loss: 0.3594\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3666 - val_loss: 0.3593\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3657 - val_loss: 0.3761\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3635 - val_loss: 0.3599\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3549 - val_loss: 0.3592\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3682 - val_loss: 0.3650\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3691 - val_loss: 0.3631\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3684 - val_loss: 0.3605\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3674 - val_loss: 0.3690\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3799 - val_loss: 0.3635\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3623 - val_loss: 0.3626\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3581 - val_loss: 0.3624\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3435 - val_loss: 0.3625\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-23:22:56] Training the entire fine-tuned model...\n",
      "[2022_04_09-23:23:04] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 855ms/step - loss: 0.6551 - val_loss: 0.4376\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.4594 - val_loss: 0.3631\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.4600 - val_loss: 0.3698\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.4319 - val_loss: 0.3654\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.4069 - val_loss: 0.3690\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3896 - val_loss: 0.3601\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3594 - val_loss: 0.3660\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3521 - val_loss: 0.3699\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3360 - val_loss: 0.3679\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3175 - val_loss: 0.3564\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3041 - val_loss: 0.3578\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2928 - val_loss: 0.3850\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.2723 - val_loss: 0.3688\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.2521 - val_loss: 0.3729\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.2420 - val_loss: 0.3751\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.2304 - val_loss: 0.3761\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.2268 - val_loss: 0.3787\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2252 - val_loss: 0.3763\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_09-23:23:45] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-23:23:45] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:23:45] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 827ms/step - loss: 0.3075 - val_loss: 0.3560\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>516</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  516  28\n",
       "1   79  35"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3954802259887006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_09_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:zkmau5te) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 31721... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▃▃▃▃▃▃▃▃▃▂▃▂▂▂▃▂▂▂▂▂▂▂▂▃▃▃▂▂▂▂▁▁▁▂</td></tr><tr><td>lr</td><td>█████████████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.35603</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30746</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.35603</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">hopeful-totem-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/zkmau5te\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/zkmau5te</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_232138-zkmau5te/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:zkmau5te). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/jpfq8rsj\" target=\"_blank\">leafy-wood-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-23:24:41] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:24:41] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:24:41] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 588ms/step - loss: 0.9149 - val_loss: 0.6224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.6361 - val_loss: 0.8454\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5199 - val_loss: 0.5236\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4755 - val_loss: 0.5512\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4197 - val_loss: 0.5375\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3848 - val_loss: 0.5200\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3753 - val_loss: 0.5190\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3690 - val_loss: 0.5200\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3793 - val_loss: 0.5053\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3804 - val_loss: 0.5096\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3684 - val_loss: 0.5422\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3538 - val_loss: 0.5041\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3659 - val_loss: 0.5022\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3467 - val_loss: 0.5060\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3440 - val_loss: 0.5898\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3906 - val_loss: 0.5264\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3446 - val_loss: 0.4935\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3469 - val_loss: 0.5582\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3231 - val_loss: 0.5118\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3447 - val_loss: 0.4980\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3237 - val_loss: 0.5373\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3415 - val_loss: 0.4957\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3286 - val_loss: 0.4966\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3187 - val_loss: 0.5196\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3151 - val_loss: 0.4936\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_09-23:25:19] Training the entire fine-tuned model...\n",
      "[2022_04_09-23:25:39] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4032WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0990s vs `on_train_batch_end` time: 0.1228s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0990s vs `on_train_batch_end` time: 0.1228s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 714ms/step - loss: 0.4032 - val_loss: 0.6353\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3929 - val_loss: 0.5211\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3687 - val_loss: 0.4743\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3217 - val_loss: 0.5567\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3450 - val_loss: 0.4873\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3111 - val_loss: 0.5414\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3100 - val_loss: 0.4862\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.2968 - val_loss: 0.5032\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.2720 - val_loss: 0.4890\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.2772 - val_loss: 0.4930\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.2716 - val_loss: 0.4993\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_09-23:26:07] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-23:26:07] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-23:26:25] Validation set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3171WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0990s vs `on_train_batch_end` time: 0.1223s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0990s vs `on_train_batch_end` time: 0.1223s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 701ms/step - loss: 0.3171 - val_loss: 0.4872\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>481</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>134</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  481   4\n",
       "1  134  14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1686746987951807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_09_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "for seed in [18, 27, 36, 42]: # 4\n",
    "    train = pd.read_csv(path.join(DATA_DIR, f\"chen/deduplicated/crossval/chen_{seed}_a.csv\"), index_col=0)\n",
    "    test = pd.read_csv(path.join(DATA_DIR, f\"chen/deduplicated/crossval/chen_{seed}_b.csv\"), index_col=0)\n",
    "    train[\"seq\"] = train[\"heavy\"] + train[\"light\"]\n",
    "    test[\"seq\"] = test[\"heavy\"] + test[\"light\"]\n",
    "    for pat in patience:\n",
    "        for lr in learning_rate:\n",
    "            finetune_by_settings_and_data(pat, lr, f\"Split {seed}a\", train, test, f\"{seed}a\")\n",
    "            finetune_by_settings_and_data(pat, lr, f\"Split {seed}b\", test, train, f\"{seed}b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec00e839-cf6f-400c-9cba-3fc6e08f52b4",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56644fd0-ccff-434a-87a3-6a053d7f1416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e816cc74-29d5-4c85-9fcc-1db0f5042900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_10-09:23:52] Test set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "seq_len = 512\n",
    "encoded_test_set = encode_dataset(test_data[\"seq\"], test_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ebf1fb3-80bd-423c-a6af-2c51d82675ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X, test_Y, test_sample_weigths = encoded_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76d7099d-ada5-4464-8337-e64f1eaa7fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antibody_ID</th>\n",
       "      <th>heavy</th>\n",
       "      <th>light</th>\n",
       "      <th>CDR_length</th>\n",
       "      <th>PSH</th>\n",
       "      <th>PPC</th>\n",
       "      <th>PNC</th>\n",
       "      <th>SFvCSP</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abagovomab</td>\n",
       "      <td>QVKLQESGAELARPGASVKLSCKASGYTFTNYWMQWVKQRPGQGLD...</td>\n",
       "      <td>DIELTQSPASLSASVGETVTITCQASENIYSYLAWHQQKQGKSPQL...</td>\n",
       "      <td>46</td>\n",
       "      <td>129.7603</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abituzumab</td>\n",
       "      <td>QVQLQQSGGELAKPGASVKVSCKASGYTFSSFWMHWVRQAPGQGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRASQDISNYLAWYQQKPGKAPKL...</td>\n",
       "      <td>45</td>\n",
       "      <td>115.9106</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0421</td>\n",
       "      <td>-3.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abrilumab</td>\n",
       "      <td>QVQLVQSGAEVKKPGASVKVSCKVSGYTLSDLSIHWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSVSASVGDRVTITCRASQGISSWLAWYQQKPGKAPKL...</td>\n",
       "      <td>45</td>\n",
       "      <td>109.6995</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8965</td>\n",
       "      <td>-4.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Actoxumab</td>\n",
       "      <td>QVQLVESGGGVVQPGRSLRLSCAASGFSFSNYGMHWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSVSASVGDRVTITCRASQGISSWLAWYQHKPGKAPKL...</td>\n",
       "      <td>49</td>\n",
       "      <td>112.6290</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.1247</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adalimumab</td>\n",
       "      <td>EVQLVESGGGLVQPGRSLRLSCAASGFTFDDYAMHWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRASQGIRNYLAWYQQKPGKAPKL...</td>\n",
       "      <td>48</td>\n",
       "      <td>111.2512</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>1.1364</td>\n",
       "      <td>-19.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Antibody_ID                                              heavy  \\\n",
       "0  Abagovomab  QVKLQESGAELARPGASVKLSCKASGYTFTNYWMQWVKQRPGQGLD...   \n",
       "1  Abituzumab  QVQLQQSGGELAKPGASVKVSCKASGYTFSSFWMHWVRQAPGQGLE...   \n",
       "2   Abrilumab  QVQLVQSGAEVKKPGASVKVSCKVSGYTLSDLSIHWVRQAPGKGLE...   \n",
       "3   Actoxumab  QVQLVESGGGVVQPGRSLRLSCAASGFSFSNYGMHWVRQAPGKGLE...   \n",
       "4  Adalimumab  EVQLVESGGGLVQPGRSLRLSCAASGFTFDDYAMHWVRQAPGKGLE...   \n",
       "\n",
       "                                               light  CDR_length       PSH  \\\n",
       "0  DIELTQSPASLSASVGETVTITCQASENIYSYLAWHQQKQGKSPQL...          46  129.7603   \n",
       "1  DIQMTQSPSSLSASVGDRVTITCRASQDISNYLAWYQQKPGKAPKL...          45  115.9106   \n",
       "2  DIQMTQSPSSVSASVGDRVTITCRASQGISSWLAWYQQKPGKAPKL...          45  109.6995   \n",
       "3  DIQMTQSPSSVSASVGDRVTITCRASQGISSWLAWYQHKPGKAPKL...          49  112.6290   \n",
       "4  DIQMTQSPSSLSASVGDRVTITCRASQGIRNYLAWYQQKPGKAPKL...          48  111.2512   \n",
       "\n",
       "      PPC     PNC  SFvCSP  Y  \n",
       "0  0.0000  0.0000   16.32  1  \n",
       "1  0.0954  0.0421   -3.10  1  \n",
       "2  0.0000  0.8965   -4.00  1  \n",
       "3  0.0000  1.1247    3.10  1  \n",
       "4  0.0485  1.1364  -19.50  1  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tap_data = pd.read_csv(path.join(DATA_DIR, \"tap/TAP_data.csv\"))\n",
    "tap_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5cc5b7cb-1cc4-450d-a52d-2a027f00a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tap_data[\"seq\"] = tap_data[\"heavy\"] +  tap_data[\"light\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b06ed7a5-5e02-48e8-8e69-6355c776ef1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_15-12:44:22] TAP set: Filtered out 0 of 241 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "encoded_tap_set = encode_dataset(tap_data[\"seq\"], tap_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'TAP set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e96bc721-e479-400a-8d09-e0b6c9635a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tap_X, tap_Y, tap_sample_weigths = encoded_tap_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49e9d36a-63d9-45b6-bcf8-b66e75fd506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = path.join(DATA_DIR, \"protein_bert/2022_03_30__05.pkl\")\n",
    "model = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1959c14b-c779-4741-9a0a-b2a453fd1fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6486486486486486"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(test_X, batch_size=32)\n",
    "y_pred_classes = (y_pred >= 0.5)\n",
    "f1_score(test_Y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c95a893-f467-405f-81b6-59b0277c0a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34285714285714286"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(tap_X, batch_size=32)\n",
    "y_pred_classes = (y_pred >= 0.5)\n",
    "f1_score(tap_Y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "345121cc-6ccb-46c2-aef0-5b173263f069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23651452282157676"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(tap_Y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "566d023a-b6e2-48e9-b884-c48037cc54bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_15-11:35:42] Test set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "seq_len = 512\n",
    "encoded_test_set = encode_dataset(test_data[\"seq\"], test_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Test set')\n",
    "test_X, test_Y, test_sample_weigths = encoded_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb44bed8-ab18-4ae3-b16e-8c90e38c027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_name, dir_name, x_test, y_test, x_tap, y_tap):\n",
    "    model_path = path.join(DATA_DIR, f\"protein_bert/{dir_name}/{model_name}\")\n",
    "    model = keras.models.load_model(model_path)\n",
    "    parts = model_name[11:].split(\"_\")\n",
    "    patience_stop = parts[0]\n",
    "    patience_lr = parts[1]\n",
    "    lr = parts[2]\n",
    "    \n",
    "    y_pred = model.predict(x_test, batch_size=32)\n",
    "    y_pred_classes = (y_pred >= 0.5)\n",
    "    #print(f\"Model {model_name}\")\n",
    "    #print(f\"Test F1: {f1}\")\n",
    "    metric_dict = {\n",
    "        \"f1\": float(metrics.f1_score(y_test, y_pred_classes)),\n",
    "        \"acc\": float(metrics.accuracy_score(y_test, y_pred_classes)),\n",
    "        \"mcc\": float(metrics.matthews_corrcoef(y_test, y_pred_classes)),\n",
    "        \"auc\": float(metrics.roc_auc_score(y_test, y_pred_classes)),\n",
    "        \"precision\": float(metrics.precision_score(y_test, y_pred_classes)),\n",
    "        \"recall\": float(metrics.recall_score(y_test, y_pred_classes))\n",
    "    }\n",
    "    filename = path.join(DATA_DIR, f\"evaluations/protein_bert/{dir_name}/{model_name}_preds.csv\")\n",
    "    str_preds = [str(int(pred)) for pred in y_pred_classes]\n",
    "    with open(filename, \"wt\") as f:\n",
    "        f.write(\",\".join(str_preds) + \"\\n\")\n",
    "        \n",
    "    filename_sum = path.join(DATA_DIR, f\"evaluations/protein_bert/{dir_name}/all.csv\")\n",
    "    line = [\"protein_bert\", patience_stop, patience_lr, lr, metric_dict[\"f1\"], metric_dict[\"mcc\"], metric_dict[\"acc\"],metric_dict[\"precision\"],metric_dict[\"recall\"],metric_dict[\"auc\"]]\n",
    "    with open(filename_sum, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter='\\t')\n",
    "        csvwriter.writerow(line)\n",
    "    \n",
    "    \n",
    "    y_pred = model.predict(x_tap, batch_size=32)\n",
    "    y_pred_classes = (y_pred >= 0.5)\n",
    "    acc = accuracy_score(y_tap, y_pred_classes)\n",
    "    metric_dict = {\n",
    "        \"f1\": float(metrics.f1_score(y_tap, y_pred_classes)),\n",
    "        \"acc\": float(metrics.accuracy_score(y_tap, y_pred_classes)),\n",
    "        \"mcc\": float(metrics.matthews_corrcoef(y_tap, y_pred_classes)),\n",
    "        \"auc\": float(metrics.roc_auc_score(y_tap, y_pred_classes)),\n",
    "        \"precision\": float(metrics.precision_score(y_tap, y_pred_classes)),\n",
    "        \"recall\": float(metrics.recall_score(y_tap, y_pred_classes))\n",
    "    }\n",
    "        \n",
    "    filename_sum = path.join(DATA_DIR, f\"evaluations/protein_bert/{dir_name}/tap.csv\")\n",
    "    line = [\"protein_bert\", patience_stop, patience_lr, lr, metric_dict[\"f1\"], metric_dict[\"mcc\"], metric_dict[\"acc\"],metric_dict[\"precision\"],metric_dict[\"recall\"],metric_dict[\"auc\"]]\n",
    "    with open(filename_sum, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter='\\t')\n",
    "        csvwriter.writerow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ab02fee7-bc99-42f9-8a64-5d7c60f12e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_10-09:53:46] Test set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "seed = 4\n",
    "split = \"a\"\n",
    "train_data = pd.read_csv(path.join(DATA_DIR, f\"chen/deduplicated/crossval/chen_{seed}_a.csv\"), index_col=0)\n",
    "test_data = pd.read_csv(path.join(DATA_DIR, f\"chen/deduplicated/crossval/chen_{seed}_b.csv\"), index_col=0)\n",
    "train_data[\"seq\"] = train_data[\"heavy\"] + train_data[\"light\"]\n",
    "test_data[\"seq\"] = test_data[\"heavy\"] + test_data[\"light\"]\n",
    "\n",
    "encoded_test_set = encode_dataset(test_data[\"seq\"], test_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Test set')\n",
    "test_X, test_Y, test_sample_weigths = encoded_test_set\n",
    "\n",
    "test_model(\"2022_04_09_3_3_0.0001\", \"4a\", test_X, test_Y, tap_X, tap_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7bddc1b4-70ec-40c7-82b7-86c006cf6412",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [4, 18, 27, 36, 42]\n",
    "seeds = [36, 42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b626b4f1-a24a-46ee-8621-3f22949b228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f201d8c-c65c-4ebb-9b78-6f2ff4bf4991",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_15-11:36:29] Test set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-11:36:29] Train set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-11:41:15] Test set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-11:41:15] Train set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for seed in seeds:\n",
    "    train_data = pd.read_csv(path.join(DATA_DIR, f\"chen/deduplicated/crossval/chen_{seed}_a.csv\"), index_col=0)\n",
    "    test_data = pd.read_csv(path.join(DATA_DIR, f\"chen/deduplicated/crossval/chen_{seed}_b.csv\"), index_col=0)\n",
    "    train_data[\"seq\"] = train_data[\"heavy\"] + train_data[\"light\"]\n",
    "    test_data[\"seq\"] = test_data[\"heavy\"] + test_data[\"light\"]\n",
    "    encoded_test_set = encode_dataset(test_data[\"seq\"], test_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Test set')\n",
    "    test_X, test_Y, test_sample_weigths = encoded_test_set\n",
    "    encoded_train_set = encode_dataset(train_data[\"seq\"], train_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Train set')\n",
    "    train_X, train_Y, test_sample_weigths = encoded_train_set\n",
    "    \n",
    "    for split in [\"a\", \"b\"]:\n",
    "        for model in os.listdir(path.join(DATA_DIR, f\"protein_bert/{seed}{split}\")):\n",
    "            if model.startswith(\"2022_04_09\"):\n",
    "                if split == \"a\":\n",
    "                    test_model(model, f\"{seed}{split}\", test_X, test_Y, tap_X, tap_Y)\n",
    "                else:\n",
    "                    test_model(model, f\"{seed}{split}\", train_X, train_Y, tap_X, tap_Y)\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc2be9b-108a-4163-a579-07ab6ca4e2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76574dfa-f777-4b64-a8f5-08df816879ab",
   "metadata": {},
   "source": [
    "# Full train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71fbac67-086d-41da-8141-aef3c17a6dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "760d1c1a-68dd-42c8-958b-0cbc2bbb4796",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_train.csv\"), index_col=0)\n",
    "test_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_test.csv\"), index_col=0)\n",
    "train_data[\"seq\"] = train_data[\"heavy\"] + train_data[\"light\"]\n",
    "test_data[\"seq\"] = test_data[\"heavy\"] + test_data[\"light\"]\n",
    "valid_data, test_data = train_test_split(test_data, test_size=0.5, random_state=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "358e32a7-9fdf-4a98-a33d-2352cdf0a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_old = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_train_data_old.csv\"), index_col=0)\n",
    "test_data_old = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_test_data_old.csv\"), index_col=0)\n",
    "valid_data_old = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_valid_data_old.csv\"), index_col=0)\n",
    "train_data_old[\"seq\"] = train_data_old[\"heavy\"] + train_data_old[\"light\"]\n",
    "test_data_old[\"seq\"] = test_data_old[\"heavy\"] + test_data_old[\"light\"]\n",
    "valid_data_old[\"seq\"] = valid_data_old[\"heavy\"] + valid_data_old[\"light\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a42ee61a-9dde-446f-8e80-040956e551b8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:rc09kp9o) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3994... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▂▂▁▂▂▁▁▂▁▂▁▁▁▁▁▁▁▂▁▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>13</td></tr><tr><td>best_val_loss</td><td>0.45486</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32702</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.46158</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">rosy-cloud-6</strong>: <a href=\"https://wandb.ai/kvetab/Easter/runs/rc09kp9o\" target=\"_blank\">https://wandb.ai/kvetab/Easter/runs/rc09kp9o</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_143331-rc09kp9o/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:rc09kp9o). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/2icd7p5e\" target=\"_blank\">pious-voice-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_15-14:37:59] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:37:59] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:37:59] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 286ms/step - loss: 0.8872 - val_loss: 0.7528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.5899 - val_loss: 0.5730\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.5244 - val_loss: 0.5120\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4718 - val_loss: 0.4757\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4801 - val_loss: 0.4985\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4512 - val_loss: 0.4987\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4273 - val_loss: 0.4444\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4183 - val_loss: 0.4401\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4154 - val_loss: 0.4394\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4138 - val_loss: 0.4271\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4076 - val_loss: 0.4960\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4425 - val_loss: 0.4456\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4032 - val_loss: 0.4281\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_15-14:38:26] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:38:34] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.4165WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1160s vs `on_train_batch_end` time: 0.1431s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1160s vs `on_train_batch_end` time: 0.1431s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 427ms/step - loss: 0.4031 - val_loss: 0.4263\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.4097 - val_loss: 0.4271\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.4015 - val_loss: 0.4251\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.4034 - val_loss: 0.4243\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.4046 - val_loss: 0.4236\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.4002 - val_loss: 0.4234\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3964 - val_loss: 0.4219\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.4003 - val_loss: 0.4215\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3964 - val_loss: 0.4216\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3974 - val_loss: 0.4200\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3905 - val_loss: 0.4197\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3886 - val_loss: 0.4183\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3860 - val_loss: 0.4182\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3920 - val_loss: 0.4167\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3899 - val_loss: 0.4156\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3962 - val_loss: 0.4159\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3902 - val_loss: 0.4143\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3896 - val_loss: 0.4132\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3836 - val_loss: 0.4128\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3864 - val_loss: 0.4123\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3838 - val_loss: 0.4114\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3764 - val_loss: 0.4114\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3784 - val_loss: 0.4100\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3802 - val_loss: 0.4105\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3739 - val_loss: 0.4085\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3807 - val_loss: 0.4078\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3725 - val_loss: 0.4074\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3730 - val_loss: 0.4068\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3686 - val_loss: 0.4061\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 0.3682 - val_loss: 0.4064\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3684 - val_loss: 0.4051\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3657 - val_loss: 0.4041\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3686 - val_loss: 0.4035\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3720 - val_loss: 0.4035\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3621 - val_loss: 0.4029\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3628 - val_loss: 0.4025\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 3s 263ms/step - loss: 0.3566 - val_loss: 0.4025\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3629 - val_loss: 0.4004\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3578 - val_loss: 0.4002\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3503 - val_loss: 0.3997\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3554 - val_loss: 0.3982\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3515 - val_loss: 0.3979\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3504 - val_loss: 0.3971\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3478 - val_loss: 0.3980\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3442 - val_loss: 0.3976\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3556 - val_loss: 0.4003\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_15-14:40:55] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:40:55] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:40:55] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3607WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1116s vs `on_train_batch_end` time: 0.1497s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1116s vs `on_train_batch_end` time: 0.1497s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 442ms/step - loss: 0.3514 - val_loss: 0.3973\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  20  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36363636363636365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2icd7p5e) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 4331... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▃▂▂▂▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>42</td></tr><tr><td>best_val_loss</td><td>0.39712</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35142</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.39726</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">pious-voice-1</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/2icd7p5e\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/2icd7p5e</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_143743-2icd7p5e/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2icd7p5e). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/12ryvo2g\" target=\"_blank\">decent-field-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_15-14:41:49] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:41:49] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:41:49] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 292ms/step - loss: 0.7868 - val_loss: 0.5233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.5189 - val_loss: 0.5264\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4777 - val_loss: 0.4837\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4511 - val_loss: 0.4648\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4381 - val_loss: 0.4581\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4323 - val_loss: 0.4499\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4276 - val_loss: 0.4476\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4202 - val_loss: 0.4336\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4110 - val_loss: 0.4281\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4066 - val_loss: 0.4312\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4042 - val_loss: 0.4202\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4117 - val_loss: 0.4244\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4000 - val_loss: 0.4191\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4057 - val_loss: 0.4178\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3917 - val_loss: 0.4095\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4015 - val_loss: 0.4206\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4042 - val_loss: 0.4054\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3935 - val_loss: 0.4044\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3775 - val_loss: 0.4023\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3899 - val_loss: 0.4222\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3847 - val_loss: 0.4142\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3762 - val_loss: 0.4094\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_15-14:42:27] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:42:36] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3674WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1171s vs `on_train_batch_end` time: 0.1435s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1171s vs `on_train_batch_end` time: 0.1435s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 424ms/step - loss: 0.3748 - val_loss: 0.3993\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3819 - val_loss: 0.3959\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3636 - val_loss: 0.4047\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3652 - val_loss: 0.4182\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3663 - val_loss: 0.3890\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 0.3703 - val_loss: 0.3905\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3569 - val_loss: 0.3861\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3584 - val_loss: 0.3878\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3531 - val_loss: 0.3829\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3461 - val_loss: 0.3797\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3426 - val_loss: 0.3802\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3333 - val_loss: 0.3733\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3299 - val_loss: 0.3742\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3307 - val_loss: 0.4043\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3361 - val_loss: 0.3978\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_15-14:43:28] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:43:28] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:43:28] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3072WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1135s vs `on_train_batch_end` time: 0.1459s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1135s vs `on_train_batch_end` time: 0.1459s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 427ms/step - loss: 0.3293 - val_loss: 0.3771\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  20  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36363636363636365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:12ryvo2g) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 4749... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▃▃▃▂▂▂▃▂▂▂▂▁▁▁▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_val_loss</td><td>0.37327</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32934</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37711</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">decent-field-2</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/12ryvo2g\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/12ryvo2g</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_144133-12ryvo2g/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:12ryvo2g). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/2kcqor19\" target=\"_blank\">skilled-music-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_15-14:44:22] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:44:23] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:44:23] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 11s 410ms/step - loss: 0.8471 - val_loss: 0.8598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.6624 - val_loss: 0.7048\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.5235 - val_loss: 0.5406\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4893 - val_loss: 0.4924\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.5007 - val_loss: 0.4811\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4561 - val_loss: 0.4555\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4313 - val_loss: 0.4444\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4252 - val_loss: 0.4371\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4193 - val_loss: 0.4361\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4121 - val_loss: 0.4511\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4223 - val_loss: 0.4237\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4149 - val_loss: 0.4291\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4070 - val_loss: 0.4199\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3938 - val_loss: 0.4149\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3858 - val_loss: 0.4216\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4110 - val_loss: 0.4349\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4049 - val_loss: 0.4332\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_15-14:44:55] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:45:03] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3821WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1147s vs `on_train_batch_end` time: 0.1436s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1147s vs `on_train_batch_end` time: 0.1436s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 436ms/step - loss: 0.3912 - val_loss: 0.4092\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3843 - val_loss: 0.4085\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3894 - val_loss: 0.4173\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3941 - val_loss: 0.3999\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3899 - val_loss: 0.4064\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3802 - val_loss: 0.3943\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3711 - val_loss: 0.3874\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3669 - val_loss: 0.3930\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3479 - val_loss: 0.3836\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3405 - val_loss: 0.3854\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3461 - val_loss: 0.3908\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3344 - val_loss: 0.4013\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_15-14:45:46] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:45:46] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:45:50] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3296WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1143s vs `on_train_batch_end` time: 0.1446s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1143s vs `on_train_batch_end` time: 0.1446s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 426ms/step - loss: 0.3517 - val_loss: 0.3826\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  20  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36363636363636365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2kcqor19) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5035... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██▁▁▂▂▃▃▄▄▅▅▅▆▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▃▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.38264</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35167</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.38264</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">skilled-music-3</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/2kcqor19\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/2kcqor19</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_144406-2kcqor19/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2kcqor19). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/3og46y6h\" target=\"_blank\">dark-energy-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_15-14:46:45] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:46:45] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:46:45] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 11s 292ms/step - loss: 0.7612 - val_loss: 0.6272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.5468 - val_loss: 0.4896\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4684 - val_loss: 0.4755\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4512 - val_loss: 0.4935\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4544 - val_loss: 0.4561\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4271 - val_loss: 0.4412\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4230 - val_loss: 0.4592\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4347 - val_loss: 0.4376\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4187 - val_loss: 0.4279\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4091 - val_loss: 0.4224\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4093 - val_loss: 0.4206\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4192 - val_loss: 0.4737\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4227 - val_loss: 0.4522\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4240 - val_loss: 0.4149\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3966 - val_loss: 0.4456\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4198 - val_loss: 0.4099\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3848 - val_loss: 0.4184\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3835 - val_loss: 0.4026\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4016 - val_loss: 0.4089\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3846 - val_loss: 0.4051\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3662 - val_loss: 0.4075\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_15-14:47:22] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:47:31] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.7090WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1136s vs `on_train_batch_end` time: 0.1461s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1136s vs `on_train_batch_end` time: 0.1461s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 431ms/step - loss: 0.6239 - val_loss: 0.4103\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.4389 - val_loss: 0.4141\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.4006 - val_loss: 0.4025\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3978 - val_loss: 0.3925\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.3651 - val_loss: 0.3878\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3655 - val_loss: 0.3916\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3615 - val_loss: 0.3831\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3431 - val_loss: 0.3794\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3294 - val_loss: 0.4244\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3166 - val_loss: 0.3822\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2888 - val_loss: 0.3879\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_15-14:48:10] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:48:10] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:48:11] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3102WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1125s vs `on_train_batch_end` time: 0.1470s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1125s vs `on_train_batch_end` time: 0.1470s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 423ms/step - loss: 0.3266 - val_loss: 0.3858\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  21  5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3og46y6h) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5283... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▅▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▃▂▂▆▃▃▃▂▂▂▂▂▁▁▂</td></tr><tr><td>lr</td><td>█████████████████████▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▄▄▃▃▃▃▂▂▂▄▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.37945</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3266</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.38577</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dark-energy-4</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/3og46y6h\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/3og46y6h</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_144628-3og46y6h/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3og46y6h). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/1td139wd\" target=\"_blank\">fast-donkey-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_15-14:49:05] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:49:05] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:49:05] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 289ms/step - loss: 0.7580 - val_loss: 0.6996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.5862 - val_loss: 0.5701\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4914 - val_loss: 0.4816\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4537 - val_loss: 0.4963\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4644 - val_loss: 0.4558\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4447 - val_loss: 0.4844\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4242 - val_loss: 0.4440\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4100 - val_loss: 0.4411\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4419 - val_loss: 0.4274\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4130 - val_loss: 0.4249\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4055 - val_loss: 0.4190\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4090 - val_loss: 0.4465\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4065 - val_loss: 0.4108\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4002 - val_loss: 0.4094\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3985 - val_loss: 0.4652\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3937 - val_loss: 0.4074\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3829 - val_loss: 0.4118\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3887 - val_loss: 0.4039\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3833 - val_loss: 0.4063\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4028 - val_loss: 0.4312\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.4020 - val_loss: 0.3983\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3887 - val_loss: 0.3978\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3833 - val_loss: 0.4020\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3829 - val_loss: 0.3955\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3772 - val_loss: 0.3896\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3676 - val_loss: 0.3908\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3584 - val_loss: 0.3864\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3682 - val_loss: 0.4442\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3837 - val_loss: 0.4133\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3937 - val_loss: 0.4148\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3660 - val_loss: 0.3907\n",
      "[2022_04_15-14:49:54] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:50:03] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3714WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1144s vs `on_train_batch_end` time: 0.1462s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1144s vs `on_train_batch_end` time: 0.1462s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 419ms/step - loss: 0.3625 - val_loss: 0.3931\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3573 - val_loss: 0.3862\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3661 - val_loss: 0.3893\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3645 - val_loss: 0.3878\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3721 - val_loss: 0.3858\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3614 - val_loss: 0.3873\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3549 - val_loss: 0.3872\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3484 - val_loss: 0.3872\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3561 - val_loss: 0.3863\n",
      "[2022_04_15-14:50:37] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:50:37] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:50:37] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3617WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1146s vs `on_train_batch_end` time: 0.1494s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1146s vs `on_train_batch_end` time: 0.1494s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 424ms/step - loss: 0.3587 - val_loss: 0.3858\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  20  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36363636363636365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1td139wd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5544... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████████████▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▃▃▃▂▂▂▂▂▂▂▂▃▁▂▁▁▂▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.3858</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35865</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.3858</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fast-donkey-5</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/1td139wd\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/1td139wd</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_144849-1td139wd/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1td139wd). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/2q37nhq7\" target=\"_blank\">gentle-yogurt-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_15-14:51:33] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:51:33] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:51:33] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 11s 408ms/step - loss: 0.8012 - val_loss: 0.6419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.5295 - val_loss: 0.4864\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4758 - val_loss: 0.4709\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4437 - val_loss: 0.4611\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4418 - val_loss: 0.4907\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4542 - val_loss: 0.4529\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4329 - val_loss: 0.4376\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4194 - val_loss: 0.4333\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4152 - val_loss: 0.4509\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4057 - val_loss: 0.4198\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4258 - val_loss: 0.4440\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4461 - val_loss: 0.4147\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4121 - val_loss: 0.4151\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4354 - val_loss: 0.4109\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3995 - val_loss: 0.4084\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3930 - val_loss: 0.4049\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3941 - val_loss: 0.4222\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3878 - val_loss: 0.4083\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3848 - val_loss: 0.4101\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3743 - val_loss: 0.4068\n",
      "[2022_04_15-14:52:09] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:52:30] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3754WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1142s vs `on_train_batch_end` time: 0.1456s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1142s vs `on_train_batch_end` time: 0.1456s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 432ms/step - loss: 0.3842 - val_loss: 0.4167\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3932 - val_loss: 0.4001\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3817 - val_loss: 0.4084\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3627 - val_loss: 0.3947\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3709 - val_loss: 0.4067\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3854 - val_loss: 0.3902\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3754 - val_loss: 0.3991\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3699 - val_loss: 0.3932\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3685 - val_loss: 0.3862\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3519 - val_loss: 0.3845\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 0.3502 - val_loss: 0.3916\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3578 - val_loss: 0.4095\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3513 - val_loss: 0.3977\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3365 - val_loss: 0.3794\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3315 - val_loss: 0.3798\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3367 - val_loss: 0.3858\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3336 - val_loss: 0.3798\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3404 - val_loss: 0.3804\n",
      "[2022_04_15-14:53:30] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:53:30] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:53:30] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3405WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1152s vs `on_train_batch_end` time: 0.1471s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1152s vs `on_train_batch_end` time: 0.1471s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 434ms/step - loss: 0.3325 - val_loss: 0.3797\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  19  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4117647058823529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2q37nhq7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5863... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▃▃▂▂▂▂▃▂▃▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▃▄▃▃▂▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>13</td></tr><tr><td>best_val_loss</td><td>0.37944</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33247</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37972</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">gentle-yogurt-6</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/2q37nhq7\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/2q37nhq7</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_145115-2q37nhq7/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2q37nhq7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/1fg0uvtb\" target=\"_blank\">elated-jazz-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_15-14:54:25] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:54:25] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:54:25] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 11s 294ms/step - loss: 0.8363 - val_loss: 0.6832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.5742 - val_loss: 0.5053\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.4685 - val_loss: 0.4820\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.4608 - val_loss: 0.4775\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.4610 - val_loss: 0.5137\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4616 - val_loss: 0.4510\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4408 - val_loss: 0.4732\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4390 - val_loss: 0.4525\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4555 - val_loss: 0.4746\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4265 - val_loss: 0.4324\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4069 - val_loss: 0.4350\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4094 - val_loss: 0.4311\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4021 - val_loss: 0.4284\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4025 - val_loss: 0.4354\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4015 - val_loss: 0.4263\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3965 - val_loss: 0.4304\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3982 - val_loss: 0.4239\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3930 - val_loss: 0.4227\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3922 - val_loss: 0.4219\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.4027 - val_loss: 0.4210\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4029 - val_loss: 0.4287\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3998 - val_loss: 0.4572\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4059 - val_loss: 0.4188\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3888 - val_loss: 0.4178\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3832 - val_loss: 0.4164\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3875 - val_loss: 0.4171\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3898 - val_loss: 0.4309\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3962 - val_loss: 0.4142\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3848 - val_loss: 0.4123\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3848 - val_loss: 0.4111\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3804 - val_loss: 0.4122\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3893 - val_loss: 0.4168\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3906 - val_loss: 0.4121\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3841 - val_loss: 0.4100\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3844 - val_loss: 0.4089\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3821 - val_loss: 0.4107\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3762 - val_loss: 0.4089\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3816 - val_loss: 0.4100\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.3834 - val_loss: 0.4094\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3801 - val_loss: 0.4088\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3735 - val_loss: 0.4087\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3764 - val_loss: 0.4087\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3836 - val_loss: 0.4087\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3728 - val_loss: 0.4089\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3877 - val_loss: 0.4088\n",
      "[2022_04_15-14:55:32] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:56:04] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3976WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1139s vs `on_train_batch_end` time: 0.1454s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1139s vs `on_train_batch_end` time: 0.1454s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 435ms/step - loss: 0.3842 - val_loss: 0.4042\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3736 - val_loss: 0.4150\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3841 - val_loss: 0.3934\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3797 - val_loss: 0.3890\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3703 - val_loss: 0.3852\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3578 - val_loss: 0.3825\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3522 - val_loss: 0.3793\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3520 - val_loss: 0.3773\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3476 - val_loss: 0.3825\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3215 - val_loss: 0.3952\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3173 - val_loss: 0.3780\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3182 - val_loss: 0.3917\n",
      "[2022_04_15-14:56:47] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:56:47] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:56:52] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3347WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1144s vs `on_train_batch_end` time: 0.1471s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1144s vs `on_train_batch_end` time: 0.1471s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 428ms/step - loss: 0.3369 - val_loss: 0.3775\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  19  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4117647058823529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1fg0uvtb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6154... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▄▃▃▃▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.37734</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33692</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.37746</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">elated-jazz-7</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/1fg0uvtb\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/1fg0uvtb</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_145408-1fg0uvtb/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1fg0uvtb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/3aeeekdw\" target=\"_blank\">desert-brook-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_15-14:57:46] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:57:46] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:57:46] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 287ms/step - loss: 0.6981 - val_loss: 0.5152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.5291 - val_loss: 0.5591\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4891 - val_loss: 0.4721\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4612 - val_loss: 0.4988\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4604 - val_loss: 0.5260\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4563 - val_loss: 0.4502\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4317 - val_loss: 0.4608\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4206 - val_loss: 0.4394\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4160 - val_loss: 0.4281\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4078 - val_loss: 0.4230\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4087 - val_loss: 0.4335\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3994 - val_loss: 0.4190\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3960 - val_loss: 0.4178\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4081 - val_loss: 0.4096\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3842 - val_loss: 0.4086\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3940 - val_loss: 0.4934\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4363 - val_loss: 0.5024\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4307 - val_loss: 0.4430\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3994 - val_loss: 0.4037\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3824 - val_loss: 0.4130\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3767 - val_loss: 0.4019\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3748 - val_loss: 0.4021\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3836 - val_loss: 0.4061\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3721 - val_loss: 0.4047\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3878 - val_loss: 0.4054\n",
      "[2022_04_15-14:58:28] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:58:45] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.6450WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1151s vs `on_train_batch_end` time: 0.1471s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1151s vs `on_train_batch_end` time: 0.1471s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 434ms/step - loss: 0.5680 - val_loss: 0.4705\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.4341 - val_loss: 0.4418\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.4188 - val_loss: 0.3979\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3815 - val_loss: 0.3914\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3696 - val_loss: 0.3928\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3473 - val_loss: 0.3913\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3549 - val_loss: 0.3830\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3263 - val_loss: 0.3739\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3415 - val_loss: 0.3704\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3130 - val_loss: 0.4170\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3102 - val_loss: 0.4432\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.2882 - val_loss: 0.4251\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.2467 - val_loss: 0.3991\n",
      "[2022_04_15-14:59:30] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:59:30] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:59:31] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.2886WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1122s vs `on_train_batch_end` time: 0.1484s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1122s vs `on_train_batch_end` time: 0.1484s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 427ms/step - loss: 0.3016 - val_loss: 0.3653\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  91  3\n",
       "1  17  9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47368421052631576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3aeeekdw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6570... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▄▄▅▁</td></tr><tr><td>loss</td><td>█▅▅▄▄▄▄▄▄▃▄▃▃▄▃▃▄▄▃▃▃▃▃▃▃▆▄▄▃▃▃▃▂▂▂▂▂▁▂</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆█▅▆▇▄▄▄▃▃▃▃▃▃▃▆▆▄▂▃▂▂▂▂▂▅▄▂▂▂▂▂▁▁▃▄▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.36531</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30157</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.36531</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">desert-brook-8</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/3aeeekdw\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/3aeeekdw</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_145730-3aeeekdw/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3aeeekdw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/2a15f8bb\" target=\"_blank\">warm-salad-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_15-15:00:24] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:00:24] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:00:24] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 295ms/step - loss: 0.8954 - val_loss: 0.8255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.6537 - val_loss: 0.6861\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.5594 - val_loss: 0.5544\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4767 - val_loss: 0.4685\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4627 - val_loss: 0.5020\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4529 - val_loss: 0.4568\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4414 - val_loss: 0.4820\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4399 - val_loss: 0.4590\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4370 - val_loss: 0.4494\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4278 - val_loss: 0.4618\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4326 - val_loss: 0.4626\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4117 - val_loss: 0.4239\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4019 - val_loss: 0.4277\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3991 - val_loss: 0.4339\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3945 - val_loss: 0.4172\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3998 - val_loss: 0.4168\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3989 - val_loss: 0.4120\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3864 - val_loss: 0.4110\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3819 - val_loss: 0.4213\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3894 - val_loss: 0.4083\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3920 - val_loss: 0.4560\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3937 - val_loss: 0.4113\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3926 - val_loss: 0.4014\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3939 - val_loss: 0.4356\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4070 - val_loss: 0.4055\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3914 - val_loss: 0.3999\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3701 - val_loss: 0.3977\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3750 - val_loss: 0.4020\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3842 - val_loss: 0.3974\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3688 - val_loss: 0.4273\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3804 - val_loss: 0.3959\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3725 - val_loss: 0.4072\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3642 - val_loss: 0.5025\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3970 - val_loss: 0.3868\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3606 - val_loss: 0.3949\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3676 - val_loss: 0.3964\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3539 - val_loss: 0.4023\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3633 - val_loss: 0.3867\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3569 - val_loss: 0.3872\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3648 - val_loss: 0.4052\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3540 - val_loss: 0.3861\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.3629 - val_loss: 0.3868\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 101ms/step - loss: 0.3638 - val_loss: 0.3895\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 102ms/step - loss: 0.3590 - val_loss: 0.3874\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.3520 - val_loss: 0.3877\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3475 - val_loss: 0.3871\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3596 - val_loss: 0.3879\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_15-15:01:32] Training the entire fine-tuned model...\n",
      "[2022_04_15-15:01:41] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3647WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1121s vs `on_train_batch_end` time: 0.1458s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1121s vs `on_train_batch_end` time: 0.1458s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 420ms/step - loss: 0.3745 - val_loss: 0.3893\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3457 - val_loss: 0.3880\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3460 - val_loss: 0.3856\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3441 - val_loss: 0.3858\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3444 - val_loss: 0.3831\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3409 - val_loss: 0.3845\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3548 - val_loss: 0.3814\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.3409 - val_loss: 0.3852\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3484 - val_loss: 0.3814\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3408 - val_loss: 0.3808\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 0.3469 - val_loss: 0.3826\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3519 - val_loss: 0.3858\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3327 - val_loss: 0.3785\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3383 - val_loss: 0.3859\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3528 - val_loss: 0.3798\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3517 - val_loss: 0.3792\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3325 - val_loss: 0.3798\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3296 - val_loss: 0.3797\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 0.3422 - val_loss: 0.3792\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_15-15:02:44] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-15:02:44] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:03:09] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3311WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1123s vs `on_train_batch_end` time: 0.1457s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1123s vs `on_train_batch_end` time: 0.1457s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 424ms/step - loss: 0.3336 - val_loss: 0.3789\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  91  3\n",
       "1  19  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3888888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2a15f8bb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6927... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇███▁▁▁▂▂▂▃▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▂▂▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>12</td></tr><tr><td>best_val_loss</td><td>0.37848</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33362</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37895</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">warm-salad-10</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/2a15f8bb\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/2a15f8bb</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_150009-2a15f8bb/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2a15f8bb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/3mhjtiio\" target=\"_blank\">elated-river-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_15-15:04:03] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:04:03] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:04:03] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 294ms/step - loss: 0.8277 - val_loss: 0.7845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.5852 - val_loss: 0.5718\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.5028 - val_loss: 0.4990\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4845 - val_loss: 0.4995\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4431 - val_loss: 0.4567\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4341 - val_loss: 0.4511\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4217 - val_loss: 0.4444\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4198 - val_loss: 0.4376\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4175 - val_loss: 0.4548\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4227 - val_loss: 0.4294\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 113ms/step - loss: 0.4097 - val_loss: 0.4280\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4098 - val_loss: 0.4234\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4161 - val_loss: 0.4662\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4006 - val_loss: 0.4087\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3974 - val_loss: 0.4091\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3816 - val_loss: 0.4395\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3877 - val_loss: 0.4055\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3898 - val_loss: 0.4267\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3884 - val_loss: 0.4626\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3746 - val_loss: 0.4000\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3730 - val_loss: 0.4016\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3875 - val_loss: 0.4031\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3838 - val_loss: 0.3999\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3901 - val_loss: 0.3954\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3968 - val_loss: 0.4305\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4057 - val_loss: 0.4116\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3780 - val_loss: 0.4177\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3757 - val_loss: 0.3966\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3721 - val_loss: 0.4108\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3697 - val_loss: 0.3904\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3514 - val_loss: 0.3874\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3591 - val_loss: 0.3881\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3630 - val_loss: 0.3910\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3619 - val_loss: 0.4010\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3602 - val_loss: 0.3866\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3619 - val_loss: 0.3870\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3630 - val_loss: 0.3927\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3638 - val_loss: 0.3865\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3625 - val_loss: 0.3899\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3662 - val_loss: 0.3881\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3555 - val_loss: 0.3874\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3537 - val_loss: 0.3872\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3541 - val_loss: 0.3874\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3569 - val_loss: 0.3877\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_15-15:05:08] Training the entire fine-tuned model...\n",
      "[2022_04_15-15:05:17] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3664WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1143s vs `on_train_batch_end` time: 0.1450s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1143s vs `on_train_batch_end` time: 0.1450s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 427ms/step - loss: 0.3788 - val_loss: 0.4072\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3651 - val_loss: 0.3821\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3397 - val_loss: 0.3816\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3504 - val_loss: 0.3789\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3407 - val_loss: 0.3826\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3472 - val_loss: 0.3767\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3331 - val_loss: 0.3707\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3340 - val_loss: 0.3751\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3389 - val_loss: 0.3981\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3329 - val_loss: 0.3718\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3258 - val_loss: 0.3702\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3210 - val_loss: 0.3719\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3236 - val_loss: 0.3717\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3273 - val_loss: 0.3692\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3289 - val_loss: 0.3689\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3117 - val_loss: 0.3683\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3140 - val_loss: 0.3687\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3227 - val_loss: 0.3690\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3168 - val_loss: 0.3690\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3101 - val_loss: 0.3701\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3147 - val_loss: 0.3673\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3046 - val_loss: 0.3670\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3154 - val_loss: 0.3679\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3115 - val_loss: 0.3685\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3097 - val_loss: 0.3667\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3086 - val_loss: 0.3671\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3085 - val_loss: 0.3696\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3054 - val_loss: 0.3676\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3174 - val_loss: 0.3673\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3041 - val_loss: 0.3674\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3058 - val_loss: 0.3674\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "[2022_04_15-15:06:54] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-15:06:54] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:06:54] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.2986WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1145s vs `on_train_batch_end` time: 0.1436s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1145s vs `on_train_batch_end` time: 0.1436s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 425ms/step - loss: 0.2999 - val_loss: 0.3721\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  92  2\n",
       "1  19  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3mhjtiio) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 7461... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▃▄▄▅▅▅▅▆▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>24</td></tr><tr><td>best_val_loss</td><td>0.36675</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.29995</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37205</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">elated-river-12</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/3mhjtiio\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/3mhjtiio</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_150348-3mhjtiio/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3mhjtiio). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/1oh19b80\" target=\"_blank\">elated-plasma-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_15-15:07:49] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:07:49] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:07:49] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 11s 296ms/step - loss: 0.7848 - val_loss: 0.6447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.5266 - val_loss: 0.4901\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4639 - val_loss: 0.4793\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4496 - val_loss: 0.4714\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4401 - val_loss: 0.4590\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4415 - val_loss: 0.4438\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4460 - val_loss: 0.4397\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4439 - val_loss: 0.4321\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4228 - val_loss: 0.5120\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4418 - val_loss: 0.4459\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4079 - val_loss: 0.4194\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4111 - val_loss: 0.4398\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4061 - val_loss: 0.4114\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4012 - val_loss: 0.4212\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4021 - val_loss: 0.4340\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4003 - val_loss: 0.4336\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3985 - val_loss: 0.4138\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3848 - val_loss: 0.4030\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3795 - val_loss: 0.4018\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3836 - val_loss: 0.4101\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3852 - val_loss: 0.4017\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3764 - val_loss: 0.4018\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3721 - val_loss: 0.4021\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.3702 - val_loss: 0.4016\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3762 - val_loss: 0.4009\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3757 - val_loss: 0.4016\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3715 - val_loss: 0.4011\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3633 - val_loss: 0.4012\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3768 - val_loss: 0.4009\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3726 - val_loss: 0.4004\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3733 - val_loss: 0.4007\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3716 - val_loss: 0.4012\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3693 - val_loss: 0.4010\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3580 - val_loss: 0.4011\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3661 - val_loss: 0.4009\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3659 - val_loss: 0.4009\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "[2022_04_15-15:08:45] Training the entire fine-tuned model...\n",
      "[2022_04_15-15:08:54] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.4098WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1138s vs `on_train_batch_end` time: 0.1460s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1138s vs `on_train_batch_end` time: 0.1460s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 427ms/step - loss: 0.3939 - val_loss: 0.3958\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3863 - val_loss: 0.3933\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3806 - val_loss: 0.3927\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3719 - val_loss: 0.3828\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.3485 - val_loss: 0.3784\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3643 - val_loss: 0.3899\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3532 - val_loss: 0.3831\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3405 - val_loss: 0.3714\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3299 - val_loss: 0.3905\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3236 - val_loss: 0.3765\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3261 - val_loss: 0.3639\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3091 - val_loss: 0.3668\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3033 - val_loss: 0.3779\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2843 - val_loss: 0.3744\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2960 - val_loss: 0.3717\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2838 - val_loss: 0.3687\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2882 - val_loss: 0.3682\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_15-15:09:50] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-15:09:50] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:09:50] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3191WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1147s vs `on_train_batch_end` time: 0.1469s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1147s vs `on_train_batch_end` time: 0.1469s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 431ms/step - loss: 0.3123 - val_loss: 0.3696\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  19  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4117647058823529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1oh19b80) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 7950... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▄▃▃▃▅▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_val_loss</td><td>0.36389</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31229</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.3696</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">elated-plasma-13</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/1oh19b80\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/1oh19b80</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_150732-1oh19b80/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1oh19b80). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/5ve9efob\" target=\"_blank\">pretty-sunset-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_15-15:10:45] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:10:45] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:10:45] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 11s 288ms/step - loss: 0.7877 - val_loss: 0.6235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.5436 - val_loss: 0.5044\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4831 - val_loss: 0.4821\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4596 - val_loss: 0.4665\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4459 - val_loss: 0.4518\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4500 - val_loss: 0.4451\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4605 - val_loss: 0.4474\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4533 - val_loss: 0.5045\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4236 - val_loss: 0.4346\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4244 - val_loss: 0.4447\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4135 - val_loss: 0.4258\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4237 - val_loss: 0.4732\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4006 - val_loss: 0.4169\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4058 - val_loss: 0.4391\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4050 - val_loss: 0.4573\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4024 - val_loss: 0.4260\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3860 - val_loss: 0.4114\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3863 - val_loss: 0.4064\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3793 - val_loss: 0.4015\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3701 - val_loss: 0.4032\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3788 - val_loss: 0.4043\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3821 - val_loss: 0.3988\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3860 - val_loss: 0.4009\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3739 - val_loss: 0.3991\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3777 - val_loss: 0.3982\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3805 - val_loss: 0.4031\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3773 - val_loss: 0.3966\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3785 - val_loss: 0.3992\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3822 - val_loss: 0.4085\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3720 - val_loss: 0.3979\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3812 - val_loss: 0.3961\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3622 - val_loss: 0.3983\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3699 - val_loss: 0.3959\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3742 - val_loss: 0.3970\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3721 - val_loss: 0.3966\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3720 - val_loss: 0.3963\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3699 - val_loss: 0.3957\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3690 - val_loss: 0.3963\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3640 - val_loss: 0.3968\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3703 - val_loss: 0.3958\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3658 - val_loss: 0.3958\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3764 - val_loss: 0.3959\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3737 - val_loss: 0.3959\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "[2022_04_15-15:11:49] Training the entire fine-tuned model...\n",
      "[2022_04_15-15:11:58] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.7044WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1156s vs `on_train_batch_end` time: 0.1457s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1156s vs `on_train_batch_end` time: 0.1457s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 428ms/step - loss: 0.5852 - val_loss: 0.4028\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.4228 - val_loss: 0.3934\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3777 - val_loss: 0.3874\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3835 - val_loss: 0.4050\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 263ms/step - loss: 0.4174 - val_loss: 0.4378\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.3685 - val_loss: 0.3829\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3653 - val_loss: 0.3788\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3423 - val_loss: 0.3809\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3364 - val_loss: 0.3602\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3167 - val_loss: 0.3619\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.2959 - val_loss: 0.3615\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2796 - val_loss: 0.3648\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2480 - val_loss: 0.3672\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2250 - val_loss: 0.3860\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2092 - val_loss: 0.4218\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_15-15:12:49] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-15:12:49] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:12:49] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3076WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1166s vs `on_train_batch_end` time: 0.1460s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1166s vs `on_train_batch_end` time: 0.1460s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 539ms/step - loss: 0.3049 - val_loss: 0.3639\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  18  8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45714285714285713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:5ve9efob) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8325... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▅▃▃▃▃▂▂▂▁▁▂</td></tr><tr><td>lr</td><td>███████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▅▃▃▄▃▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▁▂▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.36023</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30494</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.36395</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">pretty-sunset-14</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/5ve9efob\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/5ve9efob</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_151028-5ve9efob/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:5ve9efob). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/gknzs0q3\" target=\"_blank\">logical-moon-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_15-15:13:43] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:13:43] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:13:43] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 290ms/step - loss: 0.8528 - val_loss: 0.7081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.5688 - val_loss: 0.5597\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4918 - val_loss: 0.4820\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4518 - val_loss: 0.4693\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4398 - val_loss: 0.4552\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4357 - val_loss: 0.4504\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4283 - val_loss: 0.4570\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 113ms/step - loss: 0.4111 - val_loss: 0.4493\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4102 - val_loss: 0.4356\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4041 - val_loss: 0.4295\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4174 - val_loss: 0.4261\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4017 - val_loss: 0.4257\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4059 - val_loss: 0.4203\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3912 - val_loss: 0.4147\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4028 - val_loss: 0.4128\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3977 - val_loss: 0.4114\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4027 - val_loss: 0.4207\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3834 - val_loss: 0.4086\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3856 - val_loss: 0.4114\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4393 - val_loss: 0.4075\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4049 - val_loss: 0.4025\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3849 - val_loss: 0.4038\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3771 - val_loss: 0.4191\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4049 - val_loss: 0.3993\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3626 - val_loss: 0.4069\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3798 - val_loss: 0.3966\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3649 - val_loss: 0.4049\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3603 - val_loss: 0.3978\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3747 - val_loss: 0.4434\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3946 - val_loss: 0.3892\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3847 - val_loss: 0.3926\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3778 - val_loss: 0.4162\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3642 - val_loss: 0.3875\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3692 - val_loss: 0.4412\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 113ms/step - loss: 0.3939 - val_loss: 0.3927\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3587 - val_loss: 0.3901\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3494 - val_loss: 0.3898\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3620 - val_loss: 0.3947\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3614 - val_loss: 0.3899\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3620 - val_loss: 0.3920\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3678 - val_loss: 0.3876\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_15-15:14:44] Training the entire fine-tuned model...\n",
      "[2022_04_15-15:14:54] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3685WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1142s vs `on_train_batch_end` time: 0.1442s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1142s vs `on_train_batch_end` time: 0.1442s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 429ms/step - loss: 0.3500 - val_loss: 0.3962\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3630 - val_loss: 0.3865\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3530 - val_loss: 0.3893\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 0.3555 - val_loss: 0.3874\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3540 - val_loss: 0.3874\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3520 - val_loss: 0.3872\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3469 - val_loss: 0.3866\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3430 - val_loss: 0.3861\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3524 - val_loss: 0.3863\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3558 - val_loss: 0.3864\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3485 - val_loss: 0.3854\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3515 - val_loss: 0.3857\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3515 - val_loss: 0.3876\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3482 - val_loss: 0.3870\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3587 - val_loss: 0.3847\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3582 - val_loss: 0.3858\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3505 - val_loss: 0.3859\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3511 - val_loss: 0.3864\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3469 - val_loss: 0.3871\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3538 - val_loss: 0.3868\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3448 - val_loss: 0.3863\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3538 - val_loss: 0.3858\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3407 - val_loss: 0.3854\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "[2022_04_15-15:16:08] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-15:16:08] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:16:22] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3404WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1136s vs `on_train_batch_end` time: 0.1473s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1136s vs `on_train_batch_end` time: 0.1473s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 545ms/step - loss: 0.3493 - val_loss: 0.3845\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  21  5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:gknzs0q3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8728... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▄▄▄▅▁</td></tr><tr><td>loss</td><td>█▄▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████████▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.38452</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34929</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.38452</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">logical-moon-15</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/gknzs0q3\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/gknzs0q3</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_151327-gknzs0q3/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:gknzs0q3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/3j8vlelm\" target=\"_blank\">kind-serenity-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_15-15:17:17] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:17:17] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:17:17] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 287ms/step - loss: 0.8497 - val_loss: 0.9034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.5952 - val_loss: 0.5578\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4940 - val_loss: 0.4785\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4798 - val_loss: 0.5324\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4946 - val_loss: 0.5045\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4619 - val_loss: 0.5121\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4456 - val_loss: 0.4449\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4217 - val_loss: 0.4361\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4272 - val_loss: 0.4370\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4086 - val_loss: 0.4272\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3959 - val_loss: 0.4460\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4005 - val_loss: 0.4193\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3980 - val_loss: 0.4172\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4087 - val_loss: 0.4207\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4005 - val_loss: 0.4172\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4044 - val_loss: 0.4271\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4002 - val_loss: 0.4126\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3939 - val_loss: 0.4083\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3872 - val_loss: 0.4079\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3833 - val_loss: 0.4152\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3948 - val_loss: 0.4060\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3890 - val_loss: 0.4111\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3750 - val_loss: 0.4401\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4211 - val_loss: 0.5040\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3824 - val_loss: 0.3939\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3975 - val_loss: 0.4189\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4098 - val_loss: 0.5248\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3833 - val_loss: 0.3936\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3864 - val_loss: 0.4196\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3735 - val_loss: 0.3966\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3656 - val_loss: 0.3854\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3809 - val_loss: 0.4092\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3762 - val_loss: 0.3969\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4207 - val_loss: 0.4420\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4305 - val_loss: 0.3983\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3683 - val_loss: 0.4063\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3556 - val_loss: 0.4052\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3747 - val_loss: 0.3847\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3574 - val_loss: 0.3840\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3582 - val_loss: 0.3975\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3574 - val_loss: 0.3940\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3594 - val_loss: 0.3928\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3611 - val_loss: 0.3839\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3630 - val_loss: 0.3854\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.3550 - val_loss: 0.3871\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3728 - val_loss: 0.3856\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3496 - val_loss: 0.3859\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3494 - val_loss: 0.3857\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3537 - val_loss: 0.3869\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3525 - val_loss: 0.3867\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3490 - val_loss: 0.3863\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_15-15:18:31] Training the entire fine-tuned model...\n",
      "[2022_04_15-15:18:57] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3447WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1119s vs `on_train_batch_end` time: 0.1476s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1119s vs `on_train_batch_end` time: 0.1476s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 433ms/step - loss: 0.3644 - val_loss: 0.3862\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3565 - val_loss: 0.3843\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3584 - val_loss: 0.3881\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3499 - val_loss: 0.3792\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3436 - val_loss: 0.3864\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3392 - val_loss: 0.3938\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3507 - val_loss: 0.4045\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.3386 - val_loss: 0.3769\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 0.3355 - val_loss: 0.3730\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3278 - val_loss: 0.3742\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3175 - val_loss: 0.3744\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3267 - val_loss: 0.3727\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3093 - val_loss: 0.3922\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3221 - val_loss: 0.4002\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3112 - val_loss: 0.3820\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3073 - val_loss: 0.3692\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3012 - val_loss: 0.3803\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.2995 - val_loss: 0.3756\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.2960 - val_loss: 0.3662\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.2916 - val_loss: 0.3670\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2917 - val_loss: 0.3735\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2865 - val_loss: 0.3634\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2838 - val_loss: 0.3792\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2913 - val_loss: 0.4171\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2824 - val_loss: 0.4114\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2782 - val_loss: 0.3760\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2699 - val_loss: 0.3659\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.2654 - val_loss: 0.3706\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2574 - val_loss: 0.3665\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2584 - val_loss: 0.3722\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_15-15:20:31] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-15:20:31] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:20:31] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.2816WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1126s vs `on_train_batch_end` time: 0.1486s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1126s vs `on_train_batch_end` time: 0.1486s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 438ms/step - loss: 0.2853 - val_loss: 0.3657\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  90   4\n",
       "1  16  10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3j8vlelm) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9158... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▃▃▃▃▄▄▄▅▅▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▃▃▃▃▂▃▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▂▂▂▂▂▂▂▂▂▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>21</td></tr><tr><td>best_val_loss</td><td>0.36335</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.28526</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.36567</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">kind-serenity-16</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/3j8vlelm\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/3j8vlelm</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_151701-3j8vlelm/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3j8vlelm). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/1azrrqg7\" target=\"_blank\">grateful-thunder-17</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_15-15:21:25] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:21:25] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:21:25] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 288ms/step - loss: 0.8104 - val_loss: 0.5915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 116ms/step - loss: 0.5080 - val_loss: 0.4967\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4933 - val_loss: 0.4771\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4499 - val_loss: 0.4649\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4446 - val_loss: 0.4875\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4642 - val_loss: 0.4796\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4450 - val_loss: 0.4536\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4551 - val_loss: 0.4320\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4113 - val_loss: 0.4396\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4132 - val_loss: 0.4250\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4088 - val_loss: 0.4265\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4210 - val_loss: 0.4349\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4169 - val_loss: 0.4122\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4378 - val_loss: 0.4113\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4294 - val_loss: 0.4850\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4380 - val_loss: 0.4179\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3985 - val_loss: 0.4600\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3916 - val_loss: 0.4074\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3803 - val_loss: 0.4282\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3917 - val_loss: 0.4030\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3769 - val_loss: 0.3994\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3932 - val_loss: 0.4298\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4184 - val_loss: 0.4095\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3925 - val_loss: 0.4070\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3873 - val_loss: 0.4021\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3722 - val_loss: 0.4075\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3646 - val_loss: 0.3956\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3729 - val_loss: 0.3948\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3647 - val_loss: 0.3949\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3659 - val_loss: 0.3952\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3644 - val_loss: 0.3982\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3618 - val_loss: 0.3984\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3598 - val_loss: 0.3947\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3630 - val_loss: 0.3941\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3644 - val_loss: 0.3969\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3650 - val_loss: 0.3938\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3575 - val_loss: 0.3971\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3601 - val_loss: 0.3948\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3633 - val_loss: 0.3961\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3620 - val_loss: 0.3935\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3606 - val_loss: 0.3981\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3688 - val_loss: 0.3946\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3633 - val_loss: 0.3942\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3662 - val_loss: 0.3946\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3636 - val_loss: 0.3949\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3697 - val_loss: 0.3944\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3602 - val_loss: 0.3949\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3670 - val_loss: 0.3950\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_15-15:22:35] Training the entire fine-tuned model...\n",
      "[2022_04_15-15:22:44] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3987WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1137s vs `on_train_batch_end` time: 0.1469s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1137s vs `on_train_batch_end` time: 0.1469s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 426ms/step - loss: 0.3929 - val_loss: 0.3915\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3789 - val_loss: 0.3896\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3595 - val_loss: 0.3896\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3632 - val_loss: 0.3874\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3423 - val_loss: 0.3825\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3424 - val_loss: 0.3803\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3346 - val_loss: 0.3797\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3455 - val_loss: 0.3730\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3459 - val_loss: 0.3955\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3272 - val_loss: 0.3709\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3203 - val_loss: 0.3693\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3205 - val_loss: 0.3669\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2997 - val_loss: 0.3731\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3148 - val_loss: 0.3717\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2924 - val_loss: 0.3812\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.2988 - val_loss: 0.3923\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3008 - val_loss: 0.3754\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2854 - val_loss: 0.3779\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2750 - val_loss: 0.3652\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2726 - val_loss: 0.3672\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2578 - val_loss: 0.3677\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2669 - val_loss: 0.3677\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2596 - val_loss: 0.3679\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.2458 - val_loss: 0.3712\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 0.2571 - val_loss: 0.3712\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2484 - val_loss: 0.3696\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2618 - val_loss: 0.3736\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "[2022_04_15-15:24:10] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-15:24:10] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:24:12] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.2608WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1141s vs `on_train_batch_end` time: 0.1461s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1141s vs `on_train_batch_end` time: 0.1461s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 427ms/step - loss: 0.2705 - val_loss: 0.3720\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  91   3\n",
       "1  16  10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5128205128205128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1azrrqg7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9734... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇██▁▁▁▂▂▂▃▃▃▄▄▄▅▅▁</td></tr><tr><td>loss</td><td>█▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▄▅▃▃▃▂▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>18</td></tr><tr><td>best_val_loss</td><td>0.36521</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.27045</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.37203</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">grateful-thunder-17</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/1azrrqg7\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/1azrrqg7</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_152109-1azrrqg7/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1azrrqg7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/30skiikj\" target=\"_blank\">apricot-dragon-18</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_15-15:25:08] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:25:08] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:25:08] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 292ms/step - loss: 0.8022 - val_loss: 0.6105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.5308 - val_loss: 0.5151\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4713 - val_loss: 0.4786\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4585 - val_loss: 0.4622\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4384 - val_loss: 0.4639\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4365 - val_loss: 0.4458\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4301 - val_loss: 0.5041\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4568 - val_loss: 0.4528\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4152 - val_loss: 0.4562\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4044 - val_loss: 0.4256\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4026 - val_loss: 0.4286\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4140 - val_loss: 0.4207\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4122 - val_loss: 0.4189\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3959 - val_loss: 0.4144\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3936 - val_loss: 0.4153\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3846 - val_loss: 0.4568\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3998 - val_loss: 0.4169\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3893 - val_loss: 0.4033\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3821 - val_loss: 0.4016\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3879 - val_loss: 0.4040\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3768 - val_loss: 0.4027\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3793 - val_loss: 0.4098\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3881 - val_loss: 0.4005\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3997 - val_loss: 0.4137\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4236 - val_loss: 0.4698\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4013 - val_loss: 0.4474\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4242 - val_loss: 0.5761\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4293 - val_loss: 0.3927\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3691 - val_loss: 0.4026\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3652 - val_loss: 0.4107\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3602 - val_loss: 0.3917\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3734 - val_loss: 0.3976\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3669 - val_loss: 0.3893\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3527 - val_loss: 0.3883\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3738 - val_loss: 0.3892\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3562 - val_loss: 0.3877\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3538 - val_loss: 0.3883\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3586 - val_loss: 0.3886\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3484 - val_loss: 0.3883\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3697 - val_loss: 0.3969\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3678 - val_loss: 0.3886\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3523 - val_loss: 0.3947\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3530 - val_loss: 0.3890\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3553 - val_loss: 0.3898\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_15-15:26:12] Training the entire fine-tuned model...\n",
      "[2022_04_15-15:26:21] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 1.0330WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1110s vs `on_train_batch_end` time: 0.1471s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1110s vs `on_train_batch_end` time: 0.1471s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 426ms/step - loss: 0.7918 - val_loss: 0.3976\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.4121 - val_loss: 0.3941\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 0.4068 - val_loss: 0.4007\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3922 - val_loss: 0.3810\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.3726 - val_loss: 0.3808\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3529 - val_loss: 0.3783\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3494 - val_loss: 0.3975\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3348 - val_loss: 0.3681\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3143 - val_loss: 0.3715\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3380 - val_loss: 0.3858\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3073 - val_loss: 0.3953\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.2746 - val_loss: 0.3685\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 264ms/step - loss: 0.2365 - val_loss: 0.3736\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.2225 - val_loss: 0.3818\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2131 - val_loss: 0.3906\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2003 - val_loss: 0.4062\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_15-15:27:15] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-15:27:15] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:27:15] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3056WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1158s vs `on_train_batch_end` time: 0.1439s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1158s vs `on_train_batch_end` time: 0.1439s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 425ms/step - loss: 0.3067 - val_loss: 0.3806\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  18  8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45714285714285713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "for pat in patience:\n",
    "    for lr in learning_rate:\n",
    "        finetune_by_settings_and_data(pat, lr, f\"Full old\", train_data_old, valid_data_old, f\"old_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310c43cb-2358-40c5-b3b6-2027bfc4106a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "559e98e6-c31c-4650-9886-ad9dfc11f0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_15-12:45:47] Test set: Filtered out 0 of 260 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "encoded_test_set = encode_dataset(test_data[\"seq\"], test_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4cc4820c-1011-4813-9e0e-397cfa976103",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X, test_Y, test_sample_weigths = encoded_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c5e115f7-c70f-41df-b3a3-a0da075721c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_15-15:27:54] Test set old: Filtered out 0 of 119 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "encoded_test_set_old = encode_dataset(test_data_old[\"seq\"], test_data_old[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Test set old')\n",
    "test_X_old, test_Y_old, test_sample_weigths_old = encoded_test_set_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288f2631-ab65-4dea-a81d-871aeea43932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c197e30e-5ddf-453c-ad43-397bc6f1df66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for model in os.listdir(path.join(DATA_DIR, f\"protein_bert/full\")):\n",
    "    if model.startswith(\"2022_04_09\"):\n",
    "        test_model(model, f\"full\", test_X, test_Y, tap_X, tap_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8eedd51e-cb7d-4e6f-aa6d-6f8c6f8daf5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10224... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model in os.listdir(path.join(DATA_DIR, f\"protein_bert/old_full\")):\n",
    "    if model.startswith(\"2022_04_09\"):\n",
    "        test_model(model, f\"old_full\", test_X_old, test_Y_old, tap_X, tap_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ef191c63-a713-48fe-a41e-16f8ab7fe56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"2022_04_09_6_3_0.0001\"\n",
    "model_path = path.join(DATA_DIR, f\"protein_bert/full/{model_name}\")\n",
    "model = keras.models.load_model(model_path)\n",
    "parts = model_name[11:].split(\"_\")\n",
    "patience_stop = parts[0]\n",
    "patience_lr = parts[1]\n",
    "lr = parts[2]\n",
    "\n",
    "y_pred = model.predict(test_X, batch_size=32)\n",
    "y_pred_classes = (y_pred >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c783e834-4cf9-4049-a9d2-662d35b7bee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15384615384615383"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(test_Y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9e3db90a-6ef4-4e06-b6fe-0454584a2dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[200,   6],\n",
       "       [ 49,   5]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(test_Y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8543f4a1-2176-4479-b0f7-30fbfd54df04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1291"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e0b6ff60-cf54-4985-8f9e-82fe5a31a5bc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:344ub7bf) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3731... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█▁▁▂▃▃▃▄▅▅▅▆▇▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▂▁▂▃▁▄▃▂▁▂▃▂▂▃▁▁▁▂▁▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.45819</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35927</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.48604</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">still-butterfly-5</strong>: <a href=\"https://wandb.ai/kvetab/Easter/runs/344ub7bf\" target=\"_blank\">https://wandb.ai/kvetab/Easter/runs/344ub7bf</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_142948-344ub7bf/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:344ub7bf). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Easter/runs/rc09kp9o\" target=\"_blank\">rosy-cloud-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Easter\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_15-14:33:47] Training set: Filtered out 0 of 1291 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:33:48] Validation set: Filtered out 0 of 130 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:33:48] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 281ms/step - loss: 0.7999 - val_loss: 0.8898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.5859 - val_loss: 0.6600\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.5152 - val_loss: 0.5327\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4500 - val_loss: 0.5070\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4278 - val_loss: 0.4914\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4082 - val_loss: 0.5343\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.4290 - val_loss: 0.5193\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4161 - val_loss: 0.5199\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4100 - val_loss: 0.4779\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4006 - val_loss: 0.4866\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3921 - val_loss: 0.4959\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3946 - val_loss: 0.4784\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3938 - val_loss: 0.4817\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3883 - val_loss: 0.4907\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3903 - val_loss: 0.4741\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3859 - val_loss: 0.4760\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3910 - val_loss: 0.4913\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3948 - val_loss: 0.4747\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3923 - val_loss: 0.4738\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3854 - val_loss: 0.4771\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3881 - val_loss: 0.4790\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3888 - val_loss: 0.4770\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3918 - val_loss: 0.4763\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3905 - val_loss: 0.4754\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3873 - val_loss: 0.4762\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "[2022_04_15-14:34:29] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:34:45] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.4002WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1148s vs `on_train_batch_end` time: 0.1441s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1148s vs `on_train_batch_end` time: 0.1441s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 418ms/step - loss: 0.4045 - val_loss: 0.4907\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 247ms/step - loss: 0.3969 - val_loss: 0.4714\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3894 - val_loss: 0.4817\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 248ms/step - loss: 0.3789 - val_loss: 0.4876\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3825 - val_loss: 0.4609\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 248ms/step - loss: 0.3693 - val_loss: 0.4934\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3705 - val_loss: 0.5067\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3528 - val_loss: 0.4917\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3563 - val_loss: 0.4596\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 248ms/step - loss: 0.3530 - val_loss: 0.4644\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 248ms/step - loss: 0.3520 - val_loss: 0.4660\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3386 - val_loss: 0.4575\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3348 - val_loss: 0.4666\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3422 - val_loss: 0.4549\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3419 - val_loss: 0.4625\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3339 - val_loss: 0.4622\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 250ms/step - loss: 0.3254 - val_loss: 0.4636\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3271 - val_loss: 0.4599\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 250ms/step - loss: 0.3225 - val_loss: 0.4595\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3249 - val_loss: 0.4624\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "[2022_04_15-14:35:49] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:35:49] Training set: Filtered out 0 of 1291 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:35:49] Validation set: Filtered out 0 of 130 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3351WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1126s vs `on_train_batch_end` time: 0.1469s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1126s vs `on_train_batch_end` time: 0.1469s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 418ms/step - loss: 0.3270 - val_loss: 0.4616\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"Easter\", entity=\"kvetab\")\n",
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "    get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
    "training_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience = 3, factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "    keras.callbacks.EarlyStopping(patience = 6, restore_best_weights = True),\n",
    "    WandbCallback()\n",
    "]\n",
    "epoch_num = 100\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "wandb.config = {\n",
    "      \"learning_rate\": lr,\n",
    "      \"epochs\": epoch_num * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }\n",
    "print(type(lr))\n",
    "print(lr)\n",
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], valid_data['seq'], valid_data['Y'], \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = lr, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 512, final_lr = lr / 10, callbacks = training_callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a4565283-afc4-452e-a7bf-edf3343d243f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  104  2\n",
       "1   21  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "44a664fb-2244-4ead-836c-849c0c33179f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20689655172413793"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "26b31976-7e75-40d7-bfcc-aeb4a46995ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1291"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "377ee2c0-1ef8-42c7-a8c6-494d0514daf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "49746582-eab0-4f59-8411-ca38d6f35c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e26aa-4435-49dd-958d-0318b364b644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
