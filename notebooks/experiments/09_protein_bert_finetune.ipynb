{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff8a9d94-4216-4b46-92e0-67298b77bf45",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "\n",
    "from proteinbert import OutputType, OutputSpec, FinetuningModelGenerator, load_pretrained_model, \\\n",
    "finetune, evaluate_by_len\n",
    "from proteinbert.finetuning import encode_train_and_valid_sets\n",
    "from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs\n",
    "from os import path\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d2ed9b0-3023-4f0c-b93e-3330a86f35ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4798e75-9168-4aa2-b76c-bf37d692efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33c6481c-b3ce-47c3-86c4-5258a9553fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5db7bda-d751-4876-916f-23e824fd7b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TYPE = OutputType(False, 'binary')\n",
    "UNIQUE_LABELS = [0, 1]\n",
    "OUTPUT_SPEC = OutputSpec(OUTPUT_TYPE, UNIQUE_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78d4d0ff-52b0-4152-9447-182709e45923",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_generator, input_encoder = load_pretrained_model(\"../../data/protein_bert/\", \"epoch_92400_sample_23500000.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59ffc4b8-405d-4244-a4bc-f0ab0190c73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbdaf9de-0881-43dc-8d2e-317db6302948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkvetab\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/OutOfTheBox/runs/151ha3ml\" target=\"_blank\">summer-paper-2</a></strong> to <a href=\"https://wandb.ai/kvetab/OutOfTheBox\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/kvetab/OutOfTheBox/runs/151ha3ml?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fd3f8b94a50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=f\"OutOfTheBox\", entity=\"kvetab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1743c445-302b-4f4e-9325-a6ef4fb17aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antibody_ID</th>\n",
       "      <th>heavy</th>\n",
       "      <th>light</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>6aod</td>\n",
       "      <td>EVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYMHWVRQAPGQGLE...</td>\n",
       "      <td>DIVMTKSPSSLSASVGDRVTITCRASQGIRNDLGWYQQKPGKAPKR...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>4yny</td>\n",
       "      <td>EVQLVESGGGLVQPGRSLKLSCAASGFTFSNYGMAWVRQTPTKGLE...</td>\n",
       "      <td>EFVLTQPNSVSTNLGSTVKLSCKRSTGNIGSNYVNWYQQHEGRSPT...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>5xcv</td>\n",
       "      <td>EVQLVESGGGLVQPGRSLKLSCAASGFTFSNYGMAWVRQTPTKGLE...</td>\n",
       "      <td>QFVLTQPNSVSTNLGSTVKLSCKRSTGNIGSNYVNWYQQHEGRSPT...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>6and</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGYEFSRSWMNWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRSSQSIVHSVGNTFLEWYQQKPG...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>2xqy</td>\n",
       "      <td>QVQLQQPGAELVKPGASVKMSCKASGYSFTSYWMNWVKQRPGRGLE...</td>\n",
       "      <td>DIVLTQSPASLALSLGQRATISCRASKSVSTSGYSYMYWYQQKPGQ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Antibody_ID                                              heavy  \\\n",
       "2073        6aod  EVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYMHWVRQAPGQGLE...   \n",
       "1517        4yny  EVQLVESGGGLVQPGRSLKLSCAASGFTFSNYGMAWVRQTPTKGLE...   \n",
       "2025        5xcv  EVQLVESGGGLVQPGRSLKLSCAASGFTFSNYGMAWVRQTPTKGLE...   \n",
       "2070        6and  EVQLVESGGGLVQPGGSLRLSCAASGYEFSRSWMNWVRQAPGKGLE...   \n",
       "666         2xqy  QVQLQQPGAELVKPGASVKMSCKASGYSFTSYWMNWVKQRPGRGLE...   \n",
       "\n",
       "                                                  light  Y  \n",
       "2073  DIVMTKSPSSLSASVGDRVTITCRASQGIRNDLGWYQQKPGKAPKR...  0  \n",
       "1517  EFVLTQPNSVSTNLGSTVKLSCKRSTGNIGSNYVNWYQQHEGRSPT...  1  \n",
       "2025  QFVLTQPNSVSTNLGSTVKLSCKRSTGNIGSNYVNWYQQHEGRSPT...  1  \n",
       "2070  DIQMTQSPSSLSASVGDRVTITCRSSQSIVHSVGNTFLEWYQQKPG...  1  \n",
       "666   DIVLTQSPASLALSLGQRATISCRASKSVSTSGYSYMYWYQQKPGQ...  0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_train_data.csv\"), index_col=0)\n",
    "valid_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_valid_data.csv\"), index_col=0)\n",
    "test_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_test_data.csv\"), index_col=0)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c09b813-ffc8-48f7-99a5-586c76ac33f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"seq\"] = train_data[\"heavy\"] + train_data[\"light\"]\n",
    "valid_data[\"seq\"] = valid_data[\"heavy\"] + valid_data[\"light\"]\n",
    "test_data[\"seq\"] = test_data[\"heavy\"] + test_data[\"light\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fe63df3-96bf-40ed-9065-216a8e67b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience = 1, factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "    keras.callbacks.EarlyStopping(patience = 3, restore_best_weights = True),\n",
    "    WandbCallback()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faeffd00-5c33-40ed-ad02-1c71cb739527",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 50\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd45319f-0dd8-489a-b105-7163d1679298",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config = {\n",
    "      \"learning_rate\": learning_rate,\n",
    "      \"epochs\": epoch_num * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72bd9e53-8469-410c-9d42-6daa3811fc7e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_02_02-14:06:32] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_02_02-14:06:33] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_02_02-14:06:33] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-02 14:06:33.430286: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-02 14:06:34.315854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9656 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "2022-02-02 14:06:36.367547: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-02 14:06:45.738106: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 7605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 13s 316ms/step - loss: 0.7434 - val_loss: 0.5545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.5535 - val_loss: 0.5220\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.4747 - val_loss: 0.4843\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 1s 102ms/step - loss: 0.4496 - val_loss: 0.4654\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.4369 - val_loss: 0.4678\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.4360 - val_loss: 0.4542\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.4310 - val_loss: 0.4475\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4273 - val_loss: 0.4496\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.4257 - val_loss: 0.4452\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.4197 - val_loss: 0.4447\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.4247 - val_loss: 0.4441\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.4174 - val_loss: 0.4438\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 1s 102ms/step - loss: 0.4177 - val_loss: 0.4438\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.4210 - val_loss: 0.4435\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.4180 - val_loss: 0.4431\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.4141 - val_loss: 0.4430\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.4186 - val_loss: 0.4429\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4149 - val_loss: 0.4429\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.4173 - val_loss: 0.4428\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 20/50\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.4227 - val_loss: 0.4428\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 21/50\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.4159 - val_loss: 0.4428\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "Epoch 22/50\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4179 - val_loss: 0.4428\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "Epoch 23/50\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4170 - val_loss: 0.4428\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 24/50\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.4167 - val_loss: 0.4428\n",
      "Epoch 25/50\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.4195 - val_loss: 0.4428\n",
      "Epoch 26/50\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.4134 - val_loss: 0.4428\n",
      "Epoch 27/50\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.4201 - val_loss: 0.4428\n",
      "Epoch 28/50\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.4136 - val_loss: 0.4428\n",
      "Epoch 29/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4202 - val_loss: 0.4428\n",
      "Epoch 30/50\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.4193 - val_loss: 0.4428\n",
      "Epoch 31/50\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.4160 - val_loss: 0.4428\n",
      "Epoch 32/50\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4188 - val_loss: 0.4428\n",
      "Epoch 33/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4207 - val_loss: 0.4428\n",
      "Epoch 34/50\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4195 - val_loss: 0.4428\n",
      "Epoch 35/50\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.4222 - val_loss: 0.4428\n",
      "Epoch 36/50\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.4182 - val_loss: 0.4428\n",
      "Epoch 37/50\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.4181 - val_loss: 0.4428\n",
      "Epoch 38/50\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.4191 - val_loss: 0.4428\n",
      "Epoch 39/50\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.4158 - val_loss: 0.4428\n",
      "Epoch 40/50\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.4150 - val_loss: 0.4428\n",
      "Epoch 41/50\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.4209 - val_loss: 0.4428\n",
      "Epoch 42/50\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.4216 - val_loss: 0.4428\n",
      "Epoch 43/50\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.4188 - val_loss: 0.4428\n",
      "Epoch 44/50\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4223 - val_loss: 0.4428\n",
      "Epoch 45/50\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.4225 - val_loss: 0.4428\n",
      "Epoch 46/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4174 - val_loss: 0.4428\n",
      "Epoch 47/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4162 - val_loss: 0.4428\n",
      "Epoch 48/50\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.4152 - val_loss: 0.4428\n",
      "Epoch 49/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4206 - val_loss: 0.4428\n",
      "Epoch 50/50\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.4162 - val_loss: 0.4428\n",
      "[2022_02_02-14:08:06] Training the entire fine-tuned model...\n",
      "[2022_02_02-14:08:15] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/50\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.4368WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1091s vs `on_train_batch_end` time: 0.1425s). Check your callbacks.\n",
      "11/11 [==============================] - 11s 438ms/step - loss: 0.4220 - val_loss: 0.4412\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.4259 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.4170 - val_loss: 0.4363\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.4136 - val_loss: 0.4339\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.4053 - val_loss: 0.4320\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.4077 - val_loss: 0.4309\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.4046 - val_loss: 0.4302\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3971 - val_loss: 0.4284\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 0.4001 - val_loss: 0.4280\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 0.3957 - val_loss: 0.4256\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3912 - val_loss: 0.4247\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3977 - val_loss: 0.4247\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3976 - val_loss: 0.4228\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3837 - val_loss: 0.4224\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3921 - val_loss: 0.4221\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3867 - val_loss: 0.4217\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3836 - val_loss: 0.4215\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3879 - val_loss: 0.4211\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3863 - val_loss: 0.4209\n",
      "Epoch 20/50\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3846 - val_loss: 0.4203\n",
      "Epoch 21/50\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3831 - val_loss: 0.4199\n",
      "Epoch 22/50\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3861 - val_loss: 0.4198\n",
      "Epoch 23/50\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3900 - val_loss: 0.4192\n",
      "Epoch 24/50\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.3866 - val_loss: 0.4189\n",
      "Epoch 25/50\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3815 - val_loss: 0.4185\n",
      "Epoch 26/50\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3833 - val_loss: 0.4186\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "Epoch 27/50\n",
      "11/11 [==============================] - 3s 264ms/step - loss: 0.3782 - val_loss: 0.4183\n",
      "Epoch 28/50\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3795 - val_loss: 0.4182\n",
      "Epoch 29/50\n",
      "11/11 [==============================] - 3s 264ms/step - loss: 0.3787 - val_loss: 0.4182\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "Epoch 30/50\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3831 - val_loss: 0.4182\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 31/50\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3809 - val_loss: 0.4182\n",
      "[2022_02_02-14:09:53] Training on final epochs of sequence length 1024...\n",
      "[2022_02_02-14:09:53] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_02_02-14:09:53] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 1022.\n",
      " 6/21 [=======>......................] - ETA: 3s - loss: 0.3633WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1102s vs `on_train_batch_end` time: 0.1404s). Check your callbacks.\n",
      "21/21 [==============================] - 14s 381ms/step - loss: 0.4003 - val_loss: 0.4238\n"
     ]
    }
   ],
   "source": [
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], valid_data['seq'], valid_data['Y'], \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f7e7d4f-4ca7-47fd-ad0f-cedd9de6c46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d14dac3-1751-4172-96e6-7636bb9ad8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set performance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># records</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model seq len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>119</td>\n",
       "      <td>0.917346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>119</td>\n",
       "      <td>0.917346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               # records       AUC\n",
       "Model seq len                     \n",
       "512                  119  0.917346\n",
       "All                  119  0.917346"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  87   9\n",
       "1   9  14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Test-set performance:')\n",
    "display(results)\n",
    "\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "601cc41a-442a-4b5a-a651-a22f09e5ad2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6086956521739131"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "019ca4b0-682c-4b60-bbc8-fb856f0ecd17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix.loc[\"1\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c444b8d-daf7-4e24-a86c-c97f47b80d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "2022-02-02 14:15:32.228177: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/batch_128_lr_1e-4_2022_02_02.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/batch_128_lr_1e-4_2022_02_02.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "mod = model_generator.create_model(seq_len = 512)\n",
    "mod.save(path.join(DATA_DIR, \"protein_bert/batch_128_lr_1e-4_2022_02_02.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1403d28-4a33-4de8-a3f7-a4cc50ce074d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a28a6b4-2cfe-4604-bde0-0b293d2378aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf. __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f85b7eba-b91b-4e09-b8c7-b1fbf7569ab6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_02_02-15:34:41] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_02_02-15:34:41] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_02_02-15:34:41] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "11/11 [==============================] - 11s 301ms/step - loss: 0.5757 - val_loss: 0.6380\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.4577 - val_loss: 0.5868\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.4564 - val_loss: 0.4170\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3867 - val_loss: 0.4442\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.3716 - val_loss: 0.4248\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 1s 113ms/step - loss: 0.3668 - val_loss: 0.4062\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 1s 113ms/step - loss: 0.3562 - val_loss: 0.4082\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3520 - val_loss: 0.4064\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3504 - val_loss: 0.4061\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3610 - val_loss: 0.4059\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.3545 - val_loss: 0.4057\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.3593 - val_loss: 0.4056\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.3522 - val_loss: 0.4056\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.3577 - val_loss: 0.4056\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3508 - val_loss: 0.4056\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3620 - val_loss: 0.4056\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3498 - val_loss: 0.4056\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3514 - val_loss: 0.4056\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3598 - val_loss: 0.4056\n",
      "Epoch 20/50\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3541 - val_loss: 0.4056\n",
      "Epoch 21/50\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.3566 - val_loss: 0.4056\n",
      "Epoch 22/50\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.3544 - val_loss: 0.4056\n",
      "Epoch 23/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3538 - val_loss: 0.4056\n",
      "Epoch 24/50\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3569 - val_loss: 0.4056\n",
      "Epoch 25/50\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.3522 - val_loss: 0.4056\n",
      "Epoch 26/50\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3519 - val_loss: 0.4056\n",
      "Epoch 27/50\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3635 - val_loss: 0.4056\n",
      "Epoch 28/50\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3518 - val_loss: 0.4056\n",
      "Epoch 29/50\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.3579 - val_loss: 0.4056\n",
      "Epoch 30/50\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3486 - val_loss: 0.4056\n",
      "Epoch 31/50\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3566 - val_loss: 0.4056\n",
      "Epoch 32/50\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3512 - val_loss: 0.4056\n",
      "Epoch 33/50\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3576 - val_loss: 0.4056\n",
      "Epoch 34/50\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3477 - val_loss: 0.4056\n",
      "Epoch 35/50\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.3489 - val_loss: 0.4056\n",
      "Epoch 36/50\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.3534 - val_loss: 0.4056\n",
      "Epoch 37/50\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3587 - val_loss: 0.4056\n",
      "Epoch 38/50\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.3553 - val_loss: 0.4056\n",
      "Epoch 39/50\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3521 - val_loss: 0.4056\n",
      "Epoch 40/50\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3589 - val_loss: 0.4056\n",
      "Epoch 41/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3546 - val_loss: 0.4056\n",
      "Epoch 42/50\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3516 - val_loss: 0.4056\n",
      "Epoch 43/50\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.3559 - val_loss: 0.4056\n",
      "Epoch 44/50\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3639 - val_loss: 0.4056\n",
      "Epoch 45/50\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3475 - val_loss: 0.4056\n",
      "Epoch 46/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3572 - val_loss: 0.4056\n",
      "Epoch 47/50\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3586 - val_loss: 0.4056\n",
      "Epoch 48/50\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.3542 - val_loss: 0.4056\n",
      "Epoch 49/50\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3592 - val_loss: 0.4056\n",
      "Epoch 50/50\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3589 - val_loss: 0.4056\n",
      "[2022_02_02-15:36:13] Training the entire fine-tuned model...\n",
      "[2022_02_02-15:36:22] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/50\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3741WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1084s vs `on_train_batch_end` time: 0.1504s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1084s vs `on_train_batch_end` time: 0.1504s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 439ms/step - loss: 0.3575 - val_loss: 0.4049\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3539 - val_loss: 0.4036\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3470 - val_loss: 0.4031\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3518 - val_loss: 0.4012\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3431 - val_loss: 0.4008\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3508 - val_loss: 0.4002\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3405 - val_loss: 0.4001\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 0.3433 - val_loss: 0.4000\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3462 - val_loss: 0.4000\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3349 - val_loss: 0.3999\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 0.3416 - val_loss: 0.3999\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3418 - val_loss: 0.3999\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3393 - val_loss: 0.3999\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 0.3428 - val_loss: 0.3999\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3421 - val_loss: 0.3999\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 0.3439 - val_loss: 0.3999\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3376 - val_loss: 0.3999\n",
      "[2022_02_02-15:37:20] Training on final epochs of sequence length 1024...\n",
      "[2022_02_02-15:37:20] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_02_02-15:37:50] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 1022.\n",
      " 6/21 [=======>......................] - ETA: 3s - loss: 0.3380WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1085s vs `on_train_batch_end` time: 0.1443s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1085s vs `on_train_batch_end` time: 0.1443s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 14s 353ms/step - loss: 0.3691 - val_loss: 0.4045\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 50\n",
    "batch_size = 128\n",
    "learning_rate = 1e-5\n",
    "wandb.config = {\n",
    "      \"learning_rate\": learning_rate,\n",
    "      \"epochs\": epoch_num * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }\n",
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], valid_data['seq'], valid_data['Y'], \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc6f941c-f6c4-4ec0-9ba9-bdbbb942963d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  88   8\n",
       "1   9  14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6222222222222222"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)\n",
    "fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b52a48a5-ff77-497f-8b0f-798b8351a213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/batch_128_lr_1e-5_2022_02_02.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/batch_128_lr_1e-5_2022_02_02.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "mod = model_generator.create_model(seq_len = 512)\n",
    "mod.save(path.join(DATA_DIR, \"protein_bert/batch_128_lr_1e-5_2022_02_02.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "531b454a-3ba9-4d8f-bcfd-7bc2aa9df881",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_02_02-15:41:03] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_02_02-15:41:03] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_02_02-15:41:03] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "11/11 [==============================] - 11s 290ms/step - loss: 0.4145 - val_loss: 0.4081\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.3452 - val_loss: 0.4213\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3442 - val_loss: 0.4044\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.3317 - val_loss: 0.4212\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3417 - val_loss: 0.4024\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3341 - val_loss: 0.4017\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3313 - val_loss: 0.4016\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.3231 - val_loss: 0.4016\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.3396 - val_loss: 0.4016\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3299 - val_loss: 0.4013\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3283 - val_loss: 0.4010\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3277 - val_loss: 0.4009\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.3340 - val_loss: 0.4009\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3283 - val_loss: 0.4009\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3364 - val_loss: 0.4009\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3274 - val_loss: 0.4009\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3299 - val_loss: 0.4009ETA: 0s - loss: \n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3302 - val_loss: 0.4009\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3357 - val_loss: 0.4009\n",
      "Epoch 20/50\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3301 - val_loss: 0.4009\n",
      "Epoch 21/50\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.3324 - val_loss: 0.4009\n",
      "Epoch 22/50\n",
      "11/11 [==============================] - 1s 102ms/step - loss: 0.3336 - val_loss: 0.4009\n",
      "Epoch 23/50\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3309 - val_loss: 0.4009\n",
      "Epoch 24/50\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.3274 - val_loss: 0.4009\n",
      "Epoch 25/50\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3292 - val_loss: 0.4009\n",
      "Epoch 26/50\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.3349 - val_loss: 0.4009\n",
      "Epoch 27/50\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3342 - val_loss: 0.4009\n",
      "Epoch 28/50\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3337 - val_loss: 0.4009\n",
      "Epoch 29/50\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3316 - val_loss: 0.4009\n",
      "Epoch 30/50\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.3236 - val_loss: 0.4009\n",
      "Epoch 31/50\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3337 - val_loss: 0.4009\n",
      "Epoch 32/50\n",
      "11/11 [==============================] - 1s 102ms/step - loss: 0.3238 - val_loss: 0.4009\n",
      "Epoch 33/50\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3320 - val_loss: 0.4009\n",
      "Epoch 34/50\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3339 - val_loss: 0.4009\n",
      "Epoch 35/50\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3299 - val_loss: 0.4009\n",
      "Epoch 36/50\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3367 - val_loss: 0.4009\n",
      "Epoch 37/50\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3297 - val_loss: 0.4009\n",
      "Epoch 38/50\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3305 - val_loss: 0.4009\n",
      "Epoch 39/50\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.3385 - val_loss: 0.4009\n",
      "[2022_02_02-15:42:01] Training the entire fine-tuned model...\n",
      "[2022_02_02-15:42:43] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/50\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3289WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1096s vs `on_train_batch_end` time: 0.1497s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1096s vs `on_train_batch_end` time: 0.1497s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 441ms/step - loss: 0.3380 - val_loss: 0.4048\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3285 - val_loss: 0.3971\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3269 - val_loss: 0.4206\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3235 - val_loss: 0.3989\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3217 - val_loss: 0.3927\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 0.3130 - val_loss: 0.3917\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3200 - val_loss: 0.3920\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 3s 264ms/step - loss: 0.3217 - val_loss: 0.3921\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 3s 267ms/step - loss: 0.3090 - val_loss: 0.3921\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "[2022_02_02-15:43:18] Training on final epochs of sequence length 1024...\n",
      "[2022_02_02-15:43:18] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_02_02-15:43:18] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 1022.\n",
      " 6/21 [=======>......................] - ETA: 3s - loss: 0.3614WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1081s vs `on_train_batch_end` time: 0.1455s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1081s vs `on_train_batch_end` time: 0.1455s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 14s 352ms/step - loss: 0.3435 - val_loss: 0.3930\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 50\n",
    "batch_size = 128\n",
    "learning_rate = 5e-5\n",
    "wandb.config = {\n",
    "      \"learning_rate\": learning_rate,\n",
    "      \"epochs\": epoch_num * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }\n",
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], valid_data['seq'], valid_data['Y'], \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2df83ea-5adc-473c-ae52-3042fd95d68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  86  10\n",
       "1   8  15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)\n",
    "fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5eed3d57-a38c-4428-98bf-2cdfb31a3fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/batch_128_lr_5e-5_2022_02_02.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/batch_128_lr_5e-5_2022_02_02.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "mod = model_generator.create_model(seq_len = 512)\n",
    "mod.save(path.join(DATA_DIR, \"protein_bert/batch_128_lr_5e-5_2022_02_02.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5149694a-1a1c-4e39-8034-ed1946e1386c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
